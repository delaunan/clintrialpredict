{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d663bb",
   "metadata": {},
   "source": [
    "Configuration & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe002e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Dynamic Path Setup\n",
    "# Assumes your folder structure is:\n",
    "# project_root/\n",
    "# ├── data/\n",
    "# ├── src/\n",
    "# └── 03_baseline_model.ipynb\n",
    "\n",
    "# Get the current working directory\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "# Define Data Path relative to project root\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
    "CSV_PATH = os.path.join(DATA_PATH, 'project_data.csv')\n",
    "\n",
    "# Add 'src' to Python path so we can import our modules\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Path:    {DATA_PATH}\")\n",
    "\n",
    "# Check if data folder exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data folder not found at {DATA_PATH}. Please create it and add your raw .txt files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeb0cd",
   "metadata": {},
   "source": [
    " Smart Data Loading (Auto-Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data_loader import ClinicalTrialLoader\n",
    "\n",
    "# Check if the processed file exists\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(f\"Loading existing dataset from: {CSV_PATH}\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "else:\n",
    "    print(\"Processed file not found. Triggering ETL pipeline...\")\n",
    "    print(\"This may take 1-2 minutes.\")\n",
    "\n",
    "    # Initialize Loader with the path defined in Cell 1\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # Run Pipeline\n",
    "    df = loader.load_and_clean()\n",
    "    df = loader.add_features(df)\n",
    "\n",
    "    # Save for next time\n",
    "    loader.save(df, filename='project_data.csv')\n",
    "    print(\"ETL Complete. Data loaded.\")\n",
    "\n",
    "print(f\"Data Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2bd4",
   "metadata": {},
   "source": [
    "Temporal Split (Time Travel) <br>\n",
    "Why: We sort by date to ensure strict separation of Past (Train) and Future (Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sort by Start Year (Crucial for Time Series/Evolution)\n",
    "df = df.sort_values('start_year').reset_index(drop=True)\n",
    "\n",
    "# 2. Define Split Point (80% Train / 20% Test)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "# 3. Split\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "# 4. Define Features (X) and Target (y)\n",
    "target_col = 'target'\n",
    "drop_cols = [target_col, 'overall_status', 'nct_id']\n",
    "\n",
    "X_train = train_df.drop(columns=drop_cols)\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols)\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(f\"Train Set: {train_df['start_year'].min()} - {train_df['start_year'].max()} (n={len(train_df)})\")\n",
    "print(f\"Test Set:  {test_df['start_year'].min()} - {test_df['start_year'].max()} (n={len(test_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cb5b4",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf998f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.preprocessing import get_pipeline\n",
    "\n",
    "# 1. Get the Preprocessing Pipeline\n",
    "preprocessor = get_pipeline()\n",
    "\n",
    "# 2. Define the Model\n",
    "# Class Weight 'balanced' is key for our imbalanced dataset (19% failures)\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear',\n",
    "        C=0.01,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3. Train\n",
    "print(\"Training Baseline Model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caa740",
   "metadata": {},
   "source": [
    "Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99386bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# 1. Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Metrics\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 3. Plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred,\n",
    "    normalize='true',\n",
    "    cmap='Blues',\n",
    "    display_labels=['Completed', 'Failed'],\n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set_title(\"Confusion Matrix (Normalized)\")\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob, ax=ax[1], name='Baseline')\n",
    "ax[1].plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "ax[1].set_title(f\"ROC Curve (AUC={roc:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f25827",
   "metadata": {},
   "source": [
    "Save Model <br>\n",
    "Why: Saves the trained pipeline so the Streamlit app can load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'models'), exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(PROJECT_ROOT, 'models', 'baseline_pipeline.joblib')\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
