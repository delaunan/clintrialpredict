{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d663bb",
   "metadata": {},
   "source": [
    "Configuration & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe002e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# 1. Dynamic Path Setup\n",
    "current_dir = os.getcwd()\n",
    "project_root = current_dir\n",
    "\n",
    "# Climb up until we find 'src'\n",
    "while not os.path.exists(os.path.join(project_root, 'src')):\n",
    "    parent = os.path.dirname(project_root)\n",
    "    if parent == project_root:\n",
    "        raise FileNotFoundError(\"Could not find 'src'. Are you in the project folder?\")\n",
    "    project_root = parent\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "DATA_PATH = os.path.join(project_root, 'data')\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Data Path:    {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Custom Modules\n",
    "\n",
    "from src.data_loader_tx import ClinicalTrialLoader\n",
    "from src.preprocessing_XGB import get_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeb0cd",
   "metadata": {},
   "source": [
    " Smart Data Loading (Auto-Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(DATA_PATH, 'project_data.csv')\n",
    "FORCE_REGENERATE = False  # Set to True if you changed code in data_loader.py\n",
    "\n",
    "if os.path.exists(CSV_PATH) and not FORCE_REGENERATE:\n",
    "    print(f\">>> Loading existing dataset from: {CSV_PATH}\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "else:\n",
    "    print(\">>> File not found (or forced regeneration). Triggering ETL pipeline...\")\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # 1. Load & Clean\n",
    "    df = loader.load_and_clean()\n",
    "\n",
    "    # 2. Add Features (Hierarchy, Competition, Text)\n",
    "    df = loader.add_features(df)\n",
    "\n",
    "    # 3. Save\n",
    "    loader.save(df, filename='project_data.csv')\n",
    "\n",
    "print(f\"Data Ready. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2bd4",
   "metadata": {},
   "source": [
    "Temporal Split (Time Travel) <br>\n",
    "Why: We sort by date to ensure strict separation of Past (Train) and Future (Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# TEMPORAL SPLIT & STATISTICS REPORT\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Sort by Date to ensure we predict the future from the past\n",
    "df = df.sort_values('start_year').reset_index(drop=True)\n",
    "\n",
    "# 2. Define Split Point (e.g., 80% Train, 20% Test)\n",
    "split_idx = int(len(df) * 0.80)\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "# 3. Define Features & Target\n",
    "\n",
    "target_col = 'target'\n",
    "\n",
    "drop_cols = [target_col, 'overall_status', 'nct_id', 'why_stopped', 'start_date_type']\n",
    "\n",
    "X_train = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# PRINT STATISTICS\n",
    "# ----------------------------------------------------------------------------------\n",
    "def print_stats(name, dataset, y):\n",
    "    n_total = len(dataset)\n",
    "    n_good = len(dataset[y == 0]) # 0 = Completed\n",
    "    n_bad = len(dataset[y == 1])  # 1 = Terminated/Withdrawn\n",
    "\n",
    "    pct_good = (n_good / n_total) * 100\n",
    "    pct_bad = (n_bad / n_total) * 100\n",
    "\n",
    "    min_year = dataset['start_year'].min()\n",
    "    max_year = dataset['start_year'].max()\n",
    "\n",
    "    print(f\"--- {name} SET ---\")\n",
    "    print(f\"Time Period:   {int(min_year)} to {int(max_year)}\")\n",
    "    print(f\"Total Trials:  {n_total}\")\n",
    "    print(f\"Good (0):      {n_good} ({pct_good:.1f}%)\")\n",
    "    print(f\"Bad (1):       {n_bad} ({pct_bad:.1f}%)\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"DATASET SPLIT STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall\n",
    "print_stats(\"OVERALL\", df, df[target_col])\n",
    "\n",
    "# Training\n",
    "print_stats(\"TRAINING (Past)\", train_df, y_train)\n",
    "\n",
    "# Testing\n",
    "print_stats(\"TESTING (Future)\", test_df, y_test)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"Split Ratio: {len(train_df)/len(df):.0%} Train / {len(test_df)/len(df):.0%} Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cb5b4",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf998f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Calculate Class Weight (Scale Pos Weight)\n",
    "# Formula: (Count of Negatives) / (Count of Positives)\n",
    "# This tells XGBoost: \"Pay X times more attention to failures\"\n",
    "neg_count = np.sum(y_train == 0)\n",
    "pos_count = np.sum(y_train == 1)\n",
    "scale_weight = neg_count / pos_count\n",
    "\n",
    "\n",
    "# 2. Build Pipeline\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "\n",
    "    ('preprocessor', get_pipeline()),\n",
    "    ('classifier', XGBClassifier(\n",
    "\n",
    "        n_estimators=1000,               # Number of trees (conservative)\n",
    "        learning_rate=0.05,             # Step size (lower = more robust)\n",
    "\n",
    "        # --- Tree Control ---\n",
    "        max_depth=6,                    # Tree depth (lower = less overfitting)\n",
    "        min_child_weight=1,             # Min samples per leaf\n",
    "        gamma=0.1,                      # Min loss reduction to split\n",
    "\n",
    "        # --- Sampling ---\n",
    "        subsample=0.8,                  # Train on 80% of rows per tree\n",
    "        colsample_bytree=0.8,           # Train on 80% of columns per tree\n",
    "        colsample_level=0.8,            # Column subsample per level\n",
    "\n",
    "        # --- Imbalance & Regularization ---\n",
    "        scale_pos_weight=scale_weight,  # Handle Imbalance\n",
    "        reg_alpha=0,                  # L1 Regularization\n",
    "        reg_lambda=1,                 # L2 Regularization\n",
    "\n",
    "        # --- Configuration ---\n",
    "        objective='binary:logistic',  # Binary Classification\n",
    "        eval_metric='aucpr',         # Evaluation Metric ('logloss', 'auc', 'aucpr', etc.)\n",
    "        random_state=42,\n",
    "        n_jobs=-1                       # Parallel processing\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Fit\n",
    "print(f\"Training Logistic Regression on {len(X_train)} trials...\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caa740",
   "metadata": {},
   "source": [
    "Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "\n",
    "# 1. Get Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Calculate Advanced Metrics\n",
    "roc_score = roc_auc_score(y_test, y_prob)\n",
    "pr_score = average_precision_score(y_test, y_prob)\n",
    "\n",
    "print(f\"--- MODEL PERFORMANCE METRICS ---\")\n",
    "print(f\"ROC-AUC Score:      {roc_score:.4f}  (0.5 = Random, 1.0 = Perfect)\")\n",
    "print(f\"PR-AUC Score:       {pr_score:.4f}   (Baseline: {y_test.mean():.4f})\")\n",
    "print(\"-\" * 40)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 3. Visualizations\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# A. Confusion Matrix (Normalized)\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred,\n",
    "    normalize='true',\n",
    "    cmap='Blues',\n",
    "    display_labels=['Completed', 'Failed'],\n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set_title(\"Confusion Matrix (Normalized)\")\n",
    "\n",
    "# B. ROC Curve\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob, ax=ax[1], name='LogReg')\n",
    "ax[1].set_title(f\"ROC Curve (AUC = {roc_score:.2f})\")\n",
    "ax[1].plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "\n",
    "# C. Precision-Recall Curve\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_prob, ax=ax[2], name='LogReg')\n",
    "ax[2].set_title(f\"PR Curve (Avg Prec = {pr_score:.2f})\")\n",
    "ax[2].plot([0, 1], [y_test.mean(), y_test.mean()], \"k--\", label=\"Baseline\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Business Rule (Custom Threshold)\n",
    "# We lower the threshold to catch more failures (High Recall strategy)\n",
    "\n",
    "#threshold = 0.4\n",
    "#y_pred_custom = (y_prob >= threshold).astype(int)\n",
    "#tn, fp, fn, tp = confusion_matrix(y_test, y_pred_custom).ravel()\n",
    "\n",
    "#recall = tp / (tp + fn)\n",
    "#precision = tp / (tp + fp)\n",
    "\n",
    "\n",
    "# 3. Visualizations\n",
    "#fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# A. Confusion Matrix (Normalized)\n",
    "#ConfusionMatrixDisplay.from_predictions(\n",
    "#    y_test, y_pred,\n",
    "#    normalize='true',\n",
    "#    cmap='Blues',\n",
    "#    display_labels=['Completed', 'Failed'],\n",
    "#    ax=ax[0]\n",
    "#)\n",
    "#ax[0].set_title(\"Confusion Matrix (Normalized)\")\n",
    "\n",
    "# B. ROC Curve\n",
    "#RocCurveDisplay.from_predictions(y_test, y_prob, ax=ax[1], name='LogReg')\n",
    "#ax[1].set_title(f\"ROC Curve (AUC = {roc_score:.2f})\")\n",
    "#ax[1].plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "\n",
    "## C. Precision-Recall Curve\n",
    "#PrecisionRecallDisplay.from_predictions(y_test, y_prob, ax=ax[2], name='LogReg')\n",
    "#ax[2].set_title(f\"PR Curve (Avg Prec = {pr_score:.2f})\")\n",
    "#ax[2].plot([0, 1], [y_test.mean(), y_test.mean()], \"k--\", label=\"Baseline\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b2c2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def check_logreg_overfitting_temporal(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    1. Calculates the Generalization Gap (Train vs Future Test).\n",
    "    2. Generates a Time-Aware Learning Curve using TimeSeriesSplit.\n",
    "       This prevents 'looking ahead' into the future during validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- PART 1: THE NUMBERS (Train vs Future Test) ---\n",
    "    print(\"--- 1. GENERALIZATION GAP (2000-2013 vs 2013-2015) ---\")\n",
    "\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    gap = train_auc - test_auc\n",
    "\n",
    "    print(f\"Train AUC (Past):   {train_auc:.4f}\")\n",
    "    print(f\"Test AUC (Future):  {test_auc:.4f}\")\n",
    "    print(f\"Gap:                {gap:.4f}\")\n",
    "\n",
    "    if gap > 0.10:\n",
    "        print(\"Verdict:   ⚠️ HIGH OVERFITTING (Memorizing the past, failing the future)\")\n",
    "    elif gap > 0.05:\n",
    "        print(\"Verdict:   ⚠️ MODERATE OVERFITTING\")\n",
    "    else:\n",
    "        print(\"Verdict:   ✅ GOOD TEMPORAL GENERALIZATION\")\n",
    "\n",
    "    # --- PART 2: THE VISUAL (Time-Series Learning Curve) ---\n",
    "    print(\"\\n--- 2. GENERATING TEMPORAL LEARNING CURVE ---\")\n",
    "    print(\"(Using 5-Split Expanding Window: Train on Year 0-N, Validate on Year N+1)\")\n",
    "\n",
    "    # CRITICAL CHANGE: Use TimeSeriesSplit instead of standard CV\n",
    "    # This ensures we never train on future data to predict the past\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=tscv,           # <--- The Fix: Enforce Time Order\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        shuffle=False      # <--- The Fix: Do not shuffle rows\n",
    "    )\n",
    "\n",
    "    # Calculate means and standard deviations\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot Training Line\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"blue\", label=\"Training Score (Past)\")\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
    "\n",
    "    # Plot Validation Line\n",
    "    plt.plot(train_sizes, val_mean, 'o-', color=\"green\", label=\"Validation Score (Future)\")\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color=\"green\")\n",
    "\n",
    "    plt.title(\"Temporal Learning Curve (Expanding Window)\")\n",
    "    plt.xlabel(\"Training Examples Used (Chronological)\")\n",
    "    plt.ylabel(\"ROC-AUC Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add annotation explaining the split\n",
    "    plt.figtext(0.5, -0.05, \"Note: Validation scores are calculated on 'future' folds relative to the training set.\",\n",
    "                ha=\"center\", fontsize=10, style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# Note: X_train must be sorted by date for this to work correctly.\n",
    "# Your notebook sorted df by 'start_year' before splitting, so X_train is already sorted.\n",
    "check_logreg_overfitting_temporal(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1773068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "def run_pro_analysis(model, X_test, y_test, raw_test_df):\n",
    "    \"\"\"\n",
    "    Runs a 4-angle deep dive on model predictions.\n",
    "    Requires:\n",
    "    - model: Trained pipeline/model\n",
    "    - X_test: Processed features (or raw if pipeline handles it)\n",
    "    - y_test: Target labels\n",
    "    - raw_test_df: The original dataframe for the test set (to extract Year/Phase/etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get Probabilities\n",
    "    # Note: We take the probability of Class 1 (Failure)\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a temporary analysis dataframe\n",
    "    analysis_df = raw_test_df.copy().reset_index(drop=True)\n",
    "    analysis_df['target'] = y_test.values\n",
    "    analysis_df['prob_failure'] = y_probs\n",
    "\n",
    "    # Setup Plotting Grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ANGLE 1: CALIBRATION CURVE (Reliability)\n",
    "    # ==============================================================================\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_probs, n_bins=10)\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(prob_pred, prob_true, marker='o', label='Model', color='blue')\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "    ax1.set_xlabel('Mean Predicted Probability')\n",
    "    ax1.set_ylabel('Fraction of Positives (Actual Failure Rate)')\n",
    "    ax1.set_title('Angle 1: Calibration Plot (Reliability)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ANGLE 2: VINTAGE ANALYSIS (Temporal Stability)\n",
    "    # ==============================================================================\n",
    "    # Group by Start Year and calculate AUC per year\n",
    "    vintage_stats = []\n",
    "    years = sorted(analysis_df['start_year'].unique())\n",
    "\n",
    "    for year in years:\n",
    "        subset = analysis_df[analysis_df['start_year'] == year]\n",
    "        if len(subset) > 0 and subset['target'].nunique() > 1:\n",
    "            auc = roc_auc_score(subset['target'], subset['prob_failure'])\n",
    "            count = len(subset)\n",
    "            vintage_stats.append({'Year': int(year), 'AUC': auc, 'Count': count})\n",
    "\n",
    "    vintage_df = pd.DataFrame(vintage_stats)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.lineplot(data=vintage_df, x='Year', y='AUC', marker='o', color='green', ax=ax2)\n",
    "    ax2.set_ylim(0.5, 1.0)\n",
    "    ax2.set_title('Angle 2: Vintage Analysis (Stability over Time)')\n",
    "    ax2.set_ylabel('ROC-AUC Score')\n",
    "    ax2.set_xticks(vintage_df['Year'])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add count labels\n",
    "    for index, row in vintage_df.iterrows():\n",
    "        ax2.text(row['Year'], row['AUC'] + 0.01, f\"n={int(row['Count'])}\", ha='center', fontsize=9)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ANGLE 3: RISK DECILE ANALYSIS (Business Impact)\n",
    "    # ==============================================================================\n",
    "    # Bin predictions into 10 buckets (Deciles)\n",
    "    analysis_df['decile'] = pd.qcut(analysis_df['prob_failure'], 10, labels=False, duplicates='drop')\n",
    "\n",
    "    decile_stats = analysis_df.groupby('decile').agg({\n",
    "        'target': 'mean',          # Actual Failure Rate\n",
    "        'prob_failure': 'mean'     # Predicted Failure Rate\n",
    "    }).reset_index()\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    width = 0.35\n",
    "    x = np.arange(len(decile_stats))\n",
    "\n",
    "    ax3.bar(x - width/2, decile_stats['prob_failure'], width, label='Predicted Risk', color='skyblue', alpha=0.7)\n",
    "    ax3.bar(x + width/2, decile_stats['target'], width, label='Actual Failure Rate', color='salmon', alpha=0.7)\n",
    "\n",
    "    ax3.set_xlabel('Risk Decile (0=Lowest Risk, 9=Highest Risk)')\n",
    "    ax3.set_ylabel('Failure Rate')\n",
    "    ax3.set_title('Angle 3: Risk Deciles (Lift Chart)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ANGLE 4: COHORT SLICING (Phase & Sponsor)\n",
    "    # ==============================================================================\n",
    "    # Calculate AUC for specific slices\n",
    "    slices = []\n",
    "\n",
    "    # Slice by Phase\n",
    "    for phase in analysis_df['phase'].unique():\n",
    "        subset = analysis_df[analysis_df['phase'] == phase]\n",
    "        if len(subset) > 50 and subset['target'].nunique() > 1:\n",
    "            auc = roc_auc_score(subset['target'], subset['prob_failure'])\n",
    "            slices.append({'Slice': f\"Phase: {phase}\", 'AUC': auc})\n",
    "\n",
    "    # Slice by Sponsor Tier\n",
    "    for tier in analysis_df['sponsor_tier'].unique():\n",
    "        subset = analysis_df[analysis_df['sponsor_tier'] == tier]\n",
    "        if len(subset) > 50 and subset['target'].nunique() > 1:\n",
    "            auc = roc_auc_score(subset['target'], subset['prob_failure'])\n",
    "            slices.append({'Slice': f\"Sponsor: {tier}\", 'AUC': auc})\n",
    "\n",
    "    slice_df = pd.DataFrame(slices).sort_values('AUC', ascending=False)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.barplot(data=slice_df, x='AUC', y='Slice', palette='viridis', ax=ax4)\n",
    "    ax4.set_xlim(0.4, 1.0)\n",
    "    ax4.set_title('Angle 4: Cohort Performance (Bias Check)')\n",
    "    ax4.axvline(0.5, color='red', linestyle='--', label='Random')\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return analysis_df\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# EXECUTE ANALYSIS\n",
    "# ---------------------------------------------------------\n",
    "# Ensure we pass the raw test dataframe to get columns like 'start_year' and 'phase' back\n",
    "# We use test_df (which you defined in cell 11 of your notebook)\n",
    "print(\"Running Deep Dive Analysis on Baseline Model...\")\n",
    "results_df = run_pro_analysis(model, X_test, y_test, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c1bf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the Preprocessor\n",
    "preprocessor = get_pipeline()\n",
    "\n",
    "# 2. Fit and Transform X_train\n",
    "# CRITICAL FIX: Pass y_train here! TargetEncoder needs it.\n",
    "print(\"Running preprocessor... this might take a moment due to SVD...\")\n",
    "X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "\n",
    "# --- FIX FOR SPARSE MATRICES ---\n",
    "if hasattr(X_train_transformed, \"toarray\"):\n",
    "    X_train_transformed = X_train_transformed.toarray()\n",
    "\n",
    "# 3. Get Feature Names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 4. Convert to DataFrame for Easy Viewing\n",
    "df_transformed = pd.DataFrame(\n",
    "    X_train_transformed,\n",
    "    columns=feature_names,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# --- INSPECTION REPORT ---\n",
    "print(f\"\\nOriginal Shape: {X_train.shape}\")\n",
    "print(f\"Transformed Shape: {df_transformed.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5. Inspect SVD Columns\n",
    "svd_cols = [col for col in df_transformed.columns if 'svd' in col.lower()]\n",
    "if svd_cols:\n",
    "    print(f\"\\n✅ Found {len(svd_cols)} Text SVD Features\")\n",
    "    print(df_transformed[svd_cols[:5]].head(3))\n",
    "else:\n",
    "    print(\"\\n❌ Could not identify SVD columns by name.\")\n",
    "\n",
    "# 6. Show Head\n",
    "print(\"\\nFirst 5 rows of transformed data:\")\n",
    "try:\n",
    "    display(df_transformed.head())\n",
    "except:\n",
    "    print(df_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data (Assuming 'df' is your loaded dataframe from the loader)\n",
    "# If df is not in memory, uncomment:\n",
    "# from src.data_loader_tx import ClinicalTrialLoader\n",
    "# loader = ClinicalTrialLoader(data_path='data/')\n",
    "# df = loader.load_and_clean()\n",
    "# df = loader.add_features(df)\n",
    "\n",
    "print(\"=== 1. PHASE MAPPING AUDIT ===\")\n",
    "# Check how raw phases map to the ordinal feature\n",
    "phase_check = df[['phase', 'phase_ordinal']].drop_duplicates().sort_values('phase_ordinal')\n",
    "print(phase_check)\n",
    "\n",
    "print(\"\\n=== 2. FEATURE DISTRIBUTION AUDIT (Pre-Scaling) ===\")\n",
    "# Check stats of numerical columns to see their natural range\n",
    "num_cols = ['competition_broad', 'num_primary_endpoints', 'criteria_len_log', 'start_year']\n",
    "print(df[num_cols].describe().loc[['min', 'max', 'mean', 'std']])\n",
    "\n",
    "print(\"\\n=== 3. MISSING VALUES CHECK ===\")\n",
    "# XGBoost handles NaNs automatically, but good to know\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
