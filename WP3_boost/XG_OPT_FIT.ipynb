{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d663bb",
   "metadata": {},
   "source": [
    "Configuration & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74d3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings from Scikit-Learn to clean up plot outputs\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c34fe",
   "metadata": {},
   "source": [
    "### Identify the correct path on own computer\n",
    "\n",
    "**goal** : not to have to change any path manually when running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f54897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/delaunan/code/delaunan/clintrialpredict\n",
      "Data Path:    /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "Models Path:  /home/delaunan/code/delaunan/clintrialpredict/models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the Project Root\n",
    "# Start at the current directory and look for 'src' to identify the root\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "\n",
    "while not (project_root / 'src').exists():\n",
    "    if project_root == project_root.parent:\n",
    "        # Hit the filesystem root without finding the project\n",
    "        raise FileNotFoundError(\"Could not find project root containing 'src'\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "# 2. Add Project Root to System Path\n",
    "# This allows 'import src...' to work from anywhere\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# 3. Define Key Paths\n",
    "DATA_PATH = project_root / \"data\"\n",
    "MODELS_PATH = project_root / \"models\"\n",
    "\n",
    "# 4. Verification\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Data Path:    {DATA_PATH}\")\n",
    "print(f\"Models Path:  {MODELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8fa7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom Modules/Closses\n",
    "from src.prep.data_loader import ClinicalTrialLoader\n",
    "from src.prep.preprocessing import preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeb0cd",
   "metadata": {},
   "source": [
    "### Loading of source data project_data.csv\n",
    "Delete the one already present in clintrialpredict/data/ if you want the latest version. <br>\n",
    "if data_project.csv already exists in clintrialpredict/data/, the existing one will not be replaced by the latest one <br>\n",
    "**goal** : not to have to rerun and build the source file every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f0dbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading existing dataset from: /home/delaunan/code/delaunan/clintrialpredict/data/project_data.csv\n",
      "Data Ready. Shape: (88374, 67)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# We don't even need 'import os' if we use pathlib correctly\n",
    "\n",
    "# 1. Define path using the '/' operator (works because DATA_PATH is a Path object)\n",
    "CSV_PATH = DATA_PATH / 'project_data.csv'\n",
    "FORCE_REGENERATE = False\n",
    "\n",
    "# 2. Check existence using .exists() method\n",
    "if CSV_PATH.exists() and not FORCE_REGENERATE:\n",
    "    print(f\">>> Loading existing dataset from: {CSV_PATH}\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "else:\n",
    "    print(\">>> File not found (or forced regeneration). Triggering ETL pipeline...\")\n",
    "\n",
    "    # Ensure ClinicalTrialLoader is imported from your src\n",
    "    # from src.WP5_utils.data_loader import ClinicalTrialLoader\n",
    "\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # 1. Load & Clean\n",
    "    df = loader.load_and_clean()\n",
    "\n",
    "    # 2. Add Features (Hierarchy, Competition, Text)\n",
    "    df = loader.add_features(df)\n",
    "\n",
    "    # 3. Save\n",
    "    loader.save(df, filename='project_data.csv')\n",
    "\n",
    "print(f\"Data Ready. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a9098",
   "metadata": {},
   "source": [
    "### Audit of features in data_project.csv (to be runned only to dig deeper)\n",
    "Audit file with all information saved to clintrialpredict/data/audit_features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f23d36d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/delaunan/code/delaunan/clintrialpredict/data/project_data.csv...\n",
      "[1/9] Checking Dataset Health...\n",
      "[2/9] Analyzing Categorical Risk Signals...\n",
      "[3/9] Analyzing Numerical Impact...\n",
      "[4/9] Generating Preprocessing Strategy...\n",
      "[5/9] Calculating Correlations...\n",
      "[6/9] Ranking Categorical Features...\n",
      "[7/9] Checking Collinearity...\n",
      "[8/9] Running Business Logic Checks...\n",
      "[9/9] Writing Documentation...\n",
      "\n",
      "Done. Audit saved to /home/delaunan/code/delaunan/clintrialpredict/data/audit_features.txt\n"
     ]
    }
   ],
   "source": [
    "from src.prep.audit_utils import run_master_audit\n",
    "\n",
    "#Run the audit of features loaded into project_data.csv (will generate audit_full_report.txt in your data folder)\n",
    "\n",
    "run_master_audit(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d996a",
   "metadata": {},
   "source": [
    "### final fit, full database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "248ae15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for Pipeline. X Shape: (88374, 60), y Shape: (88374,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define columns to drop (Target + Metadata + Leakage)\n",
    "forbidden_cols = [\n",
    "    'target',               # The binary target itself (0/1)\n",
    "    'overall_status',       # The text target (e.g. \"Terminated\") - LEAKAGE\n",
    "    'nct_id',               # ID - Metadata\n",
    "    'start_date',           # Date - Metadata\n",
    "    'start_year',           # Date - Metadata\n",
    "    'why_stopped',          # Future info - LEAKAGE\n",
    "    'scientific_success'    # Future info - LEAKAGE\n",
    "]\n",
    "\n",
    "# 2. Create X (Features) and y (Target)\n",
    "X = df.drop(columns=forbidden_cols, errors='ignore')\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Ready for Pipeline. X Shape: {X.shape}, y Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91aaf494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 0s (Negative): 74566\n",
      "Count 1s (Positive): 13808\n",
      "Calculated scale_pos_weight: 5.4002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Count your zeros and ones\n",
    "num_neg = np.sum(y == 0)\n",
    "num_pos = np.sum(y == 1)\n",
    "\n",
    "# 2. Calculate the ratio (Negatives / Positives)\n",
    "# If you have fewer 1s, this number will be > 1, telling the model to pay more attention to them.\n",
    "ratio = float(num_neg) / float(num_pos)\n",
    "\n",
    "print(f\"Count 0s (Negative): {num_neg}\")\n",
    "print(f\"Count 1s (Positive): {num_pos}\")\n",
    "print(f\"Calculated scale_pos_weight: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703030a2",
   "metadata": {},
   "source": [
    "### Model Training (model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "330ac3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Logistic Regression...\n",
      "Fitting XGBoost Model...\n",
      "✅ XGBoost Model Trained - FINAL!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.prep.preprocessing import preprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- 1. Define the Model ---\n",
    "print(\"Initializing Logistic Regression...\")\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor()),\n",
    "    ('classifier', XGBClassifier(\n",
    "        # --- Core Parameters ---\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "\n",
    "        # --- AGGRESSIVE REGULARIZATION (The settings that fixed the gap) ---\n",
    "        n_estimators=300,            # Fixed low number\n",
    "        learning_rate=0.02,          # Slow learning\n",
    "        max_depth=3,                 # Shallow trees (CRITICAL)\n",
    "        reg_lambda=15,               # Heavy L2 penalty\n",
    "        gamma=1.0,                   # High minimum loss reduction\n",
    "        min_child_weight=5,          # High minimum child weight\n",
    "\n",
    "        # --- Standard Parameters ---\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "\n",
    "        # --- Imbalance Handling ---\n",
    "        scale_pos_weight=ratio,\n",
    "\n",
    "        # --- Technical Settings ---\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        tree_method='hist'\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Fit the Model ---\n",
    "print(\"Fitting XGBoost Model...\")\n",
    "model.fit(X, y)\n",
    "print(\"✅ XGBoost Model Trained - FINAL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8f0f3",
   "metadata": {},
   "source": [
    "### Save model (not to have to re-run model.fit next time so that you start with the correct weights)\n",
    "prerequisite will be Train_test_split to recuperate the X_train, y_train, X_test, y_test to feed into pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "586fb140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /home/delaunan/code/delaunan/clintrialpredict/models/ctp_model.joblib ...\n",
      "✅ Model successfully saved as 'ctp_model.joblib' in the project root models folder.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Define the filename in a variable\n",
    "MODEL_FILENAME = 'ctp_model.joblib'\n",
    "\n",
    "# 2. Construct the full path using the pathlib object\n",
    "# This ensures it goes to 'CLINTRIALPREDICT/models/', not 'src/models/'\n",
    "save_path = MODELS_PATH / MODEL_FILENAME\n",
    "\n",
    "# 3. Save the model\n",
    "print(f\"Saving model to: {save_path} ...\")\n",
    "joblib.dump(model, save_path)\n",
    "\n",
    "print(f\"✅ Model successfully saved as '{MODEL_FILENAME}' in the project root models folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70640f",
   "metadata": {},
   "source": [
    "### Imprort pre-trained model (not to have to re-run model.fit)\n",
    "prerequisite Train_test_split to recuperate the X_train, y_train, X_test, y_test to feed into pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading model from: /home/delaunan/code/delaunan/clintrialpredict/src/models/logreg_model.joblib\n",
      "\n",
      "Testing loaded model...\n",
      "Predictions for first 5 trials: [0.40835685 0.31814536 0.59581191 0.11837591 0.33292536]\n"
     ]
    }
   ],
   "source": [
    "# Import the loader\n",
    "from src.models.model_utils import load_model\n",
    "\n",
    "# Retrieve the model from the file\n",
    "my_loaded_model = load_model('ctp_model.joblib')\n",
    "\n",
    "# Verify it works by making a prediction on the first 5 rows of test data\n",
    "#print(\"\\nTesting loaded model...\")\n",
    "#sample_probs = my_loaded_model.predict_proba(X_test.head(5))[:, 1]\n",
    "#print(f\"Predictions for first 5 trials: {sample_probs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
