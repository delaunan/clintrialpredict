{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4c7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruima/.pyenv/versions/clintrialpredict/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install necessary libraries (Run this in your terminal or notebook cell)\n",
    "# !pip install transformers torch pandas\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1f49ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nct_id', 'start_date_type', 'start_date', 'study_type',\n",
       "       'overall_status', 'phase', 'number_of_arms', 'why_stopped', 'target',\n",
       "       'start_year', 'phase_ordinal', 'covid_exposure', 'includes_us',\n",
       "       'is_international', 'agency_class', 'allocation', 'intervention_model',\n",
       "       'primary_purpose', 'masking', 'gender', 'healthy_volunteers', 'adult',\n",
       "       'child', 'older_adult', 'num_primary_endpoints', 'best_pathology',\n",
       "       'therapeutic_area', 'therapeutic_subgroup_name', 'competition_broad',\n",
       "       'competition_niche', 'txt_tags', 'txt_criteria'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. The Dataset\n",
    "DATA_PATH = \"/home/ruima/code/delaunan/clintrialpredict/data\"\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'project_data.csv'))\n",
    "df.columns\n",
    "#df.shape\n",
    "#df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dmis-lab/biobert-v1.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruima/.pyenv/versions/clintrialpredict/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. SETUP & MODEL LOADING\n",
    "# ---------------------------------------------------------\n",
    "# Detect hardware\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load BioBERT (fine-tuned on biomedical text)\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179b55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. EMBEDDING FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def get_biobert_embeddings(text_list, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using the [CLS] token of BioBERT.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Calculate total batches for progress tracking\n",
    "    total_batches = (len(text_list) // batch_size) + 1\n",
    "\n",
    "    print(f\"Processing {len(text_list)} items in batches of {batch_size}...\")\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i : i + batch_size]\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract [CLS] token (first token of last hidden state)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Move to CPU and convert to numpy\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "        # Optional: Print progress every 10 batches\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Batch {i // batch_size}/{total_batches} done.\")\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitizing text data...\n",
      "Generating embeddings...\n",
      "Processing 105336 items in batches of 16...\n",
      "Batch 0/6584 done.\n",
      "Batch 10/6584 done.\n",
      "Batch 20/6584 done.\n",
      "Batch 30/6584 done.\n",
      "Batch 40/6584 done.\n",
      "Batch 50/6584 done.\n",
      "Batch 60/6584 done.\n",
      "Batch 70/6584 done.\n",
      "Batch 80/6584 done.\n",
      "Batch 90/6584 done.\n",
      "Batch 100/6584 done.\n",
      "Batch 110/6584 done.\n",
      "Batch 120/6584 done.\n",
      "Batch 130/6584 done.\n",
      "Batch 140/6584 done.\n",
      "Batch 150/6584 done.\n",
      "Batch 160/6584 done.\n",
      "Batch 170/6584 done.\n",
      "Batch 180/6584 done.\n",
      "Batch 190/6584 done.\n",
      "Batch 200/6584 done.\n",
      "Batch 210/6584 done.\n",
      "Batch 220/6584 done.\n",
      "Batch 230/6584 done.\n",
      "Batch 240/6584 done.\n",
      "Batch 250/6584 done.\n",
      "Batch 260/6584 done.\n",
      "Batch 270/6584 done.\n",
      "Batch 280/6584 done.\n",
      "Batch 290/6584 done.\n",
      "Batch 300/6584 done.\n",
      "Batch 310/6584 done.\n",
      "Batch 320/6584 done.\n",
      "Batch 330/6584 done.\n",
      "Batch 340/6584 done.\n",
      "Batch 350/6584 done.\n",
      "Batch 360/6584 done.\n",
      "Batch 370/6584 done.\n",
      "Batch 380/6584 done.\n",
      "Batch 390/6584 done.\n",
      "Batch 400/6584 done.\n",
      "Batch 410/6584 done.\n",
      "Batch 420/6584 done.\n",
      "Batch 430/6584 done.\n",
      "Batch 440/6584 done.\n",
      "Batch 450/6584 done.\n",
      "Batch 460/6584 done.\n",
      "Batch 470/6584 done.\n",
      "Batch 480/6584 done.\n",
      "Batch 490/6584 done.\n",
      "Batch 500/6584 done.\n",
      "Batch 510/6584 done.\n",
      "Batch 520/6584 done.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. EXECUTION WITH FIX\n",
    "# ---------------------------------------------------------\n",
    "# Assuming 'df' is your main dataframe loaded previously\n",
    "df_sample = df.copy() # Work on a copy to be safe\n",
    "\n",
    "# --- CRITICAL FIX START ---\n",
    "print(\"Sanitizing text data...\")\n",
    "# 1. Fill NaNs with empty string\n",
    "# 2. Force type to string (prevents floats/objects from breaking tokenizer)\n",
    "df_sample['txt_criteria'] = df_sample['txt_criteria'].fillna(\"\").astype(str)\n",
    "# --- CRITICAL FIX END ---\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "# Running with batch_size=16 (Safe for T4 GPU).\n",
    "# If on CPU, reduce to 2 or 4.\n",
    "embeddings = get_biobert_embeddings(df_sample['txt_criteria'].tolist(), batch_size=16)\n",
    "\n",
    "# Assign embeddings back to DataFrame\n",
    "df_sample['criteria_embedding'] = list(embeddings)\n",
    "\n",
    "print(\"Success! Embeddings generated.\")\n",
    "print(f\"Embedding shape: {df_sample['criteria_embedding'].iloc[0].shape}\")\n",
    "print(df_sample[['nct_id', 'criteria_embedding']].head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
