{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceed4378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (1.1.7)\n",
      "Requirement already satisfied: packaging in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ruima/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.1.7━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.1.7:[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.1.7━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "\n",
    "#pip install numpy\n",
    "#!ls /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64db0f",
   "metadata": {},
   "source": [
    "The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c29df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nct_id', 'study_type', 'overall_status', 'phase', 'number_of_arms',\n",
       "       'why_stopped', 'target', 'start_year', 'best_pathology',\n",
       "       'therapeutic_area', 'therapeutic_subgroup', 'therapeutic_subgroup_name',\n",
       "       'competition_broad', 'competition_niche', 'gender',\n",
       "       'healthy_volunteers', 'adult', 'child', 'older_adult',\n",
       "       'num_primary_endpoints', 'min_p_value', 'phase_ordinal',\n",
       "       'covid_exposure', 'is_international', 'agency_class', 'allocation',\n",
       "       'intervention_model', 'primary_purpose', 'masking', 'txt_tags',\n",
       "       'txt_criteria'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "DATA_PATH = \"/home/ruima/code/delaunan/clintrialpredict/data\"\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'project_data.csv'))\n",
    "df.columns\n",
    "#df.shape\n",
    "#df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf951b6",
   "metadata": {},
   "source": [
    "1. Python code structure for loading the data, initializing the BioBERT components, and tokenizing the text features using the Hugging Face transformers and datasets libraries.\n",
    "\n",
    "   This structure assumes that the the dataframe is named df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7adf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'conditions_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/clintrialpredict/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conditions_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assume 'df' is your DataFrame loaded from the SQL query\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example: df = pd.read_csv('00-data.csv')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# --- 1. Create the Combined Text Feature (X) ---\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fill any NaNs in text columns with an empty string so concatenation works\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditions_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconditions_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterventions_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterventions_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrief_summary_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrief_summary_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/clintrialpredict/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/clintrialpredict/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conditions_text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Assume 'df' is your DataFrame loaded from the SQL query\n",
    "\n",
    "# --- 1. Create the Combined Text Feature (X) ---\n",
    "# Fill any NaNs in text columns with an empty string so concatenation works\n",
    "df['conditions_text'] = df['conditions_text'].fillna('')\n",
    "df['interventions_text'] = df['interventions_text'].fillna('')\n",
    "df['brief_summary_text'] = df['brief_summary_text'].fillna('')\n",
    "\n",
    "# Concatenate all relevant text features into one column\n",
    "df['text_input'] = (\n",
    "    \"Title: \" + df['title'] +\n",
    "    \" | Summary: \" + df['brief_summary_text'] +\n",
    "    \" | Conditions: \" + df['conditions_text'] +\n",
    "    \" | Interventions: \" + df['interventions_text'])\n",
    "\n",
    "# --- 2. Create the Numerical Target (Y) ---\n",
    "# Create the binary target (0 or 1) from the categorical 'overall_status'\n",
    "# This is a simplified example; refine based on your EDA/definition\n",
    "df['label'] = df['overall_status'].apply(\n",
    "    lambda x: 1 if x in ['Completed', 'Active, not recruiting'] else 0\n",
    ")\n",
    "\n",
    "# Filter out rows where the target is undefined or incomplete\n",
    "df = df[df['text_input'].str.len() > 100].reset_index(drop=True)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Hugging Face Dataset object\n",
    "hg_dataset = Dataset.from_pandas(df[['text_input', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c9ce3",
   "metadata": {},
   "source": [
    "2. Load Tokenizer and Define Tokenization Function\n",
    "\n",
    "   BioBERT tokenizer will be loaded and create a function to apply it to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load BioBERT Tokenizer ---\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "MAX_LENGTH = 512  # Standard for BERT models\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- 4. Define Tokenization Function ---\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the text input, truncates to max length, and adds necessary\n",
    "    BERT-specific tokens ([CLS], [SEP]).\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text_input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Apply the tokenization across the entire dataset\n",
    "tokenized_datasets = hg_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc2fc9",
   "metadata": {},
   "source": [
    "3. Final Dataset Preparation\n",
    "\n",
    "   The final step is to prepare the dataset for training, including splitting it and selecting the columns the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Format and Split Data ---\n",
    "# Rename the 'label' column to 'labels' as required by the Trainer class\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Remove the original text column and unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text_input\", \"__index_level_0__\"])\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation set size: {len(eval_dataset)}\")\n",
    "print(\"\\nExample Tokenized Data Structure:\")\n",
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a06ae",
   "metadata": {},
   "source": [
    "1. Fine-Tuning\n",
    "\n",
    "Defines the Evaluation Metrics (compute_metrics function)\n",
    "The Hugging Face Trainer requires a function, compute_metrics, that accepts an EvalPrediction object (containing raw model predictions/logits and true labels) and returns a dictionary of metric scores.\n",
    "\n",
    "For this project, the key metrics should be F1 Score and AUC-ROC (Area Under the Receiver Operating Characteristic Curve).\n",
    "\n",
    "F1 Score: The harmonic mean of Precision and Recall. It penalizes models that favor one metric over the other (e.g., high Recall but low Precision), making it robust for imbalanced datasets.\n",
    "\n",
    "AUC-ROC: Measures the model's ability to discriminate between positive and negative classes across all possible classification thresholds. A score of $1.0$ is perfect; $0.5$ is random. This is highly recommended for evaluating risk-modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cfa15",
   "metadata": {},
   "source": [
    "2. Evaluation Setup\n",
    "\n",
    "Configure Training Arguments (TrainingArguments)\n",
    "\n",
    "This object specifies all the hyperparameters, saving, and logging strategies for the Trainer. These values are good starting points for a Transformer model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# --- 3. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert_clinical_trials_output\",  # Required: Directory to save checkpoints/results\n",
    "    num_train_epochs=3,                            # Start with 3 epochs (common for fine-tuning)\n",
    "    per_device_train_batch_size=16,                # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,                              # A few hundred steps for learning rate warm-up\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,                            # Standard low learning rate for fine-tuning\n",
    "    evaluation_strategy=\"epoch\",                   # Evaluate metrics after each full epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,                             # Log training loss every 100 steps\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,                   # Load the model with the best validation metric\n",
    "    metric_for_best_model=\"f1_weighted\",           # Specify which metric to track for \"best model\"\n",
    "    report_to=\"tensorboard\",                       # Optional: Visualize training progress\n",
    ")\n",
    "\n",
    "# --- 4. Initialize the Trainer ---\n",
    "# Assuming 'train_dataset' and 'eval_dataset' are the tokenized datasets from the previous step\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer, # Pass the tokenizer for padding/saving\n",
    "    compute_metrics=compute_metrics, # Pass the custom metrics function\n",
    ")\n",
    "\n",
    "# --- 5. Start Fine-Tuning ---\n",
    "# trainer.train() # This is the command that initiates Week 1's model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c11c74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4998488",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc0005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
