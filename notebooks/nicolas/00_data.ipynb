{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f03244",
   "metadata": {},
   "source": [
    "# Data Quality Check Script: Explained\n",
    "\n",
    "**Purpose:** This script acts as a \"safety scanner\" for our raw AACT text files before we load them into our analysis pipeline. It ensures we don't accidentally lose data due to formatting errors or messy content.\n",
    "\n",
    "**What it does (Step-by-Step):**\n",
    "\n",
    "1.  **Iterates through Files:** It automatically finds every `.txt` file in our `00_data` folder.\n",
    "\n",
    "2.  **Structural Validation (The \"Pipe Check\"):**\n",
    "    * It scans every line to count the pipe separators (`|`).\n",
    "    * *Why?* If a line has too many pipes (e.g., a pipe accidentally typed in a description), it shifts all the data, ruining the row. This script catches those errors.\n",
    "\n",
    "3.  **Correct Loading:**\n",
    "    * It attempts to load the file using the \"Golden Settings\":\n",
    "        * `sep='|'`: Pipe delimiter.\n",
    "        * `quotechar='\"'`: Respects quotes (fixing the \"broken rows\" issue we found earlier).\n",
    "        * `dtype=str`: Reads everything as text first (safest method).\n",
    "\n",
    "4.  **Content Analysis:**\n",
    "    * **Whitespace:** Checks if columns have invisible spaces (e.g., `\" Diabetes \"` vs `\"Diabetes\"`).\n",
    "    * **Mixed Types:** Checks if numeric columns contain text (e.g., `\"1,200\"` or `\"18-35\"`).\n",
    "\n",
    "5.  **Reporting:**\n",
    "    * It saves a full audit log to `data_quality_report_v2.txt` so we have a permanent record of our data health.\n",
    "\n",
    "**Why use it?**\n",
    "Running this ensures our cleaning script (`clean_aact_final.py`) is based on *facts*, not guesses. It proves our data integrity is solid before we start Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcdcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FINAL Data Quality Scan at 2025-11-28 22:26:02.342689\n",
      "Found 50 files in '00_data'.\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/calculated_values.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 558973 rows and 19 columns.\n",
      "  [Info] 1 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/responsible_parties.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 540493 rows and 8 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/drop_withdrawals.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 563769 rows and 10 columns.\n",
      "  [Info] 3 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/countries.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 763540 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/pending_results.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 31628 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/browse_interventions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 2432357 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/search_terms.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 186 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/ipd_information_types.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 82497 rows and 3 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/reported_events.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 11100309 rows and 17 columns.\n",
      "  [Info] 2 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/baseline_measurements.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 2720829 rows and 22 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'param_value' looks numeric but has 724 text values.\n",
      "    Garbage examples: ['10,366', '1,867', '1,135']\n",
      "  [Warning] Column 'dispersion_value' looks numeric but has 416 text values.\n",
      "    Garbage examples: ['22,607', '9,089.6', '7,470.3']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/links.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 72834 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/facility_investigators.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 222908 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/browse_conditions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 4084129 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/design_outcomes.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 3445054 rows and 7 columns.\n",
      "  [Info] 1 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/outcome_counts.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 1494835 rows and 8 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/reported_event_totals.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 562515 rows and 9 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/data_audit_report.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 108 rows and 1 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/retractions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 333 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/design_group_interventions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 1245646 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/facility_contacts.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 403227 rows and 8 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'phone_extension' looks numeric but has 1340 text values.\n",
      "    Garbage examples: ['TTY dial 711', 'springer', 'MITTENDORFER']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/documents.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 10616 rows and 6 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/milestones.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 833469 rows and 10 columns.\n",
      "  [Info] 1 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'count_units' looks numeric but has 10 text values.\n",
      "    Garbage examples: ['327,462', '306,825', '522,370']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/id_information.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 727353 rows and 7 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/search_results.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 0 rows and 7 columns.\n",
      "  [Info] 7 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/overall_officials.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 510534 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/participant_flows.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 75138 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/outcome_analysis_groups.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 594886 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/eligibilities.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 558028 rows and 14 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/outcome_measurements.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 4614173 rows and 21 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'param_value' looks numeric but has 1769 text values.\n",
      "    Garbage examples: ['-4,487.11', '29,179', '0,095']\n",
      "  [Warning] Column 'dispersion_value' looks numeric but has 1113 text values.\n",
      "    Garbage examples: ['22,730.43', '0,018', '0,016']\n",
      "  [Warning] Column 'dispersion_upper_limit_raw' looks numeric but has 272 text values.\n",
      "    Garbage examples: ['53,879', '57.5%', '24,630']\n",
      "  [Warning] Column 'dispersion_lower_limit_raw' looks numeric but has 202 text values.\n",
      "    Garbage examples: ['32.7%', '18,114', '22,350']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/provided_documents.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 73568 rows and 8 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/interventions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 945857 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/outcomes.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 620396 rows and 13 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/sponsors.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 894384 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/intervention_other_names.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 461546 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/result_agreements.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 75138 rows and 7 columns.\n",
      "  [Info] 1 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/studies.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 558973 rows and 71 columns.\n",
      "  [Info] 1 columns are 100% EMPTY.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [Action Needed] 4 columns have invisible spaces (e.g. ' Diabetes ').\n",
      "  Columns: ['baseline_population', 'brief_title', 'official_title', 'ipd_access_criteria']...\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/conditions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 989606 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/outcome_analyses.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 307261 rows and 25 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'ci_upper_limit_raw' looks numeric but has 11 text values.\n",
      "    Garbage examples: ['- 4.74', '- 18.01', '1,738']\n",
      "  [Warning] Column 'ci_lower_limit_raw' looks numeric but has 14 text values.\n",
      "    Garbage examples: ['- 4.84', '- 2.10', '- 50.83']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/design_groups.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 1023177 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/baseline_counts.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 224320 rows and 7 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/study_references.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 1043738 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/brief_summaries.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 558028 rows and 3 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/facilities.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 3343041 rows and 10 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/result_contacts.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 75138 rows and 7 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'extension' looks numeric but has 331 text values.\n",
      "    Garbage examples: ['1#', '(#1163)', '6#']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/result_groups.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 2063029 rows and 7 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/designs.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 554264 rows and 14 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/detailed_descriptions.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 558028 rows and 3 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/keywords.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 1466970 rows and 4 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/central_contacts.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 211164 rows and 8 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "  [Warning] Column 'phone_extension' looks numeric but has 1039 text values.\n",
      "    Garbage examples: ['189-1157-6946', 'option 6', '025-68307881']\n",
      "\n",
      "============================================================\n",
      "PROCESSING: 00_data/search_term_results.txt\n",
      "============================================================\n",
      "\n",
      "--- 3. STRUCTURE: Row/Col Count ---\n",
      "  Loaded 481884 rows and 5 columns.\n",
      "\n",
      "--- 1. HYGIENE: Whitespace Check ---\n",
      "  [OK] No whitespace issues found.\n",
      "\n",
      "--- 2. INTEGRITY: Mixed Data Types ---\n",
      "\n",
      "[SUCCESS] Full report saved to: /home/delaunan/code/delaunan/project/data_quality_report_v2.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"00_data\"\n",
    "REPORT_FILENAME = \"data_quality_report_v2.txt\" # New report name to distinguish from previous run\n",
    "DELIMITER = \"|\"\n",
    "EXTENSION_TO_CHECK = \".txt\"\n",
    "\n",
    "# Buffer to hold report text for file writing\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Helper function to print to console AND buffer for the report file.\"\"\"\n",
    "    print(message)\n",
    "    report_buffer.append(message)\n",
    "\n",
    "def save_report():\n",
    "    \"\"\"Writes the buffered log to the text file.\"\"\"\n",
    "    report_path = REPORT_FILENAME\n",
    "    try:\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(report_buffer))\n",
    "        print(f\"\\n[SUCCESS] Full report saved to: {os.path.abspath(report_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Could not save report file: {e}\")\n",
    "\n",
    "# --- THE CORRECT LOADING FUNCTION ---\n",
    "def load_dataframe_safely(file_path, delimiter):\n",
    "    \"\"\"\n",
    "    Loads a pipe-delimited file using the correct settings to handle quotes.\n",
    "    Returns: DataFrame or None (if failed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=delimiter,\n",
    "            dtype=str,                  # 1. Read as text (prevent type errors)\n",
    "            low_memory=False,           # 2. Read full file at once (prevent chunking errors)\n",
    "            quotechar='\"',              # 3. Handle quotes properly (fixes \"broken rows\")\n",
    "            quoting=csv.QUOTE_MINIMAL,  # 4. Allow quotes to wrap fields containing pipes\n",
    "            on_bad_lines='warn'         # 5. Warn if genuine corruption exists\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log(f\"  [Critical Load Error] {e}\")\n",
    "        return None\n",
    "\n",
    "def check_whitespace_issues(df):\n",
    "    \"\"\"Finds and lists columns that need trimming.\"\"\"\n",
    "    log(\"\\n--- 1. HYGIENE: Whitespace Check ---\")\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    dirty_cols = []\n",
    "\n",
    "    for col in text_cols:\n",
    "        sample = df[col].dropna().head(5000).astype(str)\n",
    "        if sample.empty: continue\n",
    "        if (sample.str.len() != sample.str.strip().str.len()).any():\n",
    "            dirty_cols.append(col)\n",
    "\n",
    "    if dirty_cols:\n",
    "        log(f\"  [Action Needed] {len(dirty_cols)} columns have invisible spaces (e.g. ' Diabetes ').\")\n",
    "        log(f\"  Columns: {dirty_cols[:5]}...\")\n",
    "    else:\n",
    "        log(\"  [OK] No whitespace issues found.\")\n",
    "\n",
    "def check_mixed_types(df):\n",
    "    \"\"\"Checks if a column has both numbers and text (dangerous for analysis).\"\"\"\n",
    "    log(\"\\n--- 2. INTEGRITY: Mixed Data Types ---\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Skip if explicitly string\n",
    "        if df[col].dtype == 'object':\n",
    "            # Try to force numeric\n",
    "            numeric_vals = pd.to_numeric(df[col], errors='coerce')\n",
    "            num_count = numeric_vals.notna().sum()\n",
    "            total_count = df[col].notna().sum()\n",
    "\n",
    "            # If a column is mostly numbers (90%) but has some text, warn user\n",
    "            if total_count > 0 and (num_count / total_count) > 0.9 and (num_count != total_count):\n",
    "                log(f\"  [Warning] Column '{col}' looks numeric but has {total_count - num_count} text values.\")\n",
    "                # Show non-numeric examples\n",
    "                non_nums = df[col][numeric_vals.isna() & df[col].notna()]\n",
    "                log(f\"    Garbage examples: {non_nums.head(3).tolist()}\")\n",
    "\n",
    "def check_structure_and_counts(df, file_path):\n",
    "    \"\"\"Basic structure check using the DF shape (since we trust the loader now).\"\"\"\n",
    "    log(f\"\\n--- 3. STRUCTURE: Row/Col Count ---\")\n",
    "    rows, cols = df.shape\n",
    "    log(f\"  Loaded {rows} rows and {cols} columns.\")\n",
    "\n",
    "    # Check for empty columns\n",
    "    empty_cols = [col for col in df.columns if df[col].isnull().all()]\n",
    "    if empty_cols:\n",
    "        log(f\"  [Info] {len(empty_cols)} columns are 100% EMPTY.\")\n",
    "\n",
    "def process_file(file_name):\n",
    "    # Construct full path correctly for Linux/WSL\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(f\"PROCESSING: {file_path}\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        log(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Use the CORRECT loader\n",
    "    df = load_dataframe_safely(file_path, DELIMITER)\n",
    "\n",
    "    if df is not None:\n",
    "        check_structure_and_counts(df, file_path)\n",
    "        check_whitespace_issues(df)\n",
    "        check_mixed_types(df)\n",
    "\n",
    "# --- EXECUTION LOOP ---\n",
    "log(f\"Starting FINAL Data Quality Scan at {datetime.now()}\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    log(f\"CRITICAL ERROR: The folder '{DATA_DIR}' was not found.\")\n",
    "else:\n",
    "    all_files = os.listdir(DATA_DIR)\n",
    "    target_files = [f for f in all_files if f.endswith(EXTENSION_TO_CHECK)]\n",
    "\n",
    "    if not target_files:\n",
    "        log(f\"No files ending with '{EXTENSION_TO_CHECK}' found in '{DATA_DIR}'.\")\n",
    "    else:\n",
    "        log(f\"Found {len(target_files)} files in '{DATA_DIR}'.\")\n",
    "        for file_name in target_files:\n",
    "            process_file(file_name)\n",
    "\n",
    "# Finally, save the report to disk\n",
    "save_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc212f55",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Deep Dive Diagnostic Script: Explained\n",
    "\n",
    "**Purpose:** This script is a targeted diagnostic tool that investigates specific issues flagged by the initial `check_data_quality.py` report. It drills down into problematic files to reveal exactly *why* rows are broken or why data types are mixed, helping us design the perfect cleaning strategy.\n",
    "\n",
    "**What it does (Step-by-Step):**\n",
    "\n",
    "1.  **Structural Investigator (\"Broken Row Finder\"):**\n",
    "    * **Target:** Files flagged with broken rows (e.g., `responsible_parties.txt`, `facilities.txt`).\n",
    "    * **Action:** It reads the specific lines where column counts don't match the header.\n",
    "    * **Output:** It extracts the raw text of these broken lines so we can see the root cause (e.g., a pipe `|` character hidden inside a job title like `\"Doctor | Professor\"`).\n",
    "    * **Benefit:** Confirms if we need to change our loading parameters (e.g., enabling `quotechar='\"'`).\n",
    "\n",
    "2.  **Mixed Type Investigator (\"Hidden String Finder\"):**\n",
    "    * **Target:** Numeric columns that contain text (e.g., `param_value` in `baseline_measurements.txt`).\n",
    "    * **Action:** It tries to convert the column to numbers and isolates the values that fail.\n",
    "    * **Output:** It lists the most frequent non-numeric patterns.\n",
    "        * *Example Findings:* It distinguishes between fixable issues (like `\"10,366\"` or `\"- 4.5\"`) vs. complex data (like `\"18-35\"` ranges) vs. pure garbage (like `\"units\"`).\n",
    "    * **Benefit:** Tells us exactly which cleaning functions to write (e.g., \"strip commas\", \"remove spaces\").\n",
    "\n",
    "3.  **Unparseable Date Finder:**\n",
    "    * **Target:** Date columns with errors (e.g., `anticipated_posting_date`).\n",
    "    * **Action:** It finds values that fail standard datetime conversion or fall outside a reasonable year range (1900-2030).\n",
    "    * **Output:** Shows typos like `\"3333-12-01\"` (placeholder) or `\"1018-04-10\"` (likely 2018).\n",
    "    * **Benefit:** Allows us to build \"Smart Fix\" logic to correct typos instead of deleting them.\n",
    "\n",
    "4.  **Reporting:**\n",
    "    * It saves a summary log to `deep_dive_log.txt`.\n",
    "    * It saves detailed CSV samples of the bad data to the `00_data_issues/` folder for manual inspection.\n",
    "\n",
    "**Why use it?**\n",
    "While the first script tells us *that* there is a problem, this script tells us *what* the problem is. It provides the evidence needed to write the final `clean_aact_final.py` script with confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaec725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DEEP DIVE DIAGNOSTIC based on 'data_quality_report.txt'\n",
      "\n",
      "--- INVESTIGATION: Broken Rows in 'responsible_parties.txt' ---\n",
      "  Header (8 cols): id|nct_id|responsible_party_type|name|title|organization|affiliation|old_name_title...\n",
      "  Found 5+ broken lines. Examples:\n",
      "    Line 4767 (Cols 9): 144841503|NCT03862352|PRINCIPAL_INVESTIGATOR|Dr Thomas J Ford|\"Clinical Research Fellow | Honorary Fellow (Interventional Cardiology)\"||NHS National Waiting Times Centre Board|\n",
      "    Line 30129 (Cols 9): 144851281|NCT02476669|PRINCIPAL_INVESTIGATOR|Dr. Victor H.F. Lee|\"Clinical Assistant |Professor\"||The University of Hong Kong|\n",
      "    Line 43702 (Cols 9): 144859094|NCT07108062|PRINCIPAL_INVESTIGATOR|Mohamed Abdelaziz Emam|\"Lecturer assistant at of Basic Sciences Department , Faculty of Physical Therapy, Kafrelsheikh Univerisity, Kafrelsheikh 33511, Egy...\n",
      "    Line 49365 (Cols 9): 144862701|NCT05772689|PRINCIPAL_INVESTIGATOR|Fayron Epps|\"Associate Professor | Director of Community & Research Engagement\"||Emory University|\n",
      "    Line 62050 (Cols 9): 144531334|NCT05992272|PRINCIPAL_INVESTIGATOR|Andreas Heinz|\"head of Department of Psychiatry and Psychotherapy | CCM, principle investigator\"||Charite University, Berlin, Germany|\n",
      "  [Saved] Full bad row details to: 00_data_issues/broken_rows_responsible_parties.csv\n",
      "\n",
      "--- INVESTIGATION: Broken Rows in 'facilities.txt' ---\n",
      "  Header (10 cols): id|nct_id|status|name|city|state|zip|country|latitude|longitude...\n",
      "  Found 5+ broken lines. Examples:\n",
      "    Line 613 (Cols 11): 886855724|NCT05537350||\"Eurofins | CRL, Inc.\"|Piscataway|New Jersey|08854|United States|40.499270|-74.399040\n",
      "    Line 1373 (Cols 11): 889516250|NCT06408090||\"University of Colorado Denver | Anschutz Medical Campus\"|Aurora|Colorado|80045|United States|39.729430|-104.831920\n",
      "    Line 2307 (Cols 11): 886856998|NCT04614246||\"Eastern Virginia Medical School | OB/GYN Clinical Research Center\"|Norfolk|Virginia|23507|United States|36.846810|-76.285220\n",
      "    Line 2328 (Cols 11): 886857014|NCT04614246||\"The Ottawa Hospital | The Ottawa Hospital Research Institute - Neurology - Ottawa Stroke Program\"|Ottawa|Ontario|K1Y4E9|Canada|45.411170|-75.698120\n",
      "    Line 2359 (Cols 11): 886857045|NCT04614246||\"University General Hospital of Patras | Univ Obs & Gynae Cli\"|P√°trai||26504|Greece|38.246200|21.735080\n",
      "  [Saved] Full bad row details to: 00_data_issues/broken_rows_facilities.csv\n",
      "\n",
      "--- INVESTIGATION: Broken Rows in 'studies.txt' ---\n",
      "  Header (71 cols): nct_id|nlm_download_date_description|study_first_submitted_date|results_first_submitted_date|disposi...\n",
      "  Found 5+ broken lines. Examples:\n",
      "    Line 25973 (Cols 72): NCT06989580||2025-04-17|||2025-09-05|2025-05-23|2025-05-25|ACTUAL|||||||2025-09-05|2025-09-08|ESTIMATED|2025-08-22|ACTUAL|2025-08-22|2025-04|2025-04-30|2028-12-31|ESTIMATED|2028-12-31|2028-12-31|ESTIM...\n",
      "    Line 26917 (Cols 72): NCT05033210||2021-09-01|||2022-07-21|2021-09-01|2021-09-02|ACTUAL|||||||2022-07-21|2022-07-25|ACTUAL|2022-03-01|ACTUAL|2022-03-01|2022-07|2022-07-31|2023-10|ESTIMATED|2023-10-31|2023-03|ESTIMATED|2023...\n",
      "    Line 71531 (Cols 73): NCT05610176||2022-11-02|2025-04-28||2025-07-23|2022-11-02|2022-11-09|ACTUAL|2025-07-23|2025-07-24|ACTUAL||||2025-07-23|2025-07-24|ACTUAL|2022-11-23|ACTUAL|2022-11-23|2024-06|2024-06-30|2024-06-11|ACTU...\n",
      "    Line 72332 (Cols 73): NCT04466501||2020-07-01|||2020-07-07|2020-07-07|2020-07-10|ACTUAL|||||||2020-07-07|2020-07-10|ACTUAL|2019-06-30|ACTUAL|2019-06-30|2020-07|2020-07-31|2019-09-30|ACTUAL|2019-09-30|2019-09-30|ACTUAL|2019...\n",
      "    Line 91121 (Cols 72): NCT04159272||2019-11-06|||2022-09-27|2019-11-08|2019-11-12|ACTUAL|||||||2022-09-27|2022-09-28|ACTUAL|2019-11-04|ACTUAL|2019-11-04|2022-09|2022-09-30|2022-06-03|ACTUAL|2022-06-03|2022-06-03|ACTUAL|2022...\n",
      "  [Saved] Full bad row details to: 00_data_issues/broken_rows_studies.csv\n",
      "\n",
      "--- INVESTIGATION: Broken Rows in 'outcome_measurements.txt' ---\n",
      "  Header (21 cols): id|nct_id|outcome_id|result_group_id|ctgov_group_code|classification|category|title|description|unit...\n",
      "  Found 5+ broken lines. Examples:\n",
      "    Line 2206 (Cols 22): 1203740690|NCT02625974|161944644|517807062|OG000|\"Visit 1|Positive\"||Part 1 - Number of Subjects With Positive Results in Concentration Test for T. Cruzi (for Subjects <8 Months of Age)||Participants|...\n",
      "    Line 2207 (Cols 22): 1203740691|NCT02625974|161944644|517807063|OG001|\"Visit 1|Positive\"||Part 1 - Number of Subjects With Positive Results in Concentration Test for T. Cruzi (for Subjects <8 Months of Age)||Participants|...\n",
      "    Line 2209 (Cols 22): 1203740692|NCT02625974|161944644|517807062|OG000|\"Visit 3|Positive\"||Part 1 - Number of Subjects With Positive Results in Concentration Test for T. Cruzi (for Subjects <8 Months of Age)||Participants|...\n",
      "    Line 2210 (Cols 22): 1203740693|NCT02625974|161944644|517807063|OG001|\"Visit 3|Positive\"||Part 1 - Number of Subjects With Positive Results in Concentration Test for T. Cruzi (for Subjects <8 Months of Age)||Participants|...\n",
      "    Line 2214 (Cols 22): 1203740694|NCT02625974|161944644|517807062|OG000|\"Visit 6|Positive\"||Part 1 - Number of Subjects With Positive Results in Concentration Test for T. Cruzi (for Subjects <8 Months of Age)||Participants|...\n",
      "  [Saved] Full bad row details to: 00_data_issues/broken_rows_outcome_measurements.csv\n",
      "\n",
      "--- INVESTIGATION: Mixed Types in 'baseline_measurements.txt' (Col: param_value) ---\n",
      "  Found 728 non-numeric values in 'param_value'.\n",
      "  Top non-numeric patterns:\n",
      "param_value\n",
      "15,581              4\n",
      "units on a scale    4\n",
      "4,158               3\n",
      "70,000              3\n",
      "4,141               3\n",
      "22,162              3\n",
      "22,405              3\n",
      "1,553               2\n",
      "1,272               2\n",
      "6,900               2\n",
      "  [Saved] Sample of bad values to: 00_data_issues/mixed_types_baseline_measurements_param_value.csv\n",
      "\n",
      "--- INVESTIGATION: Mixed Types in 'baseline_measurements.txt' (Col: dispersion_value) ---\n",
      "  Found 416 non-numeric values in 'dispersion_value'.\n",
      "  Top non-numeric patterns:\n",
      "dispersion_value\n",
      "na              14\n",
      "18-35            3\n",
      "Not reported     3\n",
      "42, 58           2\n",
      "24.1-70.4        2\n",
      "18-36            2\n",
      "1-79             2\n",
      "1,305.66         1\n",
      "3,600.63         1\n",
      "8,975.05         1\n",
      "  [Saved] Sample of bad values to: 00_data_issues/mixed_types_baseline_measurements_dispersion_value.csv\n",
      "\n",
      "--- INVESTIGATION: Mixed Types in 'outcome_measurements.txt' (Col: dispersion_value) ---\n",
      "  Found 1379 non-numeric values in 'dispersion_value'.\n",
      "  Top non-numeric patterns:\n",
      "dispersion_value\n",
      "Standard Deviation                                                                                                                                            71\n",
      "Full Range                                                                                                                                                    52\n",
      " 3 = Very useful. For both ease-of-use and usefulness items, scores range from 0 to 3 with higher scores indicating a better opinion of the app features.\"    34\n",
      " I am not at all confident leaving my home because of my lung condition, I sleep soundly \\                                                                    30\n",
      "Standard Error                                                                                                                                                19\n",
      "95% Confidence Interval                                                                                                                                       16\n",
      "na                                                                                                                                                            14\n",
      " + \\                                                                                                                                                          12\n",
      "Inter-Quartile Range                                                                                                                                          11\n",
      "GEOMETRIC_MEAN                                                                                                                                                 9\n",
      "  [Saved] Sample of bad values to: 00_data_issues/mixed_types_outcome_measurements_dispersion_value.csv\n",
      "\n",
      "--- INVESTIGATION: Mixed Types in 'facility_contacts.txt' (Col: phone_extension) ---\n",
      "  Found 1349 non-numeric values in 'phone_extension'.\n",
      "  Top non-numeric patterns:\n",
      "phone_extension\n",
      "TTY dial 711      261\n",
      "TTY8664111010      95\n",
      "+ 33               23\n",
      "00 33              14\n",
      "+33 (0)             9\n",
      "Ôºà022Ôºâ27435027       6\n",
      "027-85351607        5\n",
      "010-69156043        5\n",
      "86-20-87343117      5\n",
      "TR                  5\n",
      "  [Saved] Sample of bad values to: 00_data_issues/mixed_types_facility_contacts_phone_extension.csv\n",
      "\n",
      "--- INVESTIGATION: Weird Dates in 'outcomes.txt' (Col: anticipated_posting_date) ---\n",
      "  Found 79 unparseable dates.\n",
      "  Examples: ['Weeks 4, 8 and 12 after last treatment' 'LW\\\\' 'R\\\\_P1-N1\\\\'\n",
      " 'R\\\\_N1-P1\\\\' '3333-12-01'\n",
      " 'Vulvar dermatosis was diagnosed in 82 patients by vulvoscopy and 72 patients by histopathology.'\n",
      " '\"Absent Vulvar Dermatosis was diagnosed in 246 patients by vulvoscopy and 256 patients by histopathology.\"'\n",
      " 'Aceto-Whitening Test, up to 10 minutes for each participant.' '10 hours'\n",
      " 'During anesthesia from the time instant when the BIS drops below 60 for the first time and remains there for the subsequent 30 seconds to the point at which the automatic control is turned off (maintenance duration), an average of 2 hours.']\n",
      "\n",
      "--- INVESTIGATION: Weird Dates in 'provided_documents.txt' (Col: document_date) ---\n",
      "  Found 2 unparseable dates.\n",
      "  Examples: ['1018-04-10' '1014-01-24']\n",
      "\n",
      "[DONE] Diagnostic log saved to: deep_dive_log.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"00_data\"\n",
    "OUTPUT_DIR = \"00_data_issues\"  # We will save error samples here\n",
    "LOG_FILE = \"deep_dive_log.txt\"\n",
    "DELIMITER = \"|\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Buffer for the log file\n",
    "log_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Prints to console and saves to buffer.\"\"\"\n",
    "    print(message)\n",
    "    log_buffer.append(message)\n",
    "\n",
    "def save_log():\n",
    "    \"\"\"Writes the full log to a file.\"\"\"\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(log_buffer))\n",
    "    print(f\"\\n[DONE] Diagnostic log saved to: {LOG_FILE}\")\n",
    "\n",
    "# --- 1. STRUCTURAL INVESTIGATOR (The \"Broken Row\" Finder) ---\n",
    "def investigate_broken_rows(filename, problematic_lines):\n",
    "    \"\"\"\n",
    "    Reads specific line numbers to see WHY they are broken.\n",
    "    problematic_lines: list of integers (0-indexed line numbers)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    log(f\"\\n--- INVESTIGATION: Broken Rows in '{filename}' ---\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        log(f\"  [Error] File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            header = f.readline().strip()\n",
    "            expected_cols = header.count(DELIMITER) + 1\n",
    "            log(f\"  Header ({expected_cols} cols): {header[:100]}...\")\n",
    "\n",
    "            # We want to peek at specific lines. Reading sequentially is safest for text files.\n",
    "            current_line = 0 # Header is usually considered line 0 or we start counting after\n",
    "\n",
    "            # Convert target lines to a set for fast lookup.\n",
    "            # Note: User report said \"Line 4767\". In Python enumerate (0-indexed),\n",
    "            # if line 1 is header, Line 4767 is index 4766. We will look around that area.\n",
    "            # We will scan the whole file and check column counts again to be precise.\n",
    "\n",
    "            bad_samples = []\n",
    "\n",
    "            # Reset file pointer to start\n",
    "            f.seek(0)\n",
    "\n",
    "            for i, line in enumerate(f):\n",
    "                cols = line.count(DELIMITER) + 1\n",
    "                if cols != expected_cols:\n",
    "                    # Save the bad line for analysis\n",
    "                    clean_line = line.strip()\n",
    "                    bad_samples.append({\n",
    "                        \"line_number\": i + 1,\n",
    "                        \"actual_cols\": cols,\n",
    "                        \"expected_cols\": expected_cols,\n",
    "                        \"content\": clean_line[:200] + \"...\" if len(clean_line) > 200 else clean_line\n",
    "                    })\n",
    "                    if len(bad_samples) >= 5: # Just get first 5 examples\n",
    "                        break\n",
    "\n",
    "            if bad_samples:\n",
    "                log(f\"  Found {len(bad_samples)}+ broken lines. Examples:\")\n",
    "                for b in bad_samples:\n",
    "                    log(f\"    Line {b['line_number']} (Cols {b['actual_cols']}): {b['content']}\")\n",
    "\n",
    "                # Save to CSV for user to inspect\n",
    "                sample_df = pd.DataFrame(bad_samples)\n",
    "                sample_path = os.path.join(OUTPUT_DIR, f\"broken_rows_{filename.replace('.txt', '.csv')}\")\n",
    "                sample_df.to_csv(sample_path, index=False)\n",
    "                log(f\"  [Saved] Full bad row details to: {sample_path}\")\n",
    "            else:\n",
    "                log(\"  [Info] No broken rows found (maybe they were fixed?).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"  [Error] Failed to read file: {e}\")\n",
    "\n",
    "# --- 2. MIXED TYPE INVESTIGATOR (The \"Hidden String\" Finder) ---\n",
    "def investigate_mixed_types(filename, col_name):\n",
    "    \"\"\"\n",
    "    Loads a specific column and finds values that are NOT numbers.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    log(f\"\\n--- INVESTIGATION: Mixed Types in '{filename}' (Col: {col_name}) ---\")\n",
    "\n",
    "    try:\n",
    "        # Load only the specific column to save memory\n",
    "        # We use strict settings to catch the garbage\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=DELIMITER,\n",
    "            usecols=[col_name],\n",
    "            dtype=str, # Read as string first\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "            on_bad_lines='skip' # Skip broken rows for this check\n",
    "        )\n",
    "\n",
    "        # Try to convert to numeric\n",
    "        numeric = pd.to_numeric(df[col_name], errors='coerce')\n",
    "\n",
    "        # Find rows where conversion failed (Result is NaN) BUT original was NOT empty\n",
    "        mask_bad = numeric.isna() & df[col_name].notna()\n",
    "        bad_values = df[mask_bad]\n",
    "\n",
    "        if not bad_values.empty:\n",
    "            count = len(bad_values)\n",
    "            log(f\"  Found {count} non-numeric values in '{col_name}'.\")\n",
    "\n",
    "            # Get top frequent garbage (e.g., \"10,000\" vs \"unknown\")\n",
    "            top_garbage = bad_values[col_name].value_counts().head(10)\n",
    "            log(f\"  Top non-numeric patterns:\\n{top_garbage.to_string()}\")\n",
    "\n",
    "            # Save specific file\n",
    "            out_name = f\"mixed_types_{filename.replace('.txt', '')}_{col_name}.csv\"\n",
    "            bad_values.head(100).to_csv(os.path.join(OUTPUT_DIR, out_name), index=False)\n",
    "            log(f\"  [Saved] Sample of bad values to: {OUTPUT_DIR}/{out_name}\")\n",
    "        else:\n",
    "            log(\"  [OK] Column appears cleanly numeric (or empty).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"  [Error] Could not check mixed types: {e}\")\n",
    "\n",
    "# --- 3. UNPARSEABLE DATE FINDER ---\n",
    "def investigate_bad_dates(filename, col_name):\n",
    "    \"\"\"\n",
    "    Finds dates like '3333-12-01' or 'Pending'.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    log(f\"\\n--- INVESTIGATION: Weird Dates in '{filename}' (Col: {col_name}) ---\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=DELIMITER,\n",
    "            usecols=[col_name],\n",
    "            dtype=str,\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "\n",
    "        # Try to convert\n",
    "        dates = pd.to_datetime(df[col_name], errors='coerce')\n",
    "\n",
    "        # Find failures\n",
    "        mask_bad = dates.isna() & df[col_name].notna()\n",
    "        bad_values = df[mask_bad]\n",
    "\n",
    "        if not bad_values.empty:\n",
    "            log(f\"  Found {len(bad_values)} unparseable dates.\")\n",
    "            log(f\"  Examples: {bad_values[col_name].unique()[:10]}\")\n",
    "        else:\n",
    "            # Check for logical outliers (e.g. year 3000)\n",
    "            # We filter for valid dates that are way in the future\n",
    "            valid_dates = dates.dropna()\n",
    "            future_mask = valid_dates > pd.Timestamp(\"2030-01-01\")\n",
    "            weird_future = valid_dates[future_mask]\n",
    "\n",
    "            if not weird_future.empty:\n",
    "                log(f\"  Found {len(weird_future)} dates far in the future (Logical Errors).\")\n",
    "                log(f\"  Examples: {weird_future.head().astype(str).tolist()}\")\n",
    "            else:\n",
    "                log(\"  [OK] Dates look technically valid.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"  [Error] Checking dates: {e}\")\n",
    "\n",
    "\n",
    "# --- EXECUTION: TARGETING THE ISSUES FROM YOUR REPORT ---\n",
    "\n",
    "log(\"STARTING DEEP DIVE DIAGNOSTIC based on 'data_quality_report.txt'\")\n",
    "\n",
    "# 1. Structure Issues (Broken Rows)\n",
    "# responsible_parties.txt had 31 broken rows\n",
    "investigate_broken_rows(\"responsible_parties.txt\", [])\n",
    "# facilities.txt had 1448 broken rows (Major issue)\n",
    "investigate_broken_rows(\"facilities.txt\", [])\n",
    "# studies.txt had 55 broken rows (Critical table)\n",
    "investigate_broken_rows(\"studies.txt\", [])\n",
    "# outcome_measurements.txt had 704 broken rows\n",
    "investigate_broken_rows(\"outcome_measurements.txt\", [])\n",
    "\n",
    "# 2. Mixed Data Types (Numbers vs Text)\n",
    "# baseline_measurements.txt -> param_value (text like '10,366')\n",
    "investigate_mixed_types(\"baseline_measurements.txt\", \"param_value\")\n",
    "investigate_mixed_types(\"baseline_measurements.txt\", \"dispersion_value\")\n",
    "\n",
    "# outcome_measurements.txt -> dispersion_value\n",
    "investigate_mixed_types(\"outcome_measurements.txt\", \"dispersion_value\")\n",
    "\n",
    "# facility_contacts.txt -> phone_extension (text like 'extension 1')\n",
    "investigate_mixed_types(\"facility_contacts.txt\", \"phone_extension\")\n",
    "\n",
    "# 3. Date Issues\n",
    "# outcomes.txt -> anticipated_posting_date (Found '3333-12-01')\n",
    "investigate_bad_dates(\"outcomes.txt\", \"anticipated_posting_date\")\n",
    "# provided_documents.txt -> document_date (Found '1018-04-10')\n",
    "investigate_bad_dates(\"provided_documents.txt\", \"document_date\")\n",
    "\n",
    "save_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a38de",
   "metadata": {},
   "source": [
    "# Numeric Scale Check Script: Explained\n",
    "\n",
    "**Purpose:** This script acts as a diagnostic tool to understand the true nature of \"mixed\" numeric columns before we clean them. It specifically helps us decide whether percentages like `57.5%` should be treated as `57.5` (0-100 scale) or `0.575` (0-1 scale).\n",
    "\n",
    "**What it does (Step-by-Step):**\n",
    "\n",
    "1.  **Loads Specific Columns:** It reads only the target column (e.g., `param_value`) from the specified file, using safe loading parameters to avoid crashes.\n",
    "\n",
    "2.  **Isolates Pure Numbers:**\n",
    "    * It uses regex to find rows that contain *only* digits and decimals (e.g., `12.5`, `100`).\n",
    "    * It calculates statistics (Min, Max, Mean) on these pure numbers to establish a \"baseline scale.\"\n",
    "    * *Example:* If the max pure number is `10,000`, the scale is clearly not 0-1.\n",
    "\n",
    "3.  **Isolates Percentages:**\n",
    "    * It finds rows containing the `%` symbol.\n",
    "    * It strips the symbol and converts the remaining text to numbers to see their range.\n",
    "    * *Example:* `57.5%` -> `57.5`.\n",
    "\n",
    "4.  **Logic & Verdict:**\n",
    "    * It compares the \"Pure Number\" scale vs. the \"Percentage\" scale.\n",
    "    * **Verdict Logic:**\n",
    "        * If pure numbers are small (0-1) and percentages are large (0-100), it suggests we should **DIVIDE** by 100.\n",
    "        * If pure numbers are large (>1) and percentages are large (0-100), it suggests we should just **STRIP** the symbol.\n",
    "\n",
    "**Why use it?**\n",
    "Blindly stripping `%` or blindly dividing by 100 can corrupt data. This script gives us the mathematical proof needed to choose the correct cleaning strategy for our final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c05ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Scale: outcome_measurements.txt [dispersion_upper_limit_raw] ---\n",
      "  Pure Number Stats (Sample size: 759279):\n",
      "    Min: -5860000.0\n",
      "    Max: 785070000000000.0\n",
      "    Mean: 2021412878.24\n",
      "  Found 11 rows with '%'.\n",
      "    Examples: ['57.5%', '48%', '9.8%', '100%', '99.4%']\n",
      "    If we just strip '%', the range is: 0.7 to 100.0\n",
      "\n",
      "  [VERDICT] Raw numbers are likely 0-100 (Integers). You should STRIP % (Don't divide).\n",
      "\n",
      "--- Checking Scale: outcome_measurements.txt [param_value] ---\n",
      "  Pure Number Stats (Sample size: 4553587):\n",
      "    Min: -408000000.0\n",
      "    Max: 875769282253977.0\n",
      "    Mean: 423983377.92\n",
      "  No '%' signs found in this column.\n",
      "\n",
      "--- Checking Scale: baseline_measurements.txt [param_value] ---\n",
      "  Pure Number Stats (Sample size: 2712770):\n",
      "    Min: -663.0\n",
      "    Max: 13920879614.0\n",
      "    Mean: 13802.03\n",
      "  No '%' signs found in this column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"00_data\"\n",
    "# Using your proven loading params\n",
    "LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,\n",
    "    \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "def check_numeric_scale(filename, col_name):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path): return\n",
    "\n",
    "    print(f\"\\n--- Checking Scale: {filename} [{col_name}] ---\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, usecols=[col_name], **LOAD_PARAMS)\n",
    "\n",
    "        # 1. Isolate Pure Numbers\n",
    "        # (Rows that are just digits and dots, no text/symbols)\n",
    "        pure_numbers = df[col_name][df[col_name].astype(str).str.match(r'^-?\\d+\\.?\\d*$')]\n",
    "        pure_floats = pd.to_numeric(pure_numbers)\n",
    "\n",
    "        if pure_floats.empty:\n",
    "            print(\"  No pure numbers found to compare against.\")\n",
    "            return\n",
    "\n",
    "        print(f\"  Pure Number Stats (Sample size: {len(pure_floats)}):\")\n",
    "        print(f\"    Min: {pure_floats.min()}\")\n",
    "        print(f\"    Max: {pure_floats.max()}\")\n",
    "        print(f\"    Mean: {pure_floats.mean():.2f}\")\n",
    "\n",
    "        # 2. Isolate Percentage Strings\n",
    "        percent_strings = df[col_name][df[col_name].astype(str).str.contains(r'%')]\n",
    "\n",
    "        if percent_strings.empty:\n",
    "            print(\"  No '%' signs found in this column.\")\n",
    "        else:\n",
    "            print(f\"  Found {len(percent_strings)} rows with '%'.\")\n",
    "            print(f\"    Examples: {percent_strings.head().tolist()}\")\n",
    "\n",
    "            # Logic Check\n",
    "            clean_percents = pd.to_numeric(percent_strings.str.replace('%', ''), errors='coerce')\n",
    "            print(f\"    If we just strip '%', the range is: {clean_percents.min()} to {clean_percents.max()}\")\n",
    "\n",
    "            # Decision Helper\n",
    "            if pure_floats.max() <= 1.0 and clean_percents.mean() > 1.0:\n",
    "                print(\"\\n  [VERDICT] Raw numbers are likely 0-1 (Decimals). You SHOULD divide % by 100.\")\n",
    "            elif pure_floats.mean() > 1.0:\n",
    "                print(\"\\n  [VERDICT] Raw numbers are likely 0-100 (Integers). You should STRIP % (Don't divide).\")\n",
    "            else:\n",
    "                print(\"\\n  [VERDICT] Scale is ambiguous. Manual review needed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Check the problematic columns identified in logs\n",
    "check_numeric_scale(\"outcome_measurements.txt\", \"dispersion_upper_limit_raw\")\n",
    "check_numeric_scale(\"outcome_measurements.txt\", \"param_value\")\n",
    "check_numeric_scale(\"baseline_measurements.txt\", \"param_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089298b0",
   "metadata": {},
   "source": [
    "# üè≠ Master Data Conversion Pipeline: Explained\n",
    "\n",
    "**Purpose:** This is the final \"production\" script for data preparation. It processes every single file in our raw data folder (`00_data`) and converts them into a standardized, clean format ready for Machine Learning.\n",
    "\n",
    "**What it does (Step-by-Step):**\n",
    "\n",
    "1.  **Iterates through EVERYTHING:** It loops through all 50+ `.txt` files in the source folder. This ensures no part of the database is left behind in an old format.\n",
    "\n",
    "2.  **Correct Loading (The \"Safe Fix\"):**\n",
    "    * It applies the proven structural fix (`quotechar='\"'`) to every file.\n",
    "    * This guarantees that files with \"pipes inside quotes\" (like `facilities.txt`) are loaded correctly without breaking rows.\n",
    "\n",
    "3.  **Surgical Cleaning (The \"Smart Logic\"):**\n",
    "    * It checks a **Special Rules Registry**.\n",
    "    * If a file is known to be \"dirty\" (e.g., `baseline_measurements.txt`), it applies specific cleaning functions:\n",
    "        * **Numbers:** Removes commas, spaces, and `%` signs to create pure numbers.\n",
    "        * **Dates:** Fixes typos (e.g., `1018` -> `2018`) and filters impossible years.\n",
    "    * If a file is \"standard,\" it skips deep cleaning to preserve data integrity.\n",
    "\n",
    "4.  **Standard Hygiene:**\n",
    "    * For *every* file, it trims invisible whitespace from text columns (e.g., `\" Diabetes \"` -> `\"Diabetes\"`).\n",
    "\n",
    "5.  **Standard Export:**\n",
    "    * It saves every file as a **Pipe-Delimited Text File (`.txt`)**.\n",
    "    * **Crucial Feature:** It uses `quoting=csv.QUOTE_MINIMAL`. This ensures that if any field contains a pipe `|`, it is wrapped in quotes so it doesn't break the file structure again.\n",
    "\n",
    "**Why use it?**\n",
    "This script gives us a **single, unified folder (`00_data_ml_ready`)** where every file is structurally sound, clean, and formatted exactly the same way. This is the \"Gold Standard\" dataset for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a1c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch Cleaning from '00_data' to '00_data_ml_ready'...\n",
      "\n",
      "Processing: baseline_counts.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/baseline_counts.txt (224320 rows)\n",
      "Processing: baseline_measurements.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/baseline_measurements.txt (2720829 rows)\n",
      "Processing: brief_summaries.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/brief_summaries.txt (558028 rows)\n",
      "Processing: browse_conditions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/browse_conditions.txt (4084129 rows)\n",
      "Processing: browse_interventions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/browse_interventions.txt (2432357 rows)\n",
      "Processing: calculated_values.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/calculated_values.txt (558973 rows)\n",
      "Processing: central_contacts.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/central_contacts.txt (211164 rows)\n",
      "Processing: conditions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/conditions.txt (989606 rows)\n",
      "Processing: countries.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/countries.txt (763540 rows)\n",
      "Processing: design_group_interventions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/design_group_interventions.txt (1245646 rows)\n",
      "Processing: design_groups.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/design_groups.txt (1023177 rows)\n",
      "Processing: design_outcomes.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/design_outcomes.txt (3445054 rows)\n",
      "Processing: designs.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/designs.txt (554264 rows)\n",
      "Processing: detailed_descriptions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/detailed_descriptions.txt (558028 rows)\n",
      "Processing: documents.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/documents.txt (10616 rows)\n",
      "Processing: drop_withdrawals.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/drop_withdrawals.txt (563769 rows)\n",
      "Processing: eligibilities.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/eligibilities.txt (558028 rows)\n",
      "Processing: facilities.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/facilities.txt (3343041 rows)\n",
      "Processing: facility_contacts.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/facility_contacts.txt (403227 rows)\n",
      "Processing: facility_investigators.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/facility_investigators.txt (222908 rows)\n",
      "Processing: id_information.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/id_information.txt (727353 rows)\n",
      "Processing: intervention_other_names.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/intervention_other_names.txt (461546 rows)\n",
      "Processing: interventions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/interventions.txt (945857 rows)\n",
      "Processing: ipd_information_types.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/ipd_information_types.txt (82497 rows)\n",
      "Processing: keywords.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/keywords.txt (1466970 rows)\n",
      "Processing: links.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/links.txt (72834 rows)\n",
      "Processing: milestones.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/milestones.txt (833469 rows)\n",
      "Processing: outcome_analyses.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/outcome_analyses.txt (307261 rows)\n",
      "Processing: outcome_analysis_groups.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/outcome_analysis_groups.txt (594886 rows)\n",
      "Processing: outcome_counts.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/outcome_counts.txt (1494835 rows)\n",
      "Processing: outcome_measurements.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/outcome_measurements.txt (4614173 rows)\n",
      "Processing: outcomes.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/outcomes.txt (620396 rows)\n",
      "Processing: overall_officials.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/overall_officials.txt (510534 rows)\n",
      "Processing: participant_flows.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/participant_flows.txt (75138 rows)\n",
      "Processing: pending_results.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/pending_results.txt (31628 rows)\n",
      "Processing: provided_documents.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/provided_documents.txt (73568 rows)\n",
      "Processing: responsible_parties.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/responsible_parties.txt (540493 rows)\n",
      "Processing: result_agreements.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/result_agreements.txt (75138 rows)\n",
      "Processing: result_contacts.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/result_contacts.txt (75138 rows)\n",
      "Processing: result_groups.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/result_groups.txt (2063029 rows)\n",
      "Processing: retractions.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/retractions.txt (333 rows)\n",
      "Processing: search_results.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/search_results.txt (0 rows)\n",
      "Processing: search_term_results.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/search_term_results.txt (481884 rows)\n",
      "Processing: search_terms.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/search_terms.txt (186 rows)\n",
      "Processing: sponsors.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/sponsors.txt (894384 rows)\n",
      "Processing: studies.txt... [Special Cleaning Applied] -> Saved to 00_data_ml_ready/studies.txt (558973 rows)\n",
      "Processing: study_references.txt... [Standard Cleaning] -> Saved to 00_data_ml_ready/study_references.txt (1043738 rows)\n",
      "\n",
      "[DONE] 50 files cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"00_data\"\n",
    "OUTPUT_DIR = \"00_data_ml_ready\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. STRUCTURAL SETTINGS (Proven Correct)\n",
    "AACT_LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,                 # Read all as string first\n",
    "    \"quotechar\": '\"',             # Handles pipes inside quotes\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "# --- CLEANING FUNCTIONS (Matching v3 Logic) ---\n",
    "def clean_numeric_column(series):\n",
    "    \"\"\"\n",
    "    Surgical Cleaning:\n",
    "    - Removes spaces, commas, <, >\n",
    "    - Strips % (Keeps 0-100 scale)\n",
    "    - Returns floats (or NaN for text/ranges)\n",
    "    \"\"\"\n",
    "    clean = series.str.replace(r'\\s+', '', regex=True)\n",
    "    clean = clean.str.replace(r'[,<>]', '', regex=True)\n",
    "    clean = clean.str.replace('%', '')\n",
    "    return pd.to_numeric(clean, errors='coerce')\n",
    "\n",
    "def clean_date_column(series):\n",
    "    \"\"\"\n",
    "    Smart Date Cleaning:\n",
    "    - Fixes '10xx' typos (1018 -> 2018)\n",
    "    - Filters 1900-2100 (Keeps future trials)\n",
    "    \"\"\"\n",
    "    series = series.astype(str).str.replace(r'^10(\\d{2}-\\d{2}-\\d{2})', r'20\\1', regex=True)\n",
    "    dates = pd.to_datetime(series, errors='coerce')\n",
    "    mask_valid = (dates.dt.year >= 1900) & (dates.dt.year <= 2100)\n",
    "    return dates.where(mask_valid, pd.NaT)\n",
    "\n",
    "# --- SPECIAL RULES REGISTRY ---\n",
    "# Files NOT in this list will get a standard \"Safe Conversion\"\n",
    "SPECIAL_CONFIG = {\n",
    "    \"studies.txt\": {\n",
    "        \"date_cols\": [\"study_first_submitted_date\", \"results_first_submitted_date\", \"completion_date\", \"start_date\"]\n",
    "    },\n",
    "    \"baseline_measurements.txt\": {\n",
    "        \"numeric_cols\": [\"param_value\", \"dispersion_value\"]\n",
    "    },\n",
    "    \"outcome_measurements.txt\": {\n",
    "        \"numeric_cols\": [\"param_value\", \"dispersion_value\", \"dispersion_upper_limit_raw\", \"dispersion_lower_limit_raw\"]\n",
    "    },\n",
    "    \"outcome_analyses.txt\": {\n",
    "        \"numeric_cols\": [\"ci_upper_limit_raw\", \"ci_lower_limit_raw\", \"p_value\"]\n",
    "    },\n",
    "    \"outcomes.txt\": {\n",
    "        \"date_cols\": [\"anticipated_posting_date\"]\n",
    "    },\n",
    "    \"provided_documents.txt\": {\n",
    "        \"date_cols\": [\"document_date\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def process_file(filename):\n",
    "    in_path = os.path.join(DATA_DIR, filename)\n",
    "    # OUTPUT CHANGE: Saving as .txt to match original format\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    # Skip if not a text file or if it's a report file\n",
    "    if not filename.endswith(\".txt\") or \"report\" in filename: return\n",
    "\n",
    "    print(f\"Processing: {filename}...\", end=\" \")\n",
    "\n",
    "    try:\n",
    "        # 1. LOAD (Standard Safe Load)\n",
    "        df = pd.read_csv(in_path, **AACT_LOAD_PARAMS)\n",
    "\n",
    "        # 2. STANDARD HYGIENE (Apply to ALL files)\n",
    "        # Strip invisible whitespace from all text columns\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in obj_cols:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "        # 3. SPECIAL CLEANING (Only for specific files)\n",
    "        if filename in SPECIAL_CONFIG:\n",
    "            rules = SPECIAL_CONFIG[filename]\n",
    "\n",
    "            # Apply Numeric Rules\n",
    "            for col in rules.get(\"numeric_cols\", []):\n",
    "                if col in df.columns:\n",
    "                    df[col] = clean_numeric_column(df[col])\n",
    "\n",
    "            # Apply Date Rules\n",
    "            for col in rules.get(\"date_cols\", []):\n",
    "                if col in df.columns:\n",
    "                    df[col] = clean_date_column(df[col])\n",
    "\n",
    "            print(f\"[Special Cleaning Applied] -> \", end=\"\")\n",
    "        else:\n",
    "            print(f\"[Standard Cleaning] -> \", end=\"\")\n",
    "\n",
    "        # 4. EXPORT (Clean Pipe-Delimited TXT)\n",
    "        # sep='|' keeps it as a pipe file.\n",
    "        # quoting=csv.QUOTE_MINIMAL is crucial: it puts quotes around fields\n",
    "        # that contain pipes (fixing the original structural bug).\n",
    "        df.to_csv(out_path, index=False, sep='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        print(f\"Saved to {out_path} ({len(df)} rows)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  [ERROR] Failed: {e}\")\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "print(f\"Starting Batch Cleaning from '{DATA_DIR}' to '{OUTPUT_DIR}'...\\n\")\n",
    "\n",
    "files = sorted(os.listdir(DATA_DIR))\n",
    "txt_files = [f for f in files if f.endswith(\".txt\")]\n",
    "\n",
    "for i, filename in enumerate(txt_files):\n",
    "    process_file(filename)\n",
    "\n",
    "print(f\"\\n[DONE] {len(txt_files)} files cleaned and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec82451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
