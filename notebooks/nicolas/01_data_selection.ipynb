{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e610104d",
   "metadata": {},
   "source": [
    "# Clinical Trial Risk Engine: Data Engineering Pipeline\n",
    "\n",
    "**Project:** Premature Termination Risk Prediction\n",
    "**Output Artifact:** `project_data.csv`\n",
    "**Scope:** Interventional Drug Trials (Phases 1, 2, and 3).\n",
    "**Temporal Filter:** Trials starting between 2000 and the present (plus 2 years).\n",
    "\n",
    "## Data Dictionary and Feature Rationale\n",
    "\n",
    "This dataset aggregates data from the AACT (Aggregate Analysis of ClinicalTrials.gov) database to predict trial termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be47c1e",
   "metadata": {},
   "source": [
    "### 1. Target Variable\n",
    "*   **`target`** (Binary): The classification target.\n",
    "    *   `0`: **Completed**. The trial concluded according to protocol.\n",
    "    *   `1`: **Failed**. The trial was Terminated, Withdrawn, or Suspended.\n",
    "*   **`overall_status`** (String): The raw status label. *Note: This column is retained for validation but must be dropped prior to training to prevent data leakage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faa5bd",
   "metadata": {},
   "source": [
    "### 2. Operational Features (Complexity Proxies)\n",
    "*   **`num_facilities`** (Integer): The count of distinct recruiting sites. Higher counts correlate with increased operational complexity and cost.\n",
    "*   **`num_countries`** (Integer): The count of unique countries involved. Indicates regulatory complexity (e.g., FDA, EMA, PMDA coordination).\n",
    "*   **`phase_ordinal`** (Float): A numeric mapping of the trial phase (Phase 1=1.0, Phase 3=3.0). This is the primary proxy for trial magnitude and resource requirements.\n",
    "*   **`number_of_arms`** (Integer): The number of intervention groups. Indicates protocol complexity.\n",
    "*   **`start_year`** (Integer): The year the trial began. Used to capture temporal trends in clinical research standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cbe6b",
   "metadata": {},
   "source": [
    "### 3. Eligibility and Protocol Features\n",
    "*   **`criteria`** (Text): The full inclusion and exclusion criteria. This unstructured text contains high-value signals regarding protocol restrictiveness.\n",
    "*   **`gender`** (Categorical): Indicates if the trial is restricted by sex (Female, Male, or All).\n",
    "*   **`healthy_volunteers`** (Binary): Indicates if healthy participants are accepted. Often distinguishes Phase 1 safety trials (Yes) from Phase 2/3 efficacy trials (No).\n",
    "*   **`adult`, `child`, `older_adult`** (Binary): Pre-calculated flags indicating the target age groups. Used in place of raw numeric age limits to reduce noise and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8870908",
   "metadata": {},
   "source": [
    "### 4. Scientific and Design Features\n",
    "*   **`therapeutic_area`** (Categorical): High-level medical classification (e.g., Oncology, Cardiology).\n",
    "*   **`therapeutic_subgroup_name`** (Categorical): Granular disease classification (e.g., Neoplasms by Site).\n",
    "*   **`intervention_model`** (Categorical): The strategy for assigning interventions (e.g., Parallel, Crossover, Single Group).\n",
    "*   **`masking`** (Categorical): The level of blinding (e.g., Double, Quadruple, None).\n",
    "*   **`allocation`** (Categorical): Indicates if participants are randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9dc0c",
   "metadata": {},
   "source": [
    "### 5. Environmental Context (Competition)\n",
    "*   **`competition_niche`** (Integer): The count of concurrent trials with the *same* Phase and *same* Therapeutic Subgroup. Represents direct competition for specific patient populations.\n",
    "*   **`competition_broad`** (Integer): The count of concurrent trials within the *same* Therapeutic Area. Represents broader resource saturation.\n",
    "*   **`covid_exposure`** (Binary): Indicates if the trial was active during the 2019-2021 global disruption window.\n",
    "\n",
    "### 6. Sponsor Information\n",
    "*   **`agency_class`** (Categorical): The type of lead sponsor (Industry, NIH, Other). Industry trials typically have different risk profiles compared to academic or government-funded studies.\n",
    "*   **`includes_us`** (Binary): Indicates if the trial has at least one site in the United States, implying FDA regulatory oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e3b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473b2012",
   "metadata": {},
   "source": [
    "### 1. Configuration and Path Management\n",
    "This block establishes the environment settings. It implements a robust path-finding logic to ensure the code executes correctly whether run from the project root or a subdirectory (e.g., Jupyter Notebooks). It also defines safe loading parameters to handle special characters in the raw AACT text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a72c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DATA_PATH set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      ">>> Setup Complete. Ready to process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & ROBUST PATH SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Dynamic Path Finding: Looks for the 'data' folder relative to where you are\n",
    "if os.path.exists(\"data\"):\n",
    "    DATA_PATH = \"data\"                        # Scenario: Running from Project Root\n",
    "elif os.path.exists(\"../../data\"):\n",
    "    DATA_PATH = \"../../data\"                  # Scenario: Running from notebooks/nicolas\n",
    "elif os.path.exists(\"../data\"):\n",
    "    DATA_PATH = \"../data\"                     # Scenario: Running from notebooks/\n",
    "else:\n",
    "    # Fallback: Absolute path (Only used if the above fail)\n",
    "    DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "print(f\">>> DATA_PATH set to: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "OUTPUT_FILE = 'project_data.csv'\n",
    "\n",
    "# ROBUST LOADING PARAMETERS\n",
    "AACT_LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,\n",
    "    \"header\": 0,\n",
    "    \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "print(\">>> Setup Complete. Ready to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86173d",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Cohort Filtering\n",
    "This step defines the study cohort. We apply strict inclusion and exclusion criteria to ensure data quality:\n",
    "1.  **Study Type:** Retain only `INTERVENTIONAL` trials.\n",
    "2.  **Intervention:** Retain only `DRUG` or `BIOLOGICAL` trials.\n",
    "3.  **Status:** Retain only definitive outcomes (`COMPLETED`, `TERMINATED`, `WITHDRAWN`, `SUSPENDED`).\n",
    "4.  **Phase:** Exclude Phase 0 and Phase 4 to focus on the core development pipeline.\n",
    "5.  **Temporal Validity:** Filter for trials starting between 2000 and the near future, removing invalid dates (e.g., 1900) and placeholder future records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b261a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Studies & Applying Filters...\n",
      "   - Core Cohort Size (Phases 1-3, Years 2000-2027): 119201 trials\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. THE FUNNEL: LOADING & FILTERING (Updated with Year Filter)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Loading Studies & Applying Filters...\")\n",
    "\n",
    "# A. Load Studies\n",
    "cols_studies = [\n",
    "    'nct_id', 'overall_status', 'study_type', 'phase',\n",
    "    'start_date', 'start_date_type',\n",
    "    'number_of_arms', 'official_title', 'why_stopped'\n",
    "]\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'studies.txt'), usecols=cols_studies, **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Filter: Interventional Only\n",
    "df = df[df['study_type'] == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "# C. Filter: Drugs Only\n",
    "df_int = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **AACT_LOAD_PARAMS)\n",
    "drug_ids = df_int[df_int['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]['nct_id'].unique()\n",
    "df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "# D. Filter: Closed Statuses Only\n",
    "allowed_statuses = ['COMPLETED', 'TERMINATED', 'WITHDRAWN', 'SUSPENDED']\n",
    "df = df[df['overall_status'].isin(allowed_statuses)]\n",
    "\n",
    "# E. Filter: Exclude Phase 0 and Phase 4 (Refined Scope)\n",
    "excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA']\n",
    "df = df[~df['phase'].isin(excluded_phases)]\n",
    "\n",
    "# F. Create Target & Fix Dates\n",
    "df['target'] = df['overall_status'].apply(lambda x: 0 if x == 'COMPLETED' else 1)\n",
    "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "df['start_year'] = df['start_date'].dt.year\n",
    "\n",
    "# --- NEW FILTER: VALID YEARS ONLY ---\n",
    "# Drop 1900 (Errors) and Future Dates (Invalid for training)\n",
    "current_year = pd.Timestamp.now().year\n",
    "df = df[df['start_year'].between(2000, current_year + 2)]\n",
    "\n",
    "print(f\"   - Core Cohort Size (Phases 1-3, Years 2000-{current_year+2}): {len(df)} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9029d8",
   "metadata": {},
   "source": [
    "### 3. Medical Hierarchy Integration\n",
    "This block enriches the dataset with standardized medical classifications. By mapping `nct_id` to the MeSH (Medical Subject Headings) hierarchy, we derive:\n",
    "*   **`therapeutic_area`:** The broad medical category.\n",
    "*   **`therapeutic_subgroup`:** The specific disease family.\n",
    "This hierarchical approach allows the model to learn risk patterns associated with specific medical fields (e.g., Oncology vs. Infectious Diseases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd03ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Attaching Medical Hierarchy...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. MEDICAL HIERARCHY & SUBGROUPS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Attaching Medical Hierarchy...\")\n",
    "\n",
    "# A. Load Smart Lookup (Best Term per Trial)\n",
    "df_smart = pd.read_csv(os.path.join(DATA_PATH, 'smart_pathology_lookup.csv'))\n",
    "df = df.merge(df_smart, on='nct_id', how='left')\n",
    "\n",
    "# B. Fill Missing\n",
    "df['therapeutic_area'] = df['therapeutic_area'].fillna('Other/Unclassified')\n",
    "df['best_pathology'] = df['best_pathology'].fillna('Unknown')\n",
    "\n",
    "# C. Create Subgroup Code (Level 2 Hierarchy)\n",
    "df['therapeutic_subgroup'] = df['tree_number'].astype(str).apply(\n",
    "    lambda x: x[:7] if pd.notna(x) and len(x) >= 7 else 'Unknown'\n",
    ")\n",
    "\n",
    "# D. Map Subgroup Code to Name\n",
    "df_lookup = pd.read_csv(os.path.join(DATA_PATH, 'mesh_lookup.csv'), sep='|')\n",
    "code_to_name = pd.Series(df_lookup.mesh_term.values, index=df_lookup.tree_number).to_dict()\n",
    "df['therapeutic_subgroup_name'] = df['therapeutic_subgroup'].map(code_to_name).fillna('Unknown Subgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37499ac",
   "metadata": {},
   "source": [
    "### 4. Competition Intensity Calculation\n",
    "This block calculates \"Crowding\" metrics to quantify the competitive environment. We define competition using a 3-year rolling window (Start Year, +1, +2).\n",
    "*   **`competition_broad`:** Measures saturation within the general therapeutic area.\n",
    "*   **`competition_niche`:** Measures direct competition for the specific patient population (same Subgroup and Phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e0fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Calculating Competition Intensity (Dual Level)...\n",
      "   - Created 'competition_broad' and 'competition_niche'\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. DUAL-LEVEL CROWDING (Niche vs Broad)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Calculating Competition Intensity (Dual Level)...\")\n",
    "\n",
    "# A. Standardize Phase for Grouping\n",
    "phase_group_map = {\n",
    "    'PHASE1': 'PHASE1', 'PHASE1/PHASE2': 'PHASE2',\n",
    "    'PHASE2': 'PHASE2', 'PHASE2/PHASE3': 'PHASE3', 'PHASE3': 'PHASE3'\n",
    "}\n",
    "df['phase_group'] = df['phase'].map(phase_group_map).fillna('UNKNOWN')\n",
    "\n",
    "# --- LEVEL 1: BROAD COMPETITION (Area + Phase) ---\n",
    "grid_broad = df.groupby(['start_year', 'therapeutic_area', 'phase_group']).size().reset_index(name='count')\n",
    "dict_broad = dict(zip(zip(grid_broad['start_year'], grid_broad['therapeutic_area'], grid_broad['phase_group']), grid_broad['count']))\n",
    "\n",
    "def get_broad_crowding(row):\n",
    "    y, area, ph = row['start_year'], row['therapeutic_area'], row['phase_group']\n",
    "    if pd.isna(y): return 0\n",
    "    return dict_broad.get((y, area, ph), 0) + dict_broad.get((y+1, area, ph), 0) + dict_broad.get((y+2, area, ph), 0)\n",
    "\n",
    "df['competition_broad'] = df.apply(get_broad_crowding, axis=1)\n",
    "\n",
    "# --- LEVEL 2: NICHE COMPETITION (Subgroup + Phase) ---\n",
    "grid_niche = df.groupby(['start_year', 'therapeutic_subgroup', 'phase_group']).size().reset_index(name='count')\n",
    "dict_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup'], grid_niche['phase_group']), grid_niche['count']))\n",
    "\n",
    "def get_niche_crowding(row):\n",
    "    y, sub, ph = row['start_year'], row['therapeutic_subgroup'], row['phase_group']\n",
    "    if pd.isna(y) or sub == 'Unknown': return 0\n",
    "    return dict_niche.get((y, sub, ph), 0) + dict_niche.get((y+1, sub, ph), 0) + dict_niche.get((y+2, sub, ph), 0)\n",
    "\n",
    "df['competition_niche'] = df.apply(get_niche_crowding, axis=1)\n",
    "\n",
    "df.drop(columns=['phase_group'], inplace=True)\n",
    "print(\"   - Created 'competition_broad' and 'competition_niche'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862785a4",
   "metadata": {},
   "source": [
    "### 5. Protocol Details and Eligibility\n",
    "This block extracts key protocol design features.\n",
    "*   **Eligibility Flags:** We incorporate categorical flags (`gender`, `healthy_volunteers`, `adult`, `child`, `older_adult`) to characterize the study population.\n",
    "*   **Text Data:** We extract the full `criteria` text block for downstream Natural Language Processing (NLP).\n",
    "*   **Endpoints:** We calculate the number of primary endpoints as a proxy for scientific complexity.\n",
    "*   **Analysis Data:** We extract `min_p_value` for post-hoc analysis (excluded from training to prevent leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc1c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Extracting Eligibility & Endpoints...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. PROTOCOL DETAILS (Eligibility, Endpoints) - UPDATED\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Extracting Eligibility & Endpoints...\")\n",
    "\n",
    "# A. Load Eligibility Fields (No Age Parsing)\n",
    "# We rely on the pre-calculated flags (adult/child/older_adult) instead of parsing numbers.\n",
    "cols_elig = [\n",
    "    'nct_id',\n",
    "    'criteria',\n",
    "    'gender', 'healthy_volunteers',\n",
    "    'adult', 'child', 'older_adult'\n",
    "]\n",
    "\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'),\n",
    "                      usecols=cols_elig,\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Merge into Main DataFrame\n",
    "df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "# C. Endpoint Counts\n",
    "df_calc = pd.read_csv(os.path.join(DATA_PATH, 'calculated_values.txt'),\n",
    "                      usecols=['nct_id', 'number_of_primary_outcomes_to_measure'],\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "df = df.merge(df_calc, on='nct_id', how='left')\n",
    "df['num_primary_endpoints'] = pd.to_numeric(df['number_of_primary_outcomes_to_measure'], errors='coerce').fillna(1)\n",
    "\n",
    "# D. P-Values (Analysis Only)\n",
    "df_outcomes = pd.read_csv(os.path.join(DATA_PATH, 'outcomes.txt'), usecols=['id', 'nct_id', 'outcome_type'], **AACT_LOAD_PARAMS)\n",
    "prim_ids = df_outcomes[df_outcomes['outcome_type'] == 'PRIMARY']['id'].unique()\n",
    "\n",
    "df_an = pd.read_csv(os.path.join(DATA_PATH, 'outcome_analyses.txt'), usecols=['outcome_id', 'p_value'], **AACT_LOAD_PARAMS)\n",
    "df_an = df_an[df_an['outcome_id'].isin(prim_ids)]\n",
    "df_an['p_value_num'] = pd.to_numeric(df_an['p_value'], errors='coerce')\n",
    "\n",
    "min_p = df_an.groupby('outcome_id')['p_value_num'].min().reset_index()\n",
    "min_p = min_p.merge(df_outcomes[['id', 'nct_id']], left_on='outcome_id', right_on='id')\n",
    "trial_p = min_p.groupby('nct_id')['p_value_num'].min().reset_index(name='min_p_value')\n",
    "\n",
    "df = df.merge(trial_p, on='nct_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f895009",
   "metadata": {},
   "source": [
    "### 6. Operational Proxies and Sponsor Data\n",
    "This block merges operational and administrative features:\n",
    "*   **Phase Mapping:** Converts text phases to a numeric ordinal scale (1.0–3.0). Invalid phases are filtered out.\n",
    "*   **Geography:** Calculates the number of facilities and countries, and flags US-based trials.\n",
    "*   **Sponsor:** Identifies the lead agency class (e.g., Industry vs. Other).\n",
    "*   **External Factors:** Calculates `covid_exposure` based on the trial's start date relative to the pandemic window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "836335e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Merging Operational Features & Calculating COVID Exposure...\n",
      "   - Created 'is_international' and 'includes_us' flags.\n",
      "   - Dropped leaky counts (facilities/countries).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6. OPERATIONAL PROXIES, SPONSORS & EXTERNAL FACTORS (Updated)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Merging Operational Features & Calculating COVID Exposure...\")\n",
    "\n",
    "# A. Phase Ordinal\n",
    "phase_map = {'PHASE1': 1, 'PHASE1/PHASE2': 1.5, 'PHASE2': 2, 'PHASE2/PHASE3': 2.5, 'PHASE3': 3}\n",
    "df['phase_ordinal'] = df['phase'].map(phase_map).fillna(0)\n",
    "df = df[df['phase_ordinal'] > 0] # Drop unknown phases\n",
    "\n",
    "# B. COVID Exposure\n",
    "df['covid_exposure'] = df['start_year'].between(2019, 2021).astype(int)\n",
    "\n",
    "# C. Geography (International Flag + US Flag)\n",
    "# Logic:\n",
    "# 1. International = Sites in >1 unique country.\n",
    "# 2. Includes US = 'United States' is listed as a location.\n",
    "df_countries = pd.read_csv(os.path.join(DATA_PATH, 'countries.txt'), usecols=['nct_id', 'name'], **AACT_LOAD_PARAMS)\n",
    "\n",
    "country_stats = df_countries.groupby('nct_id')['name'].agg(\n",
    "    cnt='nunique',\n",
    "    includes_us=lambda x: 1 if 'United States' in x.values else 0\n",
    ").reset_index()\n",
    "\n",
    "df = df.merge(country_stats, on='nct_id', how='left')\n",
    "\n",
    "# Create the flags\n",
    "df['is_international'] = (df['cnt'] > 1).astype(int)\n",
    "df['includes_us'] = df['includes_us'].fillna(0).astype(int)\n",
    "\n",
    "# Drop the raw count (leakage prevention)\n",
    "df.drop(columns=['cnt'], inplace=True)\n",
    "\n",
    "# D. Sponsors & Design\n",
    "df_sponsors = pd.read_csv(os.path.join(DATA_PATH, 'sponsors.txt'), **AACT_LOAD_PARAMS)\n",
    "df_lead = df_sponsors[df_sponsors['lead_or_collaborator'] == 'lead'][['nct_id', 'agency_class']].drop_duplicates('nct_id')\n",
    "df = df.merge(df_lead, on='nct_id', how='left')\n",
    "\n",
    "cols_des = ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose']\n",
    "df_des = pd.read_csv(os.path.join(DATA_PATH, 'designs.txt'), usecols=cols_des, **AACT_LOAD_PARAMS)\n",
    "df = df.merge(df_des, on='nct_id', how='left')\n",
    "\n",
    "print(\"   - Created 'is_international' and 'includes_us' flags.\")\n",
    "print(\"   - Dropped leaky counts (facilities/countries).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810d7c",
   "metadata": {},
   "source": [
    "### 7. Text Integration and Data Export\n",
    "This final processing step merges the remaining unstructured text fields (`brief_summary`, `detailed_description`) and cleans up temporary technical columns. The final dataset is exported as `project_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116f7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Engineering Text Features...\n",
      "   - Creating 'txt_tags' (Title + Keywords + Drug Names + Int. Desc)...\n",
      "   - Creating 'txt_criteria' (Inclusion/Exclusion Rules only)...\n",
      "\\n>>> SUCCESS: Final Dataset saved to project_data.csv\n",
      "    Rows: 105884\n",
      "    Columns: 32\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. TEXT FEATURE ENGINEERING (\"Tags\" vs \"Complexity\") - REFINED FOR SHAP\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Engineering Text Features...\")\n",
    "\n",
    "# A. Load Components\n",
    "# 1. Keywords\n",
    "df_keys = pd.read_csv(os.path.join(DATA_PATH, 'keywords.txt'), usecols=['nct_id', 'name'], **AACT_LOAD_PARAMS)\n",
    "keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "\n",
    "# 2. Intervention Details\n",
    "df_int_det = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'),\n",
    "                         usecols=['nct_id', 'intervention_type', 'name', 'description'],\n",
    "                         **AACT_LOAD_PARAMS)\n",
    "# Filter for drugs only\n",
    "df_int_det = df_int_det[df_int_det['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]\n",
    "\n",
    "int_names = df_int_det.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_int_names')\n",
    "int_desc = df_int_det.groupby('nct_id')['description'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_int_desc')\n",
    "\n",
    "# B. Merge Components\n",
    "df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "df = df.merge(int_names, on='nct_id', how='left')\n",
    "df = df.merge(int_desc, on='nct_id', how='left')\n",
    "\n",
    "# Fill NaNs before combining\n",
    "text_cols = ['official_title', 'txt_keywords', 'txt_int_names', 'criteria', 'txt_int_desc']\n",
    "df[text_cols] = df[text_cols].fillna(\"\")\n",
    "\n",
    "# C. Create Final Features (Separated for Explainability)\n",
    "\n",
    "# 1. TAGS (The \"What\") -> TF-IDF\n",
    "# We include Intervention Description here because it contains factual keywords (e.g., \"Intravenous\")\n",
    "print(\"   - Creating 'txt_tags' (Title + Keywords + Drug Names + Int. Desc)...\")\n",
    "df['txt_tags'] = (\n",
    "    df['official_title'] + \" | \" +\n",
    "    df['txt_keywords'] + \" | \" +\n",
    "    df['txt_int_names'] + \" | \" +\n",
    "    df['txt_int_desc']\n",
    ")\n",
    "\n",
    "# 2. COMPLEXITY (The \"How Hard\") -> BERT\n",
    "# We keep Criteria ISOLATED. This allows SHAP to specifically blame \"Strict Criteria\" for failure.\n",
    "print(\"   - Creating 'txt_criteria' (Inclusion/Exclusion Rules only)...\")\n",
    "df['txt_criteria'] = df['criteria']\n",
    "\n",
    "# D. Cleanup\n",
    "cols_to_drop = [\n",
    "    'start_date', 'start_date_type', 'tree_number', 'number_of_primary_outcomes_to_measure',\n",
    "    'official_title', 'txt_keywords', 'txt_int_names', 'criteria', 'txt_int_desc'\n",
    "]\n",
    "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# E. Save\n",
    "df.to_csv(os.path.join(DATA_PATH, OUTPUT_FILE), index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "print(f\"\\\\n>>> SUCCESS: Final Dataset saved to {OUTPUT_FILE}\")\n",
    "print(f\"    Rows: {len(df)}\")\n",
    "print(f\"    Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fff20cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nct_id', 'study_type', 'overall_status', 'phase', 'number_of_arms',\n",
       "       'why_stopped', 'target', 'start_year', 'best_pathology',\n",
       "       'therapeutic_area', 'therapeutic_subgroup', 'therapeutic_subgroup_name',\n",
       "       'competition_broad', 'competition_niche', 'gender',\n",
       "       'healthy_volunteers', 'adult', 'child', 'older_adult',\n",
       "       'num_primary_endpoints', 'min_p_value', 'phase_ordinal',\n",
       "       'covid_exposure', 'includes_us', 'is_international', 'agency_class',\n",
       "       'allocation', 'intervention_model', 'primary_purpose', 'masking',\n",
       "       'txt_tags', 'txt_criteria'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf3d97",
   "metadata": {},
   "source": [
    "### 8. Quality Assurance and Encoding Strategy\n",
    "This block performs a comprehensive audit of the final dataset. It analyzes missing values, cardinality, and data types to generate an automated **Encoding Strategy Report**. This report provides specific recommendations for the machine learning pipeline (e.g., which fields require One-Hot Encoding, Target Encoding, or NLP transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e61e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> STARTING PIPELINE BLUEPRINT AUDIT ON: project_data.csv...\n",
      "SUCCESS: Pipeline Blueprint saved to audit_dataset_report.txt\n",
      "Check the 'Skew' and 'Dominant Cat' columns to confirm your strategy.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. FINAL DATASET AUDIT & PIPELINE BLUEPRINT\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FILE = 'project_data.csv'\n",
    "OUTPUT_REPORT = 'audit_dataset_report.txt'\n",
    "\n",
    "# Define the Strategy Logic\n",
    "STRATEGY_MAP = {\n",
    "    \"TARGET\": {\n",
    "        \"columns\": [\"target\"],\n",
    "        \"encoding\": \"Label (None)\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Outcome variable (0=Completed, 1=Failed).\"\n",
    "    },\n",
    "    \"NUMERIC_SKEWED\": {\n",
    "        \"columns\": [\"number_of_arms\", \"start_year\", \"competition_niche\", \"competition_broad\"],\n",
    "        \"encoding\": \"Numeric (Passthrough)\",\n",
    "        \"scaling\": \"Log1p + StandardScaler\",\n",
    "        \"why\": \"High Skewness expected. Log1p compresses outliers; Scaler normalizes range.\"\n",
    "    },\n",
    "    \"ORDINAL\": {\n",
    "        \"columns\": [\"phase_ordinal\"],\n",
    "        \"encoding\": \"Numeric (Passthrough)\",\n",
    "        \"scaling\": \"MinMax (Optional) or None\",\n",
    "        \"why\": \"Ordinal nature (1 < 2 < 3). Preserving magnitude is important.\"\n",
    "    },\n",
    "    \"CATEGORICAL_BINARY\": {\n",
    "        \"columns\": [\"includes_us\", \"is_international\", \"covid_exposure\", \"healthy_volunteers\", \"adult\", \"child\", \"older_adult\"],\n",
    "        \"encoding\": \"OneHotEncoder (drop='if_binary')\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Already Boolean. Dropping one column prevents multicollinearity.\"\n",
    "    },\n",
    "    \"CATEGORICAL_NOMINAL\": {\n",
    "        \"columns\": [\"gender\", \"agency_class\", \"masking\", \"allocation\", \"intervention_model\", \"primary_purpose\", \"therapeutic_area\"],\n",
    "        \"encoding\": \"OneHotEncoder (handle_unknown='ignore')\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Low cardinality (<50). One-Hot is interpretable for SHAP.\"\n",
    "    },\n",
    "    \"CATEGORICAL_HIGH_CARD\": {\n",
    "        \"columns\": [\"therapeutic_subgroup_name\", \"best_pathology\"],\n",
    "        \"encoding\": \"TargetEncoder\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"High cardinality (>50). One-Hot would create sparse matrices. Target Encoding captures risk probability.\"\n",
    "    },\n",
    "    \"TEXT_TAGS\": {\n",
    "        \"columns\": [\"txt_tags\"],\n",
    "        \"encoding\": \"TF-IDF Vectorizer (Top 50)\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Bag-of-words approach. Identifies specific topics (e.g., 'Placebo', 'Oncology').\"\n",
    "    },\n",
    "    \"TEXT_COMPLEXITY\": {\n",
    "        \"columns\": [\"txt_criteria\"],\n",
    "        \"encoding\": \"BERT Embeddings (Stream B)\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Semantic complexity. BERT captures the difficulty of the protocol rules.\"\n",
    "    },\n",
    "    \"EXCLUDED\": {\n",
    "        \"columns\": [\"overall_status\", \"min_p_value\", \"why_stopped\", \"nct_id\"],\n",
    "        \"encoding\": \"DROP\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"why\": \"Data Leakage (Future Info) or ID columns.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_blueprint_audit():\n",
    "    print(f\">>> STARTING PIPELINE BLUEPRINT AUDIT ON: {INPUT_FILE}...\")\n",
    "    file_path = os.path.join(DATA_PATH, INPUT_FILE)\n",
    "    if not os.path.exists(file_path): return\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    with open(os.path.join(DATA_PATH, OUTPUT_REPORT), 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"PIPELINE BLUEPRINT & DATA AUDIT\\n\")\n",
    "        f.write(f\"Rows: {len(df):,}\\nCols: {len(df.columns)}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for group, meta in STRATEGY_MAP.items():\n",
    "            f.write(f\"\\n[{group}]\\n\")\n",
    "            f.write(f\"  > Strategy:  {meta['encoding']} | {meta['scaling']}\\n\")\n",
    "            f.write(f\"  > Rationale: {meta['why']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(f\"  {'COLUMN NAME':<30} | {'MISSING':<8} | {'STATS / DISTRIBUTION'}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "            for col in meta['columns']:\n",
    "                if col not in df.columns:\n",
    "                    f.write(f\"  ❌ {col:<28} | NOT FOUND\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Calculate Stats\n",
    "                missing = df[col].isna().sum()\n",
    "                missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "                # 1. TEXT STATS\n",
    "                if \"TEXT\" in group:\n",
    "                    avg_words = df[col].astype(str).apply(lambda x: len(x.split())).mean()\n",
    "                    empty_rows = (df[col].fillna(\"\").astype(str).str.strip() == \"\").sum()\n",
    "                    stat_str = f\"Avg Words: {avg_words:.0f} | Empty Rows: {empty_rows}\"\n",
    "\n",
    "                # 2. NUMERIC STATS\n",
    "                elif group in [\"NUMERIC_SKEWED\", \"ORDINAL\"]:\n",
    "                    stats = df[col].describe()\n",
    "                    skew = df[col].skew()\n",
    "                    zeros = (df[col] == 0).sum()\n",
    "                    zero_pct = (zeros / len(df)) * 100\n",
    "                    stat_str = f\"Mean: {stats['mean']:.1f} | Max: {stats['max']:.0f} | Skew: {skew:.2f} | Zeros: {zero_pct:.1f}%\"\n",
    "\n",
    "                # 3. CATEGORICAL STATS\n",
    "                else:\n",
    "                    unique = df[col].nunique()\n",
    "                    # Check for Dominance (if one category is >90% of data)\n",
    "                    if unique > 0:\n",
    "                        top_cat_pct = df[col].value_counts(normalize=True).iloc[0] * 100\n",
    "                        stat_str = f\"Unique: {unique:<4} | Dominant Cat: {top_cat_pct:.1f}%\"\n",
    "                    else:\n",
    "                        stat_str = \"Empty\"\n",
    "\n",
    "                f.write(f\"  • {col:<28} | {missing_pct:>6.1f}% | {stat_str}\\n\")\n",
    "\n",
    "    print(f\"SUCCESS: Pipeline Blueprint saved to {OUTPUT_REPORT}\")\n",
    "    print(\"Check the 'Skew' and 'Dominant Cat' columns to confirm your strategy.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_blueprint_audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e49f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
