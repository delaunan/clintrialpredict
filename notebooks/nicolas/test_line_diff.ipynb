{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec038a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading STRICT version (with start_date_type)...\n",
      ">>> Loading LENIENT version (without start_date_type)...\n",
      "\n",
      "Strict Count (Notebook): 426907\n",
      "Lenient Count (Old Python): 426907\n",
      "\n",
      ">>> Found 0 rows that are being dropped in the Notebook/Strict version.\n",
      "No difference found! The datasets are identical.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "# Update this path to your actual data folder\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,\n",
    "    \"header\": 0,\n",
    "    \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\" # This is the key: it drops bad lines\n",
    "}\n",
    "\n",
    "def get_studies(include_problem_col=True):\n",
    "    \"\"\"\n",
    "    Loads studies.txt.\n",
    "    If include_problem_col=True, it mimics the Notebook (Strict).\n",
    "    If include_problem_col=False, it mimics the Old Python Script (Lenient).\n",
    "    \"\"\"\n",
    "    cols = [\n",
    "        'nct_id', 'overall_status', 'study_type', 'phase',\n",
    "        'start_date', 'number_of_arms', 'official_title', 'why_stopped'\n",
    "    ]\n",
    "\n",
    "    if include_problem_col:\n",
    "        cols.append('start_date_type') # The \"Bad Line\" Trigger\n",
    "        print(\">>> Loading STRICT version (with start_date_type)...\")\n",
    "    else:\n",
    "        print(\">>> Loading LENIENT version (without start_date_type)...\")\n",
    "\n",
    "    path = os.path.join(DATA_PATH, 'studies.txt')\n",
    "    df = pd.read_csv(path, usecols=cols, **LOAD_PARAMS)\n",
    "\n",
    "    # Apply Basic Filter (Interventional) to match your pipeline scope\n",
    "    df = df[df['study_type'] == 'INTERVENTIONAL']\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1. Load Both Versions\n",
    "df_strict = get_studies(include_problem_col=True)\n",
    "df_lenient = get_studies(include_problem_col=False)\n",
    "\n",
    "print(f\"\\nStrict Count (Notebook): {len(df_strict)}\")\n",
    "print(f\"Lenient Count (Old Python): {len(df_lenient)}\")\n",
    "\n",
    "# 2. Identify the \"Lost\" Rows\n",
    "# These are rows that exist in Lenient but were dropped in Strict\n",
    "strict_ids = set(df_strict['nct_id'])\n",
    "lost_rows = df_lenient[~df_lenient['nct_id'].isin(strict_ids)].copy()\n",
    "\n",
    "print(f\"\\n>>> Found {len(lost_rows)} rows that are being dropped in the Notebook/Strict version.\")\n",
    "\n",
    "# 3. Analyze the Lost Rows\n",
    "if len(lost_rows) > 0:\n",
    "    print(\"\\n--- ANALYSIS OF LOST ROWS ---\")\n",
    "\n",
    "    # A. Status Distribution\n",
    "    print(\"\\n1. Status Distribution of Lost Rows:\")\n",
    "    print(lost_rows['overall_status'].value_counts().head())\n",
    "\n",
    "    # B. Date Distribution\n",
    "    lost_rows['start_date'] = pd.to_datetime(lost_rows['start_date'], errors='coerce')\n",
    "    print(\"\\n2. Start Year Distribution:\")\n",
    "    print(lost_rows['start_date'].dt.year.value_counts().sort_index().head(5))\n",
    "    print(\"...\")\n",
    "    print(lost_rows['start_date'].dt.year.value_counts().sort_index().tail(5))\n",
    "\n",
    "    # C. Visual Inspection (The \"Smell Test\")\n",
    "    print(\"\\n3. Sample of Lost Rows (Check 'official_title' for corruption):\")\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    print(lost_rows[['nct_id', 'start_date', 'overall_status', 'official_title']].head(10))\n",
    "\n",
    "    # D. Check for obvious corruption\n",
    "    # Often bad lines have titles that look like they were cut off or merged\n",
    "    print(\"\\n4. Checking for Suspicious Titles (containing pipes '|' or quotes):\")\n",
    "    suspicious = lost_rows[lost_rows['official_title'].str.contains(r'\\||\"', regex=True, na=False)]\n",
    "    print(f\"   Found {len(suspicious)} rows with suspicious characters in the title.\")\n",
    "    if not suspicious.empty:\n",
    "        print(suspicious[['nct_id', 'official_title']].head(3))\n",
    "\n",
    "else:\n",
    "    print(\"No difference found! The datasets are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f7b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Name:    945857\n",
      "Without Name: 945857\n",
      ">>> CONFIRMED: No rows lost in interventions.txt. The difference was definitely the Year Filter.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "LOAD_PARAMS = {\"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"', \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"}\n",
    "\n",
    "# 1. Load With Name (Old Python Style)\n",
    "df_strict = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type', 'name'], **LOAD_PARAMS)\n",
    "\n",
    "# 2. Load Without Name (Notebook/New Python Style)\n",
    "df_lenient = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **LOAD_PARAMS)\n",
    "\n",
    "print(f\"With Name:    {len(df_strict)}\")\n",
    "print(f\"Without Name: {len(df_lenient)}\")\n",
    "\n",
    "if len(df_strict) == len(df_lenient):\n",
    "    print(\">>> CONFIRMED: No rows lost in interventions.txt. The difference was definitely the Year Filter.\")\n",
    "else:\n",
    "    print(f\">>> FOUND DIFFERENCE: {len(df_lenient) - len(df_strict)} rows dropped due to bad names.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
