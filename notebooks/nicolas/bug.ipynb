{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f5aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTIC REPORT ---\n",
      "Data Source: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "\n",
      "FILE                 | METHOD             | STATUS          | ROW COUNT\n",
      "----------------------------------------------------------------------\n",
      "studies.txt          | RISKY (Original)   | Check           | 558973\n",
      "studies.txt          | ROBUST (New)       | Check           | 558973\n",
      "                     | DIFFERENCE         | LOSS            | -0 rows\n",
      "----------------------------------------------------------------------\n",
      "interventions.txt    | RISKY (Original)   | Check           | 945857\n",
      "interventions.txt    | ROBUST (New)       | Check           | 945857\n",
      "                     | DIFFERENCE         | LOSS            | -0 rows\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "# Define the two configurations we want to test\n",
    "CONFIGS = {\n",
    "    \"RISKY (Original)\": {\n",
    "        \"sep\": \"|\",\n",
    "        \"quoting\": csv.QUOTE_MINIMAL,  # The setting that caused the crash\n",
    "        \"on_bad_lines\": \"warn\",        # Default behavior (might be 'error' in old pandas)\n",
    "        \"low_memory\": False\n",
    "    },\n",
    "    \"ROBUST (New)\": {\n",
    "        \"sep\": \"|\",\n",
    "        \"quoting\": 3,                  # csv.QUOTE_NONE (The Fix)\n",
    "        \"on_bad_lines\": \"warn\",\n",
    "        \"low_memory\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "def count_rows(filepath, params, name):\n",
    "    \"\"\"Attempts to load a file and counts rows. Returns count or 'CRASHED'.\"\"\"\n",
    "    try:\n",
    "        # We only load 1 column to make it fast, just to check parsing\n",
    "        # We read the first column usually 'nct_id'\n",
    "        df = pd.read_csv(filepath, usecols=[0], **params)\n",
    "        return len(df)\n",
    "    except Exception as e:\n",
    "        return f\"CRASHED ({str(e)[:50]}...)\"\n",
    "\n",
    "def run_diagnostic():\n",
    "    print(f\"--- DIAGNOSTIC REPORT ---\")\n",
    "    print(f\"Data Source: {DATA_PATH}\\n\")\n",
    "\n",
    "    files_to_test = ['studies.txt', 'interventions.txt']\n",
    "\n",
    "    # Header for the table\n",
    "    print(f\"{'FILE':<20} | {'METHOD':<18} | {'STATUS':<15} | {'ROW COUNT'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for filename in files_to_test:\n",
    "        filepath = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"{filename:<20} | FILE NOT FOUND\")\n",
    "            continue\n",
    "\n",
    "        # Test 1: Risky Method\n",
    "        res_risky = count_rows(filepath, CONFIGS[\"RISKY (Original)\"], \"RISKY\")\n",
    "        print(f\"{filename:<20} | {'RISKY (Original)':<18} | {'Check':<15} | {res_risky}\")\n",
    "\n",
    "        # Test 2: Robust Method\n",
    "        res_robust = count_rows(filepath, CONFIGS[\"ROBUST (New)\"], \"ROBUST\")\n",
    "        print(f\"{filename:<20} | {'ROBUST (New)':<18} | {'Check':<15} | {res_robust}\")\n",
    "\n",
    "        # Calculate Difference if both succeeded\n",
    "        if isinstance(res_risky, int) and isinstance(res_robust, int):\n",
    "            diff = res_robust - res_risky\n",
    "            print(f\"{' ':<20} | {'DIFFERENCE':<18} | {'LOSS':<15} | -{diff} rows\")\n",
    "        elif isinstance(res_risky, str) and \"CRASH\" in res_risky:\n",
    "             print(f\"{' ':<20} | {'DIFFERENCE':<18} | {'LOSS':<15} | TOTAL LOSS (Crash)\")\n",
    "\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa80b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Original Data Shape: (4, 6)\n",
      "\n",
      ">> Saving to test_project_data_integrity.csv using sep='|'...\n",
      "\n",
      "[TEST A] Loading with 'Robust' method (quoting=3)...\n",
      "   Rows loaded: 6 / 4\n",
      "   :x: CORRUPTION DETECTED: Data shifted due to pipes in text.\n",
      "\n",
      "[TEST B] Loading with 'Standard' method (Default Quoting)...\n",
      "   Rows loaded: 4 / 4\n",
      "   :white_check_mark: Perfect Match.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "TEST_FILENAME = \"test_project_data_integrity.csv\"\n",
    "FULL_PATH = os.path.join(DATA_PATH, TEST_FILENAME)\n",
    "\n",
    "# ==========================================\n",
    "# 2. CREATE \"TORTURE\" DATA\n",
    "# ==========================================\n",
    "# We create 4 rows with specific \"dangerous\" text content\n",
    "data = {\n",
    "    'nct_id': ['NCT001', 'NCT002', 'NCT003', 'NCT004'],\n",
    "    'target': [0, 1, 0, 1],\n",
    "\n",
    "    # 1. Safe Row\n",
    "    'official_title': ['Simple Study of Aspirin'],\n",
    "\n",
    "    # 2. The \"Quote\" Trap (Contains \" inside text)\n",
    "    # Risk: Can confuse standard parsers\n",
    "    'criteria': ['Inclusion: Patients with \"severe\" headaches.'],\n",
    "\n",
    "    # 3. The \"Pipe\" Trap (Contains | inside text)\n",
    "    # Risk: CRITICAL. If we use | as separator, this looks like a new column.\n",
    "    'txt_tags': ['Drug A | Drug B | Drug C'],\n",
    "\n",
    "    # 4. The \"Newline\" Trap (Contains \\n inside text)\n",
    "    # Risk: Can be interpreted as a new row\n",
    "    'txt_int_names': ['Drug A\\nDrug B\\nDrug C']\n",
    "}\n",
    "\n",
    "# Fill missing spots to make dataframe complete\n",
    "max_len = 4\n",
    "for key in data:\n",
    "    if len(data[key]) < max_len:\n",
    "        data[key] = data[key] + ['Safe Text'] * (max_len - len(data[key]))\n",
    "\n",
    "df_original = pd.DataFrame(data)\n",
    "\n",
    "print(f\":: Original Data Shape: {df_original.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. SIMULATE SAVE (Exactly as you should do it)\n",
    "# ==========================================\n",
    "print(f\"\\n>> Saving to {TEST_FILENAME} using sep='|'...\")\n",
    "# Note: We do NOT turn off quoting here. We let Pandas handle the saving safely.\n",
    "df_original.to_csv(FULL_PATH, index=False, sep='|')\n",
    "\n",
    "# ==========================================\n",
    "# 4. SIMULATE LOAD (The Acid Test)\n",
    "# ==========================================\n",
    "# We test 2 Loading Methods\n",
    "\n",
    "# Method A: The \"Robust\" Method (What you use for raw files)\n",
    "# params: sep='|', quoting=3 (Ignore quotes)\n",
    "print(\"\\n[TEST A] Loading with 'Robust' method (quoting=3)...\")\n",
    "try:\n",
    "    df_robust = pd.read_csv(\n",
    "        FULL_PATH,\n",
    "        sep='|',\n",
    "        quoting=3,  # <--- The setting we are testing\n",
    "        on_bad_lines='warn'\n",
    "    )\n",
    "    print(f\"   Rows loaded: {len(df_robust)} / 4\")\n",
    "    if len(df_robust) < 4:\n",
    "        print(\"   :warning: LOSS DETECTED! The 'Robust' method failed on processed data.\")\n",
    "    else:\n",
    "        # Check if data is corrupted (columns shifted)\n",
    "        # If the pipe row was split, the last column might be NaN or wrong\n",
    "        if df_robust.iloc[2].isnull().any():\n",
    "             print(\"   :x: CORRUPTION DETECTED: Data shifted due to pipes in text.\")\n",
    "        else:\n",
    "             print(\"   :white_check_mark: Success.\")\n",
    "except Exception as e:\n",
    "    print(f\"   :x: CRASHED: {e}\")\n",
    "\n",
    "# Method B: The \"Standard\" Method\n",
    "# params: sep='|', Standard Quoting (Respect quotes)\n",
    "print(\"\\n[TEST B] Loading with 'Standard' method (Default Quoting)...\")\n",
    "try:\n",
    "    df_standard = pd.read_csv(\n",
    "        FULL_PATH,\n",
    "        sep='|',\n",
    "        # We do NOT use quoting=3 here. We trust Pandas' default CSV behavior.\n",
    "        on_bad_lines='warn'\n",
    "    )\n",
    "    print(f\"   Rows loaded: {len(df_standard)} / 4\")\n",
    "    if len(df_standard) == 4:\n",
    "        print(\"   :white_check_mark: Perfect Match.\")\n",
    "except Exception as e:\n",
    "    print(f\"   :x: CRASHED: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. CLEANUP\n",
    "# ==========================================\n",
    "# os.remove(FULL_PATH) # Uncomment to delete the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f3e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                          STAGE 1: RAW TXT LOAD AUDIT                           \n",
      "================================================================================\n",
      "FILENAME                  | RISKY (Original)   | ROBUST (New)       | DIFFERENCE\n",
      "--------------------------------------------------------------------------------\n",
      "studies.txt               | 558973             | 558973             | No Loss\n",
      "interventions.txt         | 945857             | 945857             | No Loss\n",
      "countries.txt             | 763540             | 763540             | No Loss\n",
      "sponsors.txt              | 894384             | 894384             | No Loss\n",
      "designs.txt               | 554264             | 554264             | No Loss\n",
      "eligibilities.txt         | 558028             | 558028             | No Loss\n",
      "calculated_values.txt     | 558973             | 558973             | No Loss\n",
      "keywords.txt              | 1466970            | 1466970            | No Loss\n",
      "\n",
      "================================================================================\n",
      "                      STAGE 2: SAVE -> RELOAD STRESS TEST                       \n",
      "================================================================================\n",
      "Testing with 'studies.txt' (contains complex text)...\n",
      "\n",
      "1. Loading studies.txt (Robust)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102543/1514594684.py:116: ParserWarning: Skipping line 25973: expected 71 fields, saw 72\n",
      "Skipping line 26917: expected 71 fields, saw 72\n",
      "Skipping line 71531: expected 71 fields, saw 73\n",
      "Skipping line 72332: expected 71 fields, saw 73\n",
      "Skipping line 91121: expected 71 fields, saw 72\n",
      "Skipping line 91997: expected 71 fields, saw 72\n",
      "Skipping line 96504: expected 71 fields, saw 72\n",
      "Skipping line 98819: expected 71 fields, saw 72\n",
      "Skipping line 99685: expected 71 fields, saw 73\n",
      "Skipping line 112093: expected 71 fields, saw 73\n",
      "Skipping line 141292: expected 71 fields, saw 73\n",
      "Skipping line 145008: expected 71 fields, saw 73\n",
      "Skipping line 147915: expected 71 fields, saw 73\n",
      "Skipping line 200367: expected 71 fields, saw 76\n",
      "Skipping line 207631: expected 71 fields, saw 72\n",
      "Skipping line 207734: expected 71 fields, saw 73\n",
      "Skipping line 207742: expected 71 fields, saw 73\n",
      "Skipping line 210198: expected 71 fields, saw 73\n",
      "Skipping line 210646: expected 71 fields, saw 73\n",
      "Skipping line 237825: expected 71 fields, saw 73\n",
      "Skipping line 242137: expected 71 fields, saw 72\n",
      "Skipping line 242709: expected 71 fields, saw 72\n",
      "Skipping line 292889: expected 71 fields, saw 72\n",
      "Skipping line 307799: expected 71 fields, saw 72\n",
      "Skipping line 308349: expected 71 fields, saw 73\n",
      "Skipping line 313774: expected 71 fields, saw 72\n",
      "Skipping line 326463: expected 71 fields, saw 72\n",
      "Skipping line 329033: expected 71 fields, saw 72\n",
      "Skipping line 329710: expected 71 fields, saw 74\n",
      "Skipping line 334857: expected 71 fields, saw 72\n",
      "Skipping line 351796: expected 71 fields, saw 72\n",
      "Skipping line 357152: expected 71 fields, saw 74\n",
      "Skipping line 368959: expected 71 fields, saw 72\n",
      "Skipping line 370975: expected 71 fields, saw 73\n",
      "Skipping line 390279: expected 71 fields, saw 72\n",
      "Skipping line 390998: expected 71 fields, saw 72\n",
      "Skipping line 417086: expected 71 fields, saw 72\n",
      "Skipping line 422421: expected 71 fields, saw 73\n",
      "Skipping line 423986: expected 71 fields, saw 73\n",
      "Skipping line 448440: expected 71 fields, saw 72\n",
      "Skipping line 454676: expected 71 fields, saw 73\n",
      "Skipping line 465586: expected 71 fields, saw 72\n",
      "Skipping line 465705: expected 71 fields, saw 73\n",
      "Skipping line 476624: expected 71 fields, saw 73\n",
      "Skipping line 479875: expected 71 fields, saw 72\n",
      "Skipping line 482416: expected 71 fields, saw 73\n",
      "Skipping line 483008: expected 71 fields, saw 72\n",
      "Skipping line 488080: expected 71 fields, saw 73\n",
      "Skipping line 498641: expected 71 fields, saw 74\n",
      "Skipping line 511542: expected 71 fields, saw 72\n",
      "Skipping line 532375: expected 71 fields, saw 72\n",
      "Skipping line 539449: expected 71 fields, saw 72\n",
      "Skipping line 545504: expected 71 fields, saw 72\n",
      "Skipping line 554147: expected 71 fields, saw 72\n",
      "Skipping line 556308: expected 71 fields, saw 75\n",
      "\n",
      "  df = pd.read_csv(studies_path, sep='|', quoting=3, on_bad_lines='warn', low_memory=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Loaded 558918 rows.\n",
      "2. Cleaning Data (Removing pipes '|' and newlines from text)...\n",
      "3. Saving to CSV (sep='|')...\n",
      "4. Reloading from CSV (Robust)...\n",
      "   -> Reloaded 558918 rows.\n",
      "----------------------------------------\n",
      "VERDICT: :white_check_mark: 0 ROWS LOST. The pipeline is safe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "OUTPUT_CSV = \"audit_project_data.csv\"\n",
    "\n",
    "# All files used in your ClinicalTrialLoader\n",
    "FILES_TO_TEST = [\n",
    "    'studies.txt',\n",
    "    'interventions.txt',\n",
    "    'countries.txt',\n",
    "    'sponsors.txt',\n",
    "    'designs.txt',\n",
    "    'eligibilities.txt',\n",
    "    'calculated_values.txt',\n",
    "    'keywords.txt'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEFINING THE METHODS\n",
    "# ==========================================\n",
    "def get_row_count(filepath, method_name):\n",
    "    \"\"\"\n",
    "    Attempts to count rows using a specific loading strategy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method_name == \"RISKY\":\n",
    "            # The method that crashes on unbalanced quotes\n",
    "            # We read only 1 column to make it fast\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                sep='|',\n",
    "                usecols=[0],\n",
    "                quotechar='\"',       # Standard quoting\n",
    "                on_bad_lines='warn', # Skip bad lines\n",
    "                low_memory=False\n",
    "            )\n",
    "        elif method_name == \"ROBUST\":\n",
    "            # The method that ignores quotes (Your Fix)\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                sep='|',\n",
    "                usecols=[0],\n",
    "                quoting=3,           # csv.QUOTE_NONE\n",
    "                on_bad_lines='warn',\n",
    "                low_memory=False\n",
    "            )\n",
    "        return len(df)\n",
    "    except Exception as e:\n",
    "        return \"CRASHED\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXECUTION: STAGE 1 (RAW FILES)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'STAGE 1: RAW TXT LOAD AUDIT':^80}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'FILENAME':<25} | {'RISKY (Original)':<18} | {'ROBUST (New)':<18} | {'DIFFERENCE'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for filename in FILES_TO_TEST:\n",
    "    full_path = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"{filename:<25} | {'FILE NOT FOUND':<18} | {'-':<18} | -\")\n",
    "        continue\n",
    "\n",
    "    # 1. Run Risky\n",
    "    count_risky = get_row_count(full_path, \"RISKY\")\n",
    "\n",
    "    # 2. Run Robust\n",
    "    count_robust = get_row_count(full_path, \"ROBUST\")\n",
    "\n",
    "    # 3. Compare\n",
    "    diff_msg = \"\"\n",
    "    if count_risky == \"CRASHED\":\n",
    "        diff_msg = \"!!! RISKY CRASHED\"\n",
    "    else:\n",
    "        diff = count_robust - count_risky\n",
    "        if diff == 0:\n",
    "            diff_msg = \"No Loss\"\n",
    "        elif diff > 0:\n",
    "            diff_msg = f\"Risky lost {diff} rows\"\n",
    "        else:\n",
    "            diff_msg = f\"Robust lost {abs(diff)} rows\" # Unlikely\n",
    "\n",
    "    print(f\"{filename:<25} | {str(count_risky):<18} | {str(count_robust):<18} | {diff_msg}\")\n",
    "\n",
    "    # Store robust count for Stage 2 checks\n",
    "    if isinstance(count_robust, int):\n",
    "        results[filename] = count_robust\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION: STAGE 2 (CSV CYCLE)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'STAGE 2: SAVE -> RELOAD STRESS TEST':^80}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing with 'studies.txt' (contains complex text)...\\n\")\n",
    "\n",
    "studies_path = os.path.join(DATA_PATH, 'studies.txt')\n",
    "save_path = os.path.join(DATA_PATH, OUTPUT_CSV)\n",
    "\n",
    "if 'studies.txt' in results:\n",
    "    try:\n",
    "        # A. LOAD (Robust)\n",
    "        # We load ALL columns this time to test text processing\n",
    "        print(\"1. Loading studies.txt (Robust)...\")\n",
    "        df = pd.read_csv(studies_path, sep='|', quoting=3, on_bad_lines='warn', low_memory=False)\n",
    "        original_count = len(df)\n",
    "        print(f\"   -> Loaded {original_count} rows.\")\n",
    "\n",
    "        # B. CLEAN (Simulating the _prepare_text step)\n",
    "        print(\"2. Cleaning Data (Removing pipes '|' and newlines from text)...\")\n",
    "        # Identify object (text) columns\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in text_cols:\n",
    "            # Fast vectorized replacement\n",
    "            df[col] = df[col].astype(str).str.replace('|', ' ', regex=False).str.replace('\\n', ' ', regex=False).str.replace('\\r', ' ', regex=False)\n",
    "\n",
    "        # C. SAVE\n",
    "        print(\"3. Saving to CSV (sep='|')...\")\n",
    "        df.to_csv(save_path, sep='|', index=False)\n",
    "\n",
    "        # D. RELOAD (Robust)\n",
    "        print(\"4. Reloading from CSV (Robust)...\")\n",
    "        df_reloaded = pd.read_csv(save_path, sep='|', quoting=3, on_bad_lines='warn', low_memory=False)\n",
    "        new_count = len(df_reloaded)\n",
    "        print(f\"   -> Reloaded {new_count} rows.\")\n",
    "\n",
    "        # E. VERDICT\n",
    "        print(\"-\" * 40)\n",
    "        if original_count == new_count:\n",
    "             print(f\"VERDICT: :white_check_mark: 0 ROWS LOST. The pipeline is safe.\")\n",
    "        else:\n",
    "             print(f\"VERDICT: :x: LOST {original_count - new_count} ROWS.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"STAGE 2 FAILED: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Stage 2 (studies.txt failed to load in Stage 1).\")\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(save_path):\n",
    "    os.remove(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed45514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":hourglass_flowing_sand: Loading project_data.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102543/1921414497.py:19: ParserWarning: Skipping line 14945: expected 1 fields, saw 6\n",
      "Skipping line 17580: expected 1 fields, saw 2\n",
      "Skipping line 28911: expected 1 fields, saw 2\n",
      "Skipping line 44602: expected 1 fields, saw 7\n",
      "Skipping line 69163: expected 1 fields, saw 2\n",
      "Skipping line 70183: expected 1 fields, saw 4\n",
      "Skipping line 79666: expected 1 fields, saw 3\n",
      "Skipping line 103210: expected 1 fields, saw 3\n",
      "\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":white_check_mark: Success! Loaded 105328 rows.\n",
      "\n",
      "Shape: (105328, 1)\n",
      "Columns: ['nct_id,start_date_type,start_date,study_type,overall_status,phase,number_of_arms,why_stopped,target,start_year,phase_ordinal,covid_exposure,includes_us,is_international,agency_class,allocation,intervention_model,primary_purpose,masking,gender,healthy_volunteers,adult,child,older_adult,num_primary_endpoints,best_pathology,therapeutic_area,therapeutic_subgroup_name,competition_broad,competition_niche,txt_tags,txt_criteria']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. SETUP\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "FILE_NAME = \"project_data.csv\" # The file you just saved\n",
    "\n",
    "# 2. THE ROBUST LOADER\n",
    "# We use the exact same logic as your class to ensure 1-to-1 consistency.\n",
    "def get_training_data():\n",
    "    full_path = os.path.join(DATA_PATH, FILE_NAME)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\":x: Error: {full_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    print(f\":hourglass_flowing_sand: Loading {FILE_NAME}...\")\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        full_path,\n",
    "        sep='|',             # Matches the separator used in save()\n",
    "        quoting=3,           # csv.QUOTE_NONE: Ignores all quotes (Safe because we removed pipes from text)\n",
    "        on_bad_lines='warn', # Just in case, but shouldn't trigger if sanitation worked\n",
    "        low_memory=False,    # Prevents dtypes warnings\n",
    "        dtype=str            # Optional: Load as string first to be 100% safe, or let Pandas infer\n",
    "    )\n",
    "\n",
    "    # Optional: Convert numeric columns back to numbers if you used dtype=str\n",
    "    # Example: df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
    "\n",
    "    print(f\":white_check_mark: Success! Loaded {len(df)} rows.\")\n",
    "    return df\n",
    "\n",
    "# 3. EXECUTE\n",
    "df = get_training_data()\n",
    "\n",
    "# 4. QUICK CHECK (The \"One-to-One\" Verification)\n",
    "if df is not None:\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "    # Check if text columns look right (not shifted)\n",
    "    if 'txt_tags' in df.columns:\n",
    "        print(\"\\n--- Sample Text Check ---\")\n",
    "        print(df[['nct_id', 'txt_tags']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8211aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":hourglass_flowing_sand: Loading project_data.csv...\n",
      ":white_check_mark: Success! Loaded 105328 rows.\n",
      "\n",
      "Shape: (105328, 1)\n",
      "Columns: ['nct_id,start_date_type,start_date,study_type,overall_status,phase,number_of_arms,why_stopped,target,start_year,phase_ordinal,covid_exposure,includes_us,is_international,agency_class,allocation,intervention_model,primary_purpose,masking,gender,healthy_volunteers,adult,child,older_adult,num_primary_endpoints,best_pathology,therapeutic_area,therapeutic_subgroup_name,competition_broad,competition_niche,txt_tags,txt_criteria']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102543/1921414497.py:19: ParserWarning: Skipping line 14945: expected 1 fields, saw 6\n",
      "Skipping line 17580: expected 1 fields, saw 2\n",
      "Skipping line 28911: expected 1 fields, saw 2\n",
      "Skipping line 44602: expected 1 fields, saw 7\n",
      "Skipping line 69163: expected 1 fields, saw 2\n",
      "Skipping line 70183: expected 1 fields, saw 4\n",
      "Skipping line 79666: expected 1 fields, saw 3\n",
      "Skipping line 103210: expected 1 fields, saw 3\n",
      "\n",
      "  df = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. SETUP\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "FILE_NAME = \"project_data.csv\" # The file you just saved\n",
    "\n",
    "# 2. THE ROBUST LOADER\n",
    "# We use the exact same logic as your class to ensure 1-to-1 consistency.\n",
    "def get_training_data():\n",
    "    full_path = os.path.join(DATA_PATH, FILE_NAME)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\":x: Error: {full_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    print(f\":hourglass_flowing_sand: Loading {FILE_NAME}...\")\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        full_path,\n",
    "        sep='|',             # Matches the separator used in save()\n",
    "        quoting=3,           # csv.QUOTE_NONE: Ignores all quotes (Safe because we removed pipes from text)\n",
    "        on_bad_lines='warn', # Just in case, but shouldn't trigger if sanitation worked\n",
    "        low_memory=False,    # Prevents dtypes warnings\n",
    "        dtype=str            # Optional: Load as string first to be 100% safe, or let Pandas infer\n",
    "    )\n",
    "\n",
    "    # Optional: Convert numeric columns back to numbers if you used dtype=str\n",
    "    # Example: df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
    "\n",
    "    print(f\":white_check_mark: Success! Loaded {len(df)} rows.\")\n",
    "    return df\n",
    "\n",
    "# 3. EXECUTE\n",
    "df = get_training_data()\n",
    "\n",
    "# 4. QUICK CHECK (The \"One-to-One\" Verification)\n",
    "if df is not None:\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "    # Check if text columns look right (not shifted)\n",
    "    if 'txt_tags' in df.columns:\n",
    "        print(\"\\n--- Sample Text Check ---\")\n",
    "        print(df[['nct_id', 'txt_tags']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89258cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/delaunan/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from xgboost) (2.2.6)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: scipy in /home/delaunan/.pyenv/versions/3.10.6/envs/clintrialpredict/lib/python3.10/site-packages (from xgboost) (1.15.3)\n",
      "Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:31\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [xgboost]m1/2\u001b[0m [xgboost]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-nccl-cu12-2.28.9 xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
