{
 "cells": [
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "e610104d",
   "metadata": {},
   "source": [
    "# Clinical Trial Risk Engine: Data Engineering Pipeline\n",
    "\n",
    "**Project:** Premature Termination Risk Prediction\n",
    "**Output Artifact:** `project_data.csv`\n",
    "**Scope:** Interventional Drug Trials (Phases 1, 2, and 3).\n",
    "**Temporal Filter:** Trials starting between 2000 and the present (plus 2 years).\n",
    "\n",
    "## Data Dictionary and Feature Rationale\n",
    "\n",
    "This dataset aggregates data from the AACT (Aggregate Analysis of ClinicalTrials.gov) database to predict trial termination."
   ]
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fc057",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
  },
  {
   "cell_type": "markdown",
   "id": "2be47c1e",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 1. Target Variable\n",
    "*   **`target`** (Binary): The classification target.\n",
    "    *   `0`: **Completed**. The trial concluded according to protocol.\n",
    "    *   `1`: **Failed**. The trial was Terminated, Withdrawn, or Suspended.\n",
    "*   **`overall_status`** (String): The raw status label. *Note: This column is retained for validation but must be dropped prior to training to prevent data leakage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faa5bd",
   "metadata": {},
   "source": [
    "### 2. Operational Features (Complexity Proxies)\n",
    "*   **`num_facilities`** (Integer): The count of distinct recruiting sites. Higher counts correlate with increased operational complexity and cost.\n",
    "*   **`num_countries`** (Integer): The count of unique countries involved. Indicates regulatory complexity (e.g., FDA, EMA, PMDA coordination).\n",
    "*   **`phase_ordinal`** (Float): A numeric mapping of the trial phase (Phase 1=1.0, Phase 3=3.0). This is the primary proxy for trial magnitude and resource requirements.\n",
    "*   **`number_of_arms`** (Integer): The number of intervention groups. Indicates protocol complexity.\n",
    "*   **`start_year`** (Integer): The year the trial began. Used to capture temporal trends in clinical research standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cbe6b",
   "metadata": {},
   "source": [
    "### 3. Eligibility and Protocol Features\n",
    "*   **`criteria`** (Text): The full inclusion and exclusion criteria. This unstructured text contains high-value signals regarding protocol restrictiveness.\n",
    "*   **`gender`** (Categorical): Indicates if the trial is restricted by sex (Female, Male, or All).\n",
    "*   **`healthy_volunteers`** (Binary): Indicates if healthy participants are accepted. Often distinguishes Phase 1 safety trials (Yes) from Phase 2/3 efficacy trials (No).\n",
    "*   **`adult`, `child`, `older_adult`** (Binary): Pre-calculated flags indicating the target age groups. Used in place of raw numeric age limits to reduce noise and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8870908",
   "metadata": {},
   "source": [
    "### 4. Scientific and Design Features\n",
    "*   **`therapeutic_area`** (Categorical): High-level medical classification (e.g., Oncology, Cardiology).\n",
    "*   **`therapeutic_subgroup_name`** (Categorical): Granular disease classification (e.g., Neoplasms by Site).\n",
    "*   **`intervention_model`** (Categorical): The strategy for assigning interventions (e.g., Parallel, Crossover, Single Group).\n",
    "*   **`masking`** (Categorical): The level of blinding (e.g., Double, Quadruple, None).\n",
    "*   **`allocation`** (Categorical): Indicates if participants are randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9dc0c",
   "metadata": {},
   "source": [
    "### 5. Environmental Context (Competition)\n",
    "*   **`competition_niche`** (Integer): The count of concurrent trials with the *same* Phase and *same* Therapeutic Subgroup. Represents direct competition for specific patient populations.\n",
    "*   **`competition_broad`** (Integer): The count of concurrent trials within the *same* Therapeutic Area. Represents broader resource saturation.\n",
    "*   **`covid_exposure`** (Binary): Indicates if the trial was active during the 2019-2021 global disruption window.\n",
    "\n",
    "### 6. Sponsor Information\n",
    "*   **`agency_class`** (Categorical): The type of lead sponsor (Industry, NIH, Other). Industry trials typically have different risk profiles compared to academic or government-funded studies.\n",
    "*   **`includes_us`** (Binary): Indicates if the trial has at least one site in the United States, implying FDA regulatory oversight."
=======
    "Here is the **Comprehensive Data Dictionary**. It merges the **Statistical Reality** (from your audit) with the **Business Definitions & Technical Explanations** you requested.\n",
    "\n",
    "This document explains **what** the data is, **where** it comes from, **what the labels mean**, and **why** it matters for predicting trial failure.\n",
    "\n",
    "***\n",
    "\n",
    "# Clinical Trial Risk Engine: Master Data Dictionary\n",
    "\n",
    "**Dataset Name:** `project_data.csv`\n",
    "**Total Records:** 124,490\n",
    "**Scope:** Interventional Drug Trials (Phases 1, 2, 3). *Excludes Phase 0 and Phase 4.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Target Variable\n",
    "\n",
    "### **`target`**\n",
    "*   **Definition:** The binary classification target indicating if the trial successfully completed its protocol.\n",
    "    *   `0`: **Completed**. The trial finished normally.\n",
    "    *   `1`: **Failed**. The trial was Terminated, Withdrawn, or Suspended.\n",
    "*   **Source:** Derived from `studies.overall_status`.\n",
    "*   **Statistics:**\n",
    "    *   **Completed (0):** 81.2%\n",
    "    *   **Failed (1):** 18.8%\n",
    "*   **Relevance:** This is the variable we are training the model to predict.\n",
    "\n",
    "### **`overall_status`**\n",
    "*   **Definition:** The raw status label provided by ClinicalTrials.gov.\n",
    "*   **Source:** `studies.overall_status`\n",
    "*   **Labels:**\n",
    "    *   `COMPLETED`: The study has concluded normally; participants are no longer being examined or treated.\n",
    "    *   `TERMINATED`: The study has stopped early and will not start again. Participants are no longer being examined or treated.\n",
    "    *   `WITHDRAWN`: The study stopped early, before enrolling its first participant.\n",
    "    *   `SUSPENDED`: The study has stopped early but may start again.\n",
    "*   **Relevance:** Source of truth for the Target. **Must be dropped before training** to prevent leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Operational Signals\n",
    "*Metrics describing the logistical scale, cost, and feasibility of the study.*\n",
    "\n",
    "### **`num_facilities`**\n",
    "*   **Definition:** The total count of distinct medical centers (hospitals, clinics) listed as recruiting sites.\n",
    "*   **Source:** Calculated count of rows in `facilities.txt` per `nct_id`.\n",
    "*   **Statistics:** Mean: 13.02 sites | Max: 1,745 sites.\n",
    "*   **Relevance:** Proxy for **Operational Footprint**. High facility counts imply massive coordination costs and logistical complexity, but also suggest strong financial backing (usually Phase 3).\n",
    "\n",
    "### **`num_countries`**\n",
    "*   **Definition:** The count of unique countries where the trial is active.\n",
    "*   **Source:** Calculated count of unique values in `countries.name`.\n",
    "*   **Statistics:** Mean: 2.23 countries.\n",
    "*   **Relevance:** Proxy for **Regulatory Complexity**. Multi-country trials must satisfy multiple health authorities (FDA, EMA, PMDA) simultaneously, increasing the risk of administrative delays or shutdowns.\n",
    "\n",
    "### **`phase_ordinal`**\n",
    "*   **Definition:** A numeric mapping of the trial phase to represent the progression of drug development.\n",
    "*   **Source:** Mapped from `studies.phase`.\n",
    "*   **Labels & Logic:**\n",
    "    *   `1.0` (**Phase 1**): Safety & Dosage. Small cohorts. High technical risk, low operational risk.\n",
    "    *   `1.5` (**Phase 1/Phase 2**): Adaptive design. Seamless transition from safety to efficacy.\n",
    "    *   `2.0` (**Phase 2**): Efficacy. Medium cohorts. High risk of scientific failure (drug doesn't work).\n",
    "    *   `2.5` (**Phase 2/Phase 3**): Adaptive design.\n",
    "    *   `3.0` (**Phase 3**): Confirmation. Large cohorts. High risk of operational failure (cost/recruitment) and statistical futility.\n",
    "*   **Relevance:** The single strongest predictor of trial size and cost.\n",
    "\n",
    "### **`start_year`**\n",
    "*   **Definition:** The calendar year the trial began.\n",
    "*   **Source:** Extracted from `studies.start_date`.\n",
    "*   **Statistics:** Mean: 2012.4.\n",
    "*   **Relevance:** Captures **Modernization**. Clinical trial standards have become stricter over time. \"Fail Fast\" strategies in Pharma mean newer trials might be terminated more aggressively than older ones.\n",
    "\n",
    "### **`number_of_arms`**\n",
    "*   **Definition:** The number of distinct intervention groups in the study design (e.g., Placebo, Low Dose, High Dose = 3 arms).\n",
    "*   **Source:** `studies.number_of_arms`.\n",
    "*   **Statistics:** Mean: 2.38 arms.\n",
    "*   **Relevance:** Proxy for **Protocol Complexity**. More arms require more patients and more complex supply chain management.\n",
    "\n",
    "### **`min_age` / `max_age`**\n",
    "*   **Definition:** The numeric age limits (in years) for participant eligibility.\n",
    "*   **Source:** Parsed from `eligibilities.minimum_age` and `maximum_age`.\n",
    "*   **Statistics:** `max_age` is missing in 45% of cases (implies \"No Upper Limit\").\n",
    "*   **Relevance:** Defines the **Recruitment Pool**. Extremely narrow age ranges (e.g., \"18 to 25\") make recruitment statistically difficult, increasing the risk of termination due to \"Slow Accrual.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Scientific Signals (Design)\n",
    "*Technical attributes of the experimental protocol.*\n",
    "\n",
    "### **`intervention_model`**\n",
    "*   **Definition:** The general design strategy for assigning interventions.\n",
    "*   **Source:** `designs.intervention_model`.\n",
    "*   **Labels:**\n",
    "    *   `PARALLEL` (54.5%): Participants are assigned to one group (e.g., Drug OR Placebo) and remain there. Standard for Phase 3.\n",
    "    *   `SINGLE_GROUP` (30.4%): All participants receive the intervention. No control group. Common in Phase 1 or Oncology.\n",
    "    *   `CROSSOVER` (10.2%): Participants receive Treatment A, wait, then receive Treatment B. They serve as their own control.\n",
    "    *   `FACTORIAL`: Evaluates two or more interventions simultaneously (e.g., A, B, A+B, Neither). High statistical complexity.\n",
    "    *   `SEQUENTIAL`: Participants are enrolled in stages; the trial may stop early based on interim results.\n",
    "*   **Relevance:** Complex designs (Factorial, Crossover) have higher operational risks. Single Group designs are often scientifically riskier (early phase).\n",
    "\n",
    "### **`masking`**\n",
    "*   **Definition:** The level of blinding used to prevent bias.\n",
    "*   **Source:** `designs.masking`.\n",
    "*   **Labels:**\n",
    "    *   `NONE` (Open Label): Everyone knows the treatment. High bias risk, but operationally easier.\n",
    "    *   `DOUBLE`: Participant and Investigator are blinded. Standard for rigor.\n",
    "    *   `QUADRUPLE`: Participant, Investigator, Care Provider, and Outcomes Assessor are blinded. Gold standard.\n",
    "*   **Relevance:** High rigor (Quadruple Masking) requires complex supply chains (placebo matching), increasing operational risk. Open Label (None) is common in Phase 1 but less rigorous.\n",
    "\n",
    "### **`allocation`**\n",
    "*   **Definition:** The method used to assign participants to arms.\n",
    "*   **Source:** `designs.allocation`.\n",
    "*   **Labels:** `RANDOMIZED` (82%), `NON_RANDOMIZED` (18%).\n",
    "*   **Relevance:** Randomized trials are the scientific standard but require more infrastructure than non-randomized observational-style trials.\n",
    "\n",
    "### **`primary_purpose`**\n",
    "*   **Definition:** The main reason for the clinical trial.\n",
    "*   **Source:** `designs.primary_purpose`.\n",
    "*   **Labels:** `TREATMENT` (80%), `PREVENTION`, `DIAGNOSTIC`.\n",
    "*   **Relevance:** Prevention trials (vaccines) often require massive sample sizes compared to Treatment trials, altering the risk profile.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Medical Signals (Pathology)\n",
    "*Classification of the disease being studied.*\n",
    "\n",
    "### **`therapeutic_area`**\n",
    "*   **Definition:** The highest level of the medical hierarchy (Level 1).\n",
    "*   **Source:** Derived from `browse_conditions.mesh_term` mapped to MeSH Tree Codes (C01-C26).\n",
    "*   **Labels:** Oncology, Cardiovascular, Neurology, Infectious Disease, etc.\n",
    "*   **Relevance:** **Biological Risk**. Oncology trials have historically higher failure rates due to the complexity of cancer biology compared to, say, Antibiotics (Infectious).\n",
    "\n",
    "### **`therapeutic_subgroup_name`**\n",
    "*   **Definition:** The mid-level classification derived from the MeSH Tree Structure (Level 2). Groups specific diseases into families.\n",
    "*   **Source:** Derived from `mesh_terms.xml` (Tree Number slicing).\n",
    "*   **Examples:** \"Neoplasms by Site\" (groups Breast, Lung, Colon cancer), \"Metabolic Diseases\" (groups Diabetes, Thyroid).\n",
    "*   **Relevance:** Allows the model to learn risks specific to disease families (e.g., \"Neurodegenerative diseases are hard to treat\") without getting lost in thousands of specific disease names.\n",
    "\n",
    "### **`best_pathology`**\n",
    "*   **Definition:** The specific disease name derived from the trial's condition list using a medical priority logic.\n",
    "*   **Source:** `browse_conditions.mesh_term` (Smart Selection).\n",
    "*   **Relevance:** The granular disease target.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. External Environment\n",
    "*Contextual factors impacting trial success.*\n",
    "\n",
    "### **`competition_intensity`**\n",
    "*   **Definition:** A calculated index representing recruitment competition.\n",
    "*   **Calculation:** The count of *other* drug trials that started within a **3-year window** (Start Year, +1, +2), targeting the **same Therapeutic Subgroup** and the **same Phase**.\n",
    "*   **Source:** Calculated feature.\n",
    "*   **Relevance:** **Market Saturation**. High values indicate a \"crowded\" market. If 50 companies are recruiting for \"Breast Cancer Phase 3\" at the same time, finding eligible patients becomes statistically difficult, leading to \"Slow Accrual\" termination.\n",
    "\n",
    "### **`covid_exposure`**\n",
    "*   **Definition:** A binary flag indicating if the trial started immediately before or during the peak of the COVID-19 pandemic (2019-2021).\n",
    "*   **Source:** Calculated from `studies.start_date`.\n",
    "*   **Relevance:** **External Shock**. Trials in this window faced unique risks: site closures, patient dropout, and supply chain breaks.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Sponsor & Geography\n",
    "\n",
    "### **`agency_class`**\n",
    "*   **Definition:** The type of organization sponsoring the trial.\n",
    "*   **Source:** `sponsors.agency_class`.\n",
    "*   **Labels:**\n",
    "    *   `INDUSTRY` (51%): Pharmaceutical/Biotech. High funding, but strict \"Go/No-Go\" business decisions.\n",
    "    *   `OTHER` (41%): Academic/Hospitals. Often grant-funded. May run longer/slower.\n",
    "    *   `NIH/FED` (5%): Government.\n",
    "*   **Relevance:** **Financial Risk**. Industry sponsors are more likely to terminate a trial for \"Business Reasons\" (e.g., change in strategy) even if the science is okay.\n",
    "\n",
    "### **`includes_us`**\n",
    "*   **Definition:** A binary flag indicating if at least one trial site is located in the United States.\n",
    "*   **Source:** Derived from `countries.name`.\n",
    "*   **Relevance:** **Regulatory Environment**. US trials are subject to FDA oversight (strict safety monitoring) and high healthcare costs. This often correlates with higher termination rates compared to trials in developing regions.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Text Data\n",
    "*Unstructured text fields available for Natural Language Processing.*\n",
    "\n",
    "### **`official_title`**\n",
    "*   **Definition:** The scientific title of the study.\n",
    "*   **Source:** `studies.official_title`.\n",
    "*   **Relevance:** Contains technical keywords (e.g., \"Monoclonal Antibody\", \"Placebo-Controlled\") that signal complexity.\n",
    "\n",
    "### **`criteria`**\n",
    "*   **Definition:** The detailed list of Inclusion and Exclusion criteria.\n",
    "*   **Source:** `eligibilities.criteria`.\n",
    "*   **Relevance:** **The most valuable text field.** It defines the \"narrowness\" of the eligible population. Complex, restrictive criteria are a leading cause of recruitment failure.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Analysis Fields (Excluded from Training)\n",
    "*Fields that contain future information (Data Leakage) but are useful for post-hoc analysis.*\n",
    "\n",
    "### **`min_p_value`**\n",
    "*   **Definition:** The lowest P-value reported for the trial's primary outcome measures.\n",
    "*   **Source:** `outcome_analyses.p_value`.\n",
    "*   **Relevance:** Explains **Scientific Failure**. If a trial Completed but failed to prove efficacy, the P-value will be high (>0.05).\n",
    "\n",
    "### **`why_stopped`**\n",
    "*   **Definition:** The free-text reason provided by the sponsor for termination.\n",
    "*   **Source:** `studies.why_stopped`.\n",
    "*   **Relevance:** **Ground Truth for Validation**. Used to verify if the model correctly identified a high-risk trial that later stopped for \"Lack of funding\" or \"Adverse Events\"."
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "7e3e3b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473b2012",
   "metadata": {},
   "source": [
    "### 1. Configuration and Path Management\n",
    "This block establishes the environment settings. It also defines safe loading parameters to handle special characters in the raw AACT text files."
=======
   "id": "61b2e72f",
   "metadata": {},
   "source": [
    "Here is the complete, modular Data Engineering Pipeline. You can copy and paste these blocks sequentially into a Jupyter Notebook or a Python script.\n",
    "\n",
    "### Block 1: Setup & Robust Configuration\n",
    "**What this does:**\n",
    "Sets up the file paths and defines the **\"Safe Load Parameters\"**. The AACT database is messy; text fields often contain the pipe character (`|`) or unescaped quotes, which breaks standard CSV readers. These parameters force Python to read everything carefully as a string first, preventing crashes."
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "id": "7a72c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DATA_PATH set to: /home/delaunan/code/delaunan/clintrialpredict/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PATH SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "print(f\">>> DATA_PATH set to: {os.path.abspath(DATA_PATH)}\")\n"
=======
   "execution_count": null,
   "id": "7a72c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/delaunan/code/delaunan/project/00_data'"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": null,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "af4db2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Setup Complete. Ready to process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_FILE = 'project_data.csv'\n",
    "\n",
    "# ROBUST LOADING PARAMETERS\n",
    "AACT_LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,\n",
    "    \"header\": 0,\n",
    "    \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "print(\">>> Setup Complete. Ready to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86173d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 2. Data Loading and Cohort Filtering\n",
    "This step defines the study cohort. We apply strict inclusion and exclusion criteria to ensure data quality:\n",
    "1.  **Study Type:** Retain only `INTERVENTIONAL` trials.\n",
    "2.  **Intervention:** Retain only `DRUG` or `BIOLOGICAL` trials.\n",
    "3.  **Status:** Retain only definitive outcomes (`COMPLETED`, `TERMINATED`, `WITHDRAWN`, `SUSPENDED`).\n",
    "4.  **Phase:** Exclude Phase 0 and Phase 4 to focus on the core development pipeline.\n",
    "5.  **Temporal Validity:** Filter for trials starting between 2000 and the near future, removing invalid dates (e.g., 1900) and placeholder future records."
=======
    "### Block 2: The Funnel (Loading & Filtering)\n",
    "**What this does:**\n",
    "1.  Loads the core `studies` table.\n",
    "2.  **Filters:** Keeps only **Interventional** trials (no observational).\n",
    "3.  **Filters:** Keeps only **Drug/Biologic** trials (using the `interventions` table).\n",
    "4.  **Filters:** Keeps only **Closed** trials (Completed, Terminated, Withdrawn, Suspended) so we have a definite target.\n",
    "5.  **Target Creation:** Creates the `target` column (0 = Completed, 1 = Failed)."
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 3,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "f0b261a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Studies & Applying Filters...\n",
<<<<<<< HEAD
      "   - Core Cohort Size (Phases 1-3, Years 2000-2027): 119201 trials\n"
=======
      "   - Core Cohort Size (Phases 1-3 only): 124490 trials\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
<<<<<<< HEAD
    "# 2. THE FUNNEL: LOADING & FILTERING (Updated with Year Filter)\n",
=======
    "# 2. THE FUNNEL: LOADING & FILTERING\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Loading Studies & Applying Filters...\")\n",
    "\n",
    "# A. Load Studies\n",
    "cols_studies = [\n",
    "    'nct_id', 'overall_status', 'study_type', 'phase',\n",
    "    'start_date', 'start_date_type',\n",
    "    'number_of_arms', 'official_title', 'why_stopped'\n",
    "]\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'studies.txt'), usecols=cols_studies, **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Filter: Interventional Only\n",
    "df = df[df['study_type'] == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "# C. Filter: Drugs Only\n",
    "df_int = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **AACT_LOAD_PARAMS)\n",
    "drug_ids = df_int[df_int['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]['nct_id'].unique()\n",
    "df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "# D. Filter: Closed Statuses Only\n",
    "allowed_statuses = ['COMPLETED', 'TERMINATED', 'WITHDRAWN', 'SUSPENDED']\n",
    "df = df[df['overall_status'].isin(allowed_statuses)]\n",
    "\n",
    "# E. Filter: Exclude Phase 0 and Phase 4 (Refined Scope)\n",
<<<<<<< HEAD
=======
    "# We only want Phase 1, 1/2, 2, 2/3, 3\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA']\n",
    "df = df[~df['phase'].isin(excluded_phases)]\n",
    "\n",
    "# F. Create Target & Fix Dates\n",
    "df['target'] = df['overall_status'].apply(lambda x: 0 if x == 'COMPLETED' else 1)\n",
    "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "df['start_year'] = df['start_date'].dt.year\n",
    "\n",
<<<<<<< HEAD
    "# --- NEW FILTER: VALID YEARS ONLY ---\n",
    "# Drop 1900 (Errors) and Future Dates (Invalid for training)\n",
    "current_year = pd.Timestamp.now().year\n",
    "df = df[df['start_year'].between(2000, current_year + 2)]\n",
    "\n",
    "print(f\"   - Core Cohort Size (Phases 1-3, Years 2000-{current_year+2}): {len(df)} trials\")"
=======
    "print(f\"   - Core Cohort Size (Phases 1-3 only): {len(df)} trials\")"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9029d8",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 3. Medical Hierarchy Integration\n",
    "This block enriches the dataset with standardized medical classifications. By mapping `nct_id` to the MeSH (Medical Subject Headings) hierarchy, we derive:\n",
    "*   **`therapeutic_area`:** The broad medical category.\n",
    "*   **`therapeutic_subgroup`:** The specific disease family.\n",
    "This hierarchical approach allows the model to learn risk patterns associated with specific medical fields (e.g., Oncology vs. Infectious Diseases).\n"
=======
    "### Block 3: Medical Hierarchy & Subgroups\n",
    "**What this does:**\n",
    "After import of an external database with hierarchy of therapeutic area + disease, <br>\n",
    "merges the `smart_pathology_lookup.csv` you the therapeutic area information not originally present in the table <br>. \n",
    "It aslos extracts the **Therapeutic Subgroup** (e.g., `C04.588`) which is critical for the crowding calculation (how many trials are recruiting patients for the same therapeutic purpose) in the next step.\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 4,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "9dd03ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Attaching Medical Hierarchy...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. MEDICAL HIERARCHY & SUBGROUPS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Attaching Medical Hierarchy...\")\n",
    "\n",
    "# A. Load Smart Lookup (Best Term per Trial)\n",
    "df_smart = pd.read_csv(os.path.join(DATA_PATH, 'smart_pathology_lookup.csv'))\n",
    "df = df.merge(df_smart, on='nct_id', how='left')\n",
    "\n",
    "# B. Fill Missing\n",
    "df['therapeutic_area'] = df['therapeutic_area'].fillna('Other/Unclassified')\n",
    "df['best_pathology'] = df['best_pathology'].fillna('Unknown')\n",
    "\n",
    "# C. Create Subgroup Code (Level 2 Hierarchy)\n",
    "# Logic: Take first 7 chars of tree number (e.g., C04.588.180 -> C04.588)\n",
    "df['therapeutic_subgroup'] = df['tree_number'].astype(str).apply(\n",
    "    lambda x: x[:7] if pd.notna(x) and len(x) >= 7 else 'Unknown'\n",
    ")\n",
    "\n",
    "# D. Map Subgroup Code to Name (Optional but good for Explainability)\n",
    "# We load the full lookup to get the name for \"C04.588\"\n",
    "df_lookup = pd.read_csv(os.path.join(DATA_PATH, 'mesh_lookup.csv'), sep='|')\n",
    "code_to_name = pd.Series(df_lookup.mesh_term.values, index=df_lookup.tree_number).to_dict()\n",
    "df['therapeutic_subgroup_name'] = df['therapeutic_subgroup'].map(code_to_name).fillna('Unknown Subgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37499ac",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 4. Competition Intensity Calculation\n",
    "This block calculates \"Crowding\" metrics to quantify the competitive environment. We define competition using a 3-year rolling window (Start Year, +1, +2).\n",
    "*   **`competition_broad`:** Measures saturation within the general therapeutic area.\n",
    "*   **`competition_niche`:** Measures direct competition for the specific patient population (same Subgroup and Phase)."
=======
    "\n",
    "### Block 4: Research Space Crowding (Competition Intensity)\n",
    "**What this does:**\n",
    "Calculates **`competition_intensity`**.\n",
    "*   **Logic:** It counts how many trials started in the **same 3-year window** (Start Year, +1, +2), for the **same Medical Subgroup**, and the **same Phase**.\n",
    "*   **Why:** A Phase 1 Glaucoma trial does not compete with a Phase 3 Breast Cancer trial. This metric is specific.\n",
    "\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 5,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "66e0fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Calculating Competition Intensity (Dual Level)...\n",
      "   - Created 'competition_broad' and 'competition_niche'\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. DUAL-LEVEL CROWDING (Niche vs Broad) - UPDATED\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Calculating Competition Intensity (Dual Level)...\")\n",
    "\n",
    "# A. Standardize Phase for Grouping\n",
    "# Group \"Phase 1/2\" with \"Phase 2\" for competition purposes\n",
    "phase_group_map = {\n",
    "    'PHASE1': 'PHASE1', 'PHASE1/PHASE2': 'PHASE2',\n",
    "    'PHASE2': 'PHASE2', 'PHASE2/PHASE3': 'PHASE3', 'PHASE3': 'PHASE3'\n",
    "}\n",
    "df['phase_group'] = df['phase'].map(phase_group_map).fillna('UNKNOWN')\n",
    "\n",
    "# --- LEVEL 1: BROAD COMPETITION (Area + Phase) ---\n",
    "# How many trials in \"Oncology\" + \"Phase 3\" started in this window?\n",
    "grid_broad = df.groupby(['start_year', 'therapeutic_area', 'phase_group']).size().reset_index(name='count')\n",
    "dict_broad = dict(zip(zip(grid_broad['start_year'], grid_broad['therapeutic_area'], grid_broad['phase_group']), grid_broad['count']))\n",
    "\n",
    "def get_broad_crowding(row):\n",
    "    y, area, ph = row['start_year'], row['therapeutic_area'], row['phase_group']\n",
    "    if pd.isna(y): return 0\n",
    "    # Sum Year 0, +1, +2\n",
    "    return dict_broad.get((y, area, ph), 0) + dict_broad.get((y+1, area, ph), 0) + dict_broad.get((y+2, area, ph), 0)\n",
    "\n",
    "df['competition_broad'] = df.apply(get_broad_crowding, axis=1)\n",
    "\n",
    "# --- LEVEL 2: NICHE COMPETITION (Subgroup + Phase) ---\n",
    "# How many trials in \"Gastrointestinal Neoplasms\" + \"Phase 3\" started in this window?\n",
    "grid_niche = df.groupby(['start_year', 'therapeutic_subgroup', 'phase_group']).size().reset_index(name='count')\n",
    "dict_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup'], grid_niche['phase_group']), grid_niche['count']))\n",
    "\n",
    "def get_niche_crowding(row):\n",
    "    y, sub, ph = row['start_year'], row['therapeutic_subgroup'], row['phase_group']\n",
    "    if pd.isna(y) or sub == 'Unknown': return 0\n",
    "    # Sum Year 0, +1, +2\n",
    "    return dict_niche.get((y, sub, ph), 0) + dict_niche.get((y+1, sub, ph), 0) + dict_niche.get((y+2, sub, ph), 0)\n",
    "\n",
    "df['competition_niche'] = df.apply(get_niche_crowding, axis=1)\n",
    "\n",
    "df.drop(columns=['phase_group'], inplace=True)\n",
    "print(\"   - Created 'competition_broad' and 'competition_niche'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862785a4",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 5. Protocol Details and Eligibility\n",
    "This block extracts key protocol design features.\n",
    "*   **Eligibility Flags:** We incorporate categorical flags (`gender`, `healthy_volunteers`, `adult`, `child`, `older_adult`) to characterize the study population.\n",
    "*   **Text Data:** We extract the full `criteria` text block for downstream Natural Language Processing (NLP).\n",
    "*   **Endpoints:** We calculate the number of primary endpoints as a proxy for scientific complexity.\n",
    "*   **Analysis Data:** We extract `min_p_value` for post-hoc analysis (excluded from training to prevent leakage).\n"
=======
    "### Block 5: Protocol (Age) & Results (P-Values)\n",
    "**What this does:**\n",
    "1.  **Age:** Parses `min_age` and `max_age` into numbers (Years), age of patients to enter the clinical trial.\n",
    "2.  **Endpoints:** Gets the count of primary endpoints (complexity).Many core objectives defined for a study could mean two things: <br>\n",
    "*- intent to increase scientific evidence*, you increase the number of goals so that at least one is relevant<br>\n",
    "*- complex and potentially large study*\n",
    "3.  **P-Values:** Extracts the minimum P-value for primary outcomes in case we want <br>to use it. **(Note: for now, this is for Analysis only, not Training).**\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 6,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "edc1c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      ">>> Extracting Eligibility & Endpoints...\n"
=======
      ">>> Extracting Age, Endpoints & P-Values...\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
<<<<<<< HEAD
    "# 5. PROTOCOL DETAILS (Eligibility, Endpoints) - UPDATED\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Extracting Eligibility & Endpoints...\")\n",
    "\n",
    "# A. Load Eligibility Fields (No Age Parsing)\n",
    "# We rely on the pre-calculated flags (adult/child/older_adult) instead of parsing numbers.\n",
    "cols_elig = [\n",
    "    'nct_id',\n",
    "    'criteria',\n",
    "    'gender', 'healthy_volunteers',\n",
    "    'adult', 'child', 'older_adult'\n",
    "]\n",
    "\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'),\n",
    "                      usecols=cols_elig,\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Merge into Main DataFrame\n",
    "df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "# C. Endpoint Counts (Existing Logic)\n",
=======
    "# 5. PROTOCOL DETAILS & ANALYTICAL RESULTS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Extracting Age, Endpoints & P-Values...\")\n",
    "\n",
    "# A. Age Parsing (From Eligibilities)\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'),\n",
    "                      usecols=['nct_id', 'minimum_age', 'maximum_age', 'criteria'],\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "\n",
    "def parse_age(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    val = str(val).lower()\n",
    "    try:\n",
    "        num = float(val.split()[0])\n",
    "        if 'month' in val: return num / 12\n",
    "        if 'week' in val: return num / 52\n",
    "        if 'day' in val: return num / 365\n",
    "        return num\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_elig['min_age'] = df_elig['minimum_age'].apply(parse_age)\n",
    "df_elig['max_age'] = df_elig['maximum_age'].apply(parse_age)\n",
    "df = df.merge(df_elig[['nct_id', 'min_age', 'max_age', 'criteria']], on='nct_id', how='left')\n",
    "\n",
    "# B. Endpoint Counts\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "df_calc = pd.read_csv(os.path.join(DATA_PATH, 'calculated_values.txt'),\n",
    "                      usecols=['nct_id', 'number_of_primary_outcomes_to_measure'],\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "df = df.merge(df_calc, on='nct_id', how='left')\n",
    "df['num_primary_endpoints'] = pd.to_numeric(df['number_of_primary_outcomes_to_measure'], errors='coerce').fillna(1)\n",
    "\n",
<<<<<<< HEAD
    "# D. P-Values (Analysis Only - Existing Logic)\n",
    "df_outcomes = pd.read_csv(os.path.join(DATA_PATH, 'outcomes.txt'), usecols=['id', 'nct_id', 'outcome_type'], **AACT_LOAD_PARAMS)\n",
    "prim_ids = df_outcomes[df_outcomes['outcome_type'] == 'PRIMARY']['id'].unique()\n",
    "\n",
=======
    "# C. P-Values (Analysis Only)\n",
    "# Load Outcomes to find Primary IDs\n",
    "df_outcomes = pd.read_csv(os.path.join(DATA_PATH, 'outcomes.txt'), usecols=['id', 'nct_id', 'outcome_type'], **AACT_LOAD_PARAMS)\n",
    "prim_ids = df_outcomes[df_outcomes['outcome_type'] == 'PRIMARY']['id'].unique()\n",
    "\n",
    "# Load Analyses\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "df_an = pd.read_csv(os.path.join(DATA_PATH, 'outcome_analyses.txt'), usecols=['outcome_id', 'p_value'], **AACT_LOAD_PARAMS)\n",
    "df_an = df_an[df_an['outcome_id'].isin(prim_ids)]\n",
    "df_an['p_value_num'] = pd.to_numeric(df_an['p_value'], errors='coerce')\n",
    "\n",
<<<<<<< HEAD
    "min_p = df_an.groupby('outcome_id')['p_value_num'].min().reset_index()\n",
=======
    "# Get Min P-Value per Trial\n",
    "min_p = df_an.groupby('outcome_id')['p_value_num'].min().reset_index()\n",
    "# Link back to NCT via outcomes table\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "min_p = min_p.merge(df_outcomes[['id', 'nct_id']], left_on='outcome_id', right_on='id')\n",
    "trial_p = min_p.groupby('nct_id')['p_value_num'].min().reset_index(name='min_p_value')\n",
    "\n",
    "df = df.merge(trial_p, on='nct_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f895009",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 6. Operational Proxies and Sponsor Data\n",
    "This block merges operational and administrative features:\n",
    "*   **Phase Mapping:** Converts text phases to a numeric ordinal scale (1.0â€“3.0). Invalid phases are filtered out.\n",
    "*   **Geography:** Calculates the number of facilities and countries, and flags US-based trials.\n",
    "*   **Sponsor:** Identifies the lead agency class (e.g., Industry vs. Other).\n",
    "*   **External Factors:** Calculates `covid_exposure` based on the trial's start date relative to the pandemic window."
=======
    "### Block 6: Operational Proxies, Sponsors & External Factors (covid_exposure)\n",
    "**What this does:** <br><br>\n",
    "Merges the standard operational features (Lead Sponsor, Clinical trial study design defined at the start).\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 7,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "836335e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      ">>> Merging Operational Features & Calculating COVID Exposure...\n",
      "   - Filtered out invalid phases (0.0).\n"
=======
      ">>> Merging Operational Features & Calculating COVID Exposure...\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
<<<<<<< HEAD
    "# 6. OPERATIONAL PROXIES, SPONSORS & EXTERNAL FACTORS (COVID)\n",
=======
    "# 6. OPERATIONAL PROXIES, SPONSORS & EXTERNAL FACTORS - UPDATED\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Merging Operational Features & Calculating COVID Exposure...\")\n",
    "\n",
    "# A. Phase Ordinal\n",
    "phase_map = {'PHASE1': 1, 'PHASE1/PHASE2': 1.5, 'PHASE2': 2, 'PHASE2/PHASE3': 2.5, 'PHASE3': 3}\n",
    "df['phase_ordinal'] = df['phase'].map(phase_map).fillna(0)\n",
    "\n",
<<<<<<< HEAD
    "# --- NEW FILTER: DROP UNKNOWN PHASES ---\n",
    "# We only want ordinal 1.0 to 3.0. Drop 0.0.\n",
    "df = df[df['phase_ordinal'] > 0]\n",
    "\n",
    "# B. COVID Exposure\n",
=======
    "# B. COVID Exposure (The Missing Piece)\n",
    "# Logic: Trials starting just before or during the peak disruption (2019-2021)\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "df['covid_exposure'] = df['start_year'].between(2019, 2021).astype(int)\n",
    "\n",
    "# C. Facilities (Raw Count)\n",
    "df_fac = pd.read_csv(os.path.join(DATA_PATH, 'facilities.txt'), usecols=['nct_id', 'id'], **AACT_LOAD_PARAMS)\n",
    "fac_counts = df_fac.groupby('nct_id')['id'].count().reset_index(name='num_facilities')\n",
    "df = df.merge(fac_counts, on='nct_id', how='left')\n",
    "df['num_facilities'] = df['num_facilities'].fillna(1).astype(int)\n",
    "\n",
    "# D. Countries\n",
    "df_countries = pd.read_csv(os.path.join(DATA_PATH, 'countries.txt'), usecols=['nct_id', 'name'], **AACT_LOAD_PARAMS)\n",
    "country_stats = df_countries.groupby('nct_id').agg(\n",
    "    num_countries=('name', 'nunique'),\n",
    "    includes_us=('name', lambda x: 1 if 'United States' in x.values else 0)\n",
    ").reset_index()\n",
    "df = df.merge(country_stats, on='nct_id', how='left')\n",
    "df['num_countries'] = df['num_countries'].fillna(1).astype(int)\n",
    "df['includes_us'] = df['includes_us'].fillna(0).astype(int)\n",
    "\n",
    "# E. Sponsors & Design\n",
    "df_sponsors = pd.read_csv(os.path.join(DATA_PATH, 'sponsors.txt'), **AACT_LOAD_PARAMS)\n",
    "df_lead = df_sponsors[df_sponsors['lead_or_collaborator'] == 'lead'][['nct_id', 'agency_class']].drop_duplicates('nct_id')\n",
    "df = df.merge(df_lead, on='nct_id', how='left')\n",
    "\n",
    "cols_des = ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose']\n",
    "df_des = pd.read_csv(os.path.join(DATA_PATH, 'designs.txt'), usecols=cols_des, **AACT_LOAD_PARAMS)\n",
<<<<<<< HEAD
    "df = df.merge(df_des, on='nct_id', how='left')\n",
    "\n",
    "print(\"   - Filtered out invalid phases (0.0).\")"
=======
    "df = df.merge(df_des, on='nct_id', how='left')"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810d7c",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 7. Text Integration and Data Export\n",
    "This final processing step merges the remaining unstructured text fields (`brief_summary`, `detailed_description`) and cleans up temporary technical columns. The final dataset is exported as `project_data.csv`."
=======
    "### Block 7: Save & Health Check\n",
    "**What this does:**\n",
    "1.  Drops technical columns (`start_date` is replaced by `start_year`).\n",
    "2.  Saves the final CSV.\n",
    "3.  Prints a summary so you can verify the data quality immediately."
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 8,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "116f7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Merging Text & Saving...\n",
      "\n",
      ">>> SUCCESS: Final Dataset saved to project_data.csv\n",
<<<<<<< HEAD
      "    Rows: 105884\n",
      "    Columns: 35\n",
=======
      "    Rows: 124490\n",
      "    Columns: 32\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
      "    New Features: 'competition_intensity', 'min_age', 'max_age', 'min_p_value'\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. TEXT MERGE & FINAL SAVE\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Merging Text & Saving...\")\n",
    "\n",
    "# A. Text Data\n",
    "df_brief = pd.read_csv(os.path.join(DATA_PATH, 'brief_summaries.txt'), usecols=['nct_id', 'description'], **AACT_LOAD_PARAMS)\n",
    "df_brief.rename(columns={'description': 'brief_summary'}, inplace=True)\n",
    "df = df.merge(df_brief, on='nct_id', how='left')\n",
    "\n",
    "df_detail = pd.read_csv(os.path.join(DATA_PATH, 'detailed_descriptions.txt'), usecols=['nct_id', 'description'], **AACT_LOAD_PARAMS)\n",
    "df_detail.rename(columns={'description': 'detailed_description'}, inplace=True)\n",
    "df = df.merge(df_detail, on='nct_id', how='left')\n",
    "\n",
    "# B. Cleanup\n",
    "# Drop technical columns\n",
    "df.drop(columns=['start_date', 'start_date_type', 'tree_number', 'number_of_primary_outcomes_to_measure'], inplace=True, errors='ignore')\n",
    "\n",
    "# C. Save\n",
    "df.to_csv(os.path.join(DATA_PATH, OUTPUT_FILE), index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "print(f\"\\n>>> SUCCESS: Final Dataset saved to {OUTPUT_FILE}\")\n",
    "print(f\"    Rows: {len(df)}\")\n",
    "print(f\"    Columns: {len(df.columns)}\")\n",
    "print(f\"    New Features: 'competition_intensity', 'min_age', 'max_age', 'min_p_value'\")"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "e0cf3d97",
   "metadata": {},
   "source": [
    "### 8. Quality Assurance and Encoding Strategy\n",
    "This block performs a comprehensive audit of the final dataset. It analyzes missing values, cardinality, and data types to generate an automated **Encoding Strategy Report**. This report provides specific recommendations for the machine learning pipeline (e.g., which fields require One-Hot Encoding, Target Encoding, or NLP transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
=======
   "cell_type": "code",
   "execution_count": 9,
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "id": "4e61e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      ">>> Running Dataset Audit...\n",
      ">>> STARTING ENRICHED AUDIT ON: project_data.csv...\n",
      "   - Loaded 105,884 rows.\n",
      "SUCCESS: Audit report saved to audit_dataset_report.txt\n"
=======
      ">>> STARTING AUDIT ON: project_data.csv...\n",
      "   - Loaded 124,490 rows.\n",
      "SUCCESS: Audit saved to audit_dataset_report.txt\n",
      "You can now open this file to verify your data quality before modeling.\n"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
<<<<<<< HEAD
    "# 8. FINAL DATASET AUDIT & ENCODING STRATEGY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Running Dataset Audit...\")\n",
    "\n",
    "INPUT_FILE = 'project_data.csv'\n",
    "OUTPUT_REPORT = 'audit_dataset_report.txt'\n",
    "\n",
    "# Define the Schema & Groups\n",
    "FEATURE_GROUPS = {\n",
    "    \"1. TARGET\": [\"target\", \"overall_status\"],\n",
    "    \"2. NUMERIC (Scale & Log)\": [\n",
    "        \"num_facilities\", \"num_countries\", \"number_of_arms\",\n",
    "        \"start_year\", \"competition_niche\", \"competition_broad\"\n",
    "    ],\n",
    "    \"3. ORDINAL (Keep Numeric)\": [\"phase_ordinal\"],\n",
    "    \"4. CATEGORICAL (One-Hot Encode)\": [\n",
    "        \"gender\", \"healthy_volunteers\", \"adult\", \"child\", \"older_adult\",\n",
    "        \"agency_class\", \"includes_us\", \"covid_exposure\",\n",
    "        \"masking\", \"allocation\", \"intervention_model\", \"primary_purpose\",\n",
    "        \"therapeutic_area\"\n",
    "    ],\n",
    "    \"5. HIGH CARDINALITY (Target Encode)\": [\n",
    "        \"therapeutic_subgroup_name\", \"best_pathology\"\n",
    "    ],\n",
    "    \"6. TEXT (NLP / BERT)\": [\n",
    "        \"official_title\", \"criteria\", \"brief_summary\", \"detailed_description\"\n",
    "    ],\n",
    "    \"7. EXCLUDED (Leakage/Analysis)\": [\"min_p_value\", \"why_stopped\"]\n",
    "}\n",
    "\n",
    "def recommend_encoding(col, dtype, unique_count, is_text=False):\n",
    "    \"\"\"Logic to suggest the best Scikit-Learn transformer.\"\"\"\n",
    "    if col == 'target': return \"LABEL (Do not process)\"\n",
    "    if col in ['overall_status', 'why_stopped', 'min_p_value']: return \"DROP (Leakage/Analysis)\"\n",
    "\n",
    "    if is_text:\n",
    "        return \"NLP (BERT Embeddings or TF-IDF)\"\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(dtype):\n",
    "        if unique_count < 10: return \"ORDINAL / PASSTHROUGH\"\n",
    "        return \"NUMERIC (StandardScaler + Log1p if skewed)\"\n",
    "\n",
    "    if unique_count <= 2: return \"BINARY (One-Hot drop='if_binary')\"\n",
    "    if unique_count < 50: return \"ONE-HOT ENCODING\"\n",
    "    return \"TARGET ENCODING (High Cardinality)\"\n",
    "\n",
    "def run_enriched_audit():\n",
    "    print(f\">>> STARTING ENRICHED AUDIT ON: {INPUT_FILE}...\")\n",
=======
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FILE = 'project_data.csv'  # Make sure this matches your final filename\n",
    "OUTPUT_REPORT = 'audit_dataset_report.txt'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OBJECTIVE-BASED FEATURE GROUPS (Updated for Final Schema)\n",
    "# -----------------------------------------------------------------------------\n",
    "FEATURE_GROUPS = {\n",
    "    \"1. TARGET VARIABLE (The Goal)\": [\n",
    "        \"target\",\n",
    "        \"overall_status\"\n",
    "    ],\n",
    "\n",
    "    \"2. OPERATIONAL SIGNALS (Complexity & Feasibility)\": [\n",
    "        \"num_facilities\",       # Raw count of sites\n",
    "        \"num_countries\",        # Geographic spread\n",
    "        \"phase_ordinal\",        # Size proxy (1.0 - 3.0)\n",
    "        \"number_of_arms\",       # Logistical complexity\n",
    "        \"start_year\",           # Modernization factor\n",
    "        \"min_age\",              # Protocol restrictiveness\n",
    "        \"max_age\"               # Protocol restrictiveness\n",
    "    ],\n",
    "\n",
    "    \"3. SCIENTIFIC SIGNALS (Design & Pathology)\": [\n",
    "        \"therapeutic_area\",         # High level (Oncology)\n",
    "        \"therapeutic_subgroup_name\",# Mid level (Neoplasms by Site)\n",
    "        \"best_pathology\",           # Low level (Breast Cancer)\n",
    "        \"intervention_model\",       # Parallel/Crossover\n",
    "        \"masking\",                  # Blinded?\n",
    "        \"allocation\",               # Randomized?\n",
    "        \"primary_purpose\",          # Treatment/Prevention\n",
    "        \"num_primary_endpoints\"     # Scientific complexity\n",
    "    ],\n",
    "\n",
    "    \"4. EXTERNAL ENVIRONMENT (Competition & Context)\": [\n",
    "        \"competition_niche\",        # Direct competitors (Same Subgroup + Phase)\n",
    "        \"competition_broad\",        # Resource competitors (Same Area + Phase)\n",
    "        \"covid_exposure\"            # Pandemic impact\n",
    "    ],\n",
    "\n",
    "    \"5. SPONSOR & BIAS (Who is running it?)\": [\n",
    "        \"agency_class\",             # Industry vs Academic\n",
    "        \"includes_us\"               # Regulatory environment (FDA)\n",
    "    ],\n",
    "\n",
    "    \"6. LEAKAGE CHECKS (Do not use for Training)\": [\n",
    "        \"min_p_value\",              # Result (Should be highly correlated with target)\n",
    "        \"why_stopped\"               # Result (Text explanation of failure)\n",
    "    ],\n",
    "\n",
    "    \"7. NLP INPUTS (Text Data Quality)\": [\n",
    "        \"official_title\",\n",
    "        \"brief_summary\",\n",
    "        \"detailed_description\",\n",
    "        \"criteria\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def run_objective_audit():\n",
    "    print(f\">>> STARTING AUDIT ON: {INPUT_FILE}...\")\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "\n",
    "    file_path = os.path.join(DATA_PATH, INPUT_FILE)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[ERROR] File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    print(f\"   - Loaded {len(df):,} rows.\")\n",
    "\n",
<<<<<<< HEAD
    "    with open(os.path.join(DATA_PATH, OUTPUT_REPORT), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"================================================================\\n\")\n",
    "        f.write(f\"FINAL DATASET AUDIT & STRATEGY REPORT\\n\")\n",
=======
    "    with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"================================================================\\n\")\n",
    "        f.write(f\"FINAL DATASET AUDIT REPORT\\n\")\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "        f.write(f\"File: {INPUT_FILE}\\n\")\n",
    "        f.write(f\"Rows: {len(df):,}\\n\")\n",
    "        f.write(f\"Cols: {len(df.columns)}\\n\")\n",
    "        f.write(\"================================================================\\n\\n\")\n",
    "\n",
    "        for category, columns in FEATURE_GROUPS.items():\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
<<<<<<< HEAD
    "            f.write(f\"GROUP: {category}\\n\")\n",
=======
    "            f.write(f\"OBJECTIVE: {category}\\n\")\n",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
    "            f.write(f\"{'='*80}\\n\")\n",
    "\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
<<<<<<< HEAD
    "                    f.write(f\"\\n[MISSING] '{col}' not found.\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Stats\n",
    "                missing = df[col].isna().sum()\n",
    "                missing_pct = (missing / len(df)) * 100\n",
    "                unique = df[col].nunique()\n",
    "                dtype = df[col].dtype\n",
    "\n",
    "                # Determine Strategy\n",
    "                is_text = col in FEATURE_GROUPS[\"6. TEXT (NLP / BERT)\"]\n",
    "                strategy = recommend_encoding(col, dtype, unique, is_text)\n",
    "\n",
    "                f.write(f\"\\n>>> FIELD: {col.upper()}\\n\")\n",
    "                f.write(f\"    Type: {dtype} | Unique: {unique} | Missing: {missing_pct:.2f}%\\n\")\n",
    "                f.write(f\"    STRATEGY: {strategy}\\n\")\n",
    "\n",
    "                # Content Analysis\n",
    "                if is_text:\n",
    "                    avg_words = df[col].astype(str).apply(lambda x: len(x.split()) if pd.notna(x) else 0).mean()\n",
    "                    f.write(f\"    - Avg Length: {avg_words:.0f} words\\n\")\n",
    "\n",
    "                elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    stats = df[col].describe()\n",
    "                    zeros = (df[col] == 0).sum()\n",
    "                    f.write(f\"    - Stats: Mean={stats['mean']:.2f}, Min={stats['min']}, Max={stats['max']}\\n\")\n",
    "                    f.write(f\"    - Zeros: {zeros} ({zeros/len(df):.1%})\\n\")\n",
    "                    if col != 'target':\n",
    "                        corr = df[[col, 'target']].corr().iloc[0,1]\n",
    "                        f.write(f\"    - Correlation w/ Target: {corr:.4f}\\n\")\n",
    "\n",
    "                else:\n",
    "                    # Categorical Distribution\n",
    "                    top_n = df[col].value_counts().head(5)\n",
    "                    f.write(\"    - Top Values:\\n\")\n",
    "                    for val, count in top_n.items():\n",
    "                        f.write(f\"      * {str(val)[:30]:<30} : {count} ({count/len(df):.1%})\\n\")\n",
    "\n",
    "    print(f\"SUCCESS: Audit report saved to {OUTPUT_REPORT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_enriched_audit()"
=======
    "                    f.write(f\"\\n[MISSING COLUMN] '{col}' was expected but not found.\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Basic Stats\n",
    "                missing = df[col].isna().sum()\n",
    "                missing_pct = (missing / len(df)) * 100\n",
    "                dtype = df[col].dtype\n",
    "\n",
    "                f.write(f\"\\n>>> FIELD: {col} ({dtype})\\n\")\n",
    "                f.write(f\"    - Missing: {missing} ({missing_pct:.2f}%)\\n\")\n",
    "\n",
    "                # NUMERIC ANALYSIS\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    stats = df[col].describe()\n",
    "                    zeros = (df[col] == 0).sum()\n",
    "                    f.write(f\"    - Mean:    {stats['mean']:.2f} (Std: {stats['std']:.2f})\\n\")\n",
    "                    f.write(f\"    - Min/Max: {stats['min']} / {stats['max']}\\n\")\n",
    "                    f.write(f\"    - Zeros:   {zeros} ({zeros/len(df):.1%})\\n\")\n",
    "\n",
    "                    # Correlation with Target (Signal Strength Check)\n",
    "                    if col != 'target':\n",
    "                        try:\n",
    "                            corr = df[[col, 'target']].corr().iloc[0,1]\n",
    "                            f.write(f\"    - Corr w/ Target: {corr:.4f} \")\n",
    "                            if abs(corr) > 0.1: f.write(\"(STRONG SIGNAL)\")\n",
    "                            elif abs(corr) < 0.01: f.write(\"(NO SIGNAL)\")\n",
    "                            f.write(\"\\n\")\n",
    "                        except:\n",
    "                            f.write(\"    - Corr w/ Target: NaN (Constant value?)\\n\")\n",
    "\n",
    "                # CATEGORICAL ANALYSIS\n",
    "                else:\n",
    "                    unique = df[col].nunique()\n",
    "                    f.write(f\"    - Unique Labels: {unique}\\n\")\n",
    "\n",
    "                    # Show distribution (Top 10)\n",
    "                    if unique < 50:\n",
    "                        f.write(\"    - Distribution:\\n\")\n",
    "                        dist = df[col].value_counts(normalize=True).head(10) * 100\n",
    "                        for val, pct in dist.items():\n",
    "                            f.write(f\"      * {str(val)[:40].ljust(40)} : {pct:.1f}%\\n\")\n",
    "                    else:\n",
    "                        f.write(\"    - Top 5 Most Frequent:\\n\")\n",
    "                        dist = df[col].value_counts().head(5)\n",
    "                        for val, count in dist.items():\n",
    "                            f.write(f\"      * {str(val)[:40].ljust(40)} : {count}\\n\")\n",
    "\n",
    "                # TEXT ANALYSIS (Specific)\n",
    "                if col in ['official_title', 'brief_summary', 'detailed_description', 'criteria', 'why_stopped']:\n",
    "                    # Avg word count\n",
    "                    word_counts = df[col].astype(str).apply(lambda x: len(x.split()) if pd.notna(x) and x.lower() != 'nan' else 0)\n",
    "                    avg_words = word_counts.mean()\n",
    "                    empty_rows = (word_counts < 3).sum()\n",
    "                    f.write(f\"    - Text Stats: Avg {avg_words:.0f} words. {empty_rows} rows are empty/short.\\n\")\n",
    "\n",
    "    print(f\"SUCCESS: Audit saved to {OUTPUT_REPORT}\")\n",
    "    print(\"You can now open this file to verify your data quality before modeling.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_objective_audit()"
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "915adc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> STARTING TEXT STRATEGY AUDIT...\n",
      ">>> Data Path: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      ">>> Building Filtered Cohort (Interventional Drugs, Phase 1-3, 2000+)...\n",
      "   - Final Cohort Size: 119201 trials\n",
      ">>> Loading Text Candidates...\n",
      ">>> Calculating International Flag...\n",
      ">>> Generating Report: text_strategy_audit.txt...\n",
      ">>> DONE. Please upload 'text_strategy_audit.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & PATHS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Robust path finding\n",
    "if os.path.exists(\"data\"):\n",
    "    DATA_PATH = \"data\"\n",
    "elif os.path.exists(\"../../data\"):\n",
    "    DATA_PATH = \"../../data\"\n",
    "else:\n",
    "    DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "OUTPUT_REPORT = \"text_strategy_audit.txt\"\n",
    "\n",
    "AACT_PARAMS = {\n",
    "    \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "print(f\">>> STARTING TEXT STRATEGY AUDIT...\")\n",
    "print(f\">>> Data Path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. BUILD THE COHORT (Apply Filters)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Building Filtered Cohort (Interventional Drugs, Phase 1-3, 2000+)...\")\n",
    "\n",
    "# Load Studies\n",
    "cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date', 'official_title']\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'studies.txt'), usecols=cols_studies, **AACT_PARAMS)\n",
    "\n",
    "# Filters\n",
    "df = df[df['study_type'] == 'INTERVENTIONAL']\n",
    "df = df[df['overall_status'].isin(['COMPLETED', 'TERMINATED', 'WITHDRAWN', 'SUSPENDED'])]\n",
    "df = df[~df['phase'].isin(['EARLY_PHASE1', 'PHASE4', 'NA'])]\n",
    "\n",
    "# Date Filter\n",
    "df['start_year'] = pd.to_datetime(df['start_date'], errors='coerce').dt.year\n",
    "current_year = pd.Timestamp.now().year\n",
    "df = df[df['start_year'].between(2000, current_year + 2)]\n",
    "\n",
    "# Drug Filter\n",
    "df_int = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **AACT_PARAMS)\n",
    "drug_ids = df_int[df_int['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]['nct_id'].unique()\n",
    "df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "print(f\"   - Final Cohort Size: {len(df)} trials\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. LOAD & MERGE CANDIDATE FIELDS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Loading Text Candidates...\")\n",
    "\n",
    "# A. Criteria (The Gold Standard for Complexity)\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'), usecols=['nct_id', 'criteria'], **AACT_PARAMS)\n",
    "df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "# B. Keywords (For XGBoost Tags)\n",
    "df_keys = pd.read_csv(os.path.join(DATA_PATH, 'keywords.txt'), usecols=['nct_id', 'name'], **AACT_PARAMS)\n",
    "keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "\n",
    "# C. Intervention Details (Name = Tag, Desc = Complexity)\n",
    "df_int_det = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'),\n",
    "                         usecols=['nct_id', 'intervention_type', 'name', 'description'],\n",
    "                         **AACT_PARAMS)\n",
    "# Filter for drugs only in the details\n",
    "df_int_det = df_int_det[df_int_det['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]\n",
    "\n",
    "int_names = df_int_det.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_intervention_names')\n",
    "int_desc = df_int_det.groupby('nct_id')['description'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_intervention_desc')\n",
    "\n",
    "df = df.merge(int_names, on='nct_id', how='left')\n",
    "df = df.merge(int_desc, on='nct_id', how='left')\n",
    "\n",
    "# D. International Flag (Calculated)\n",
    "print(\">>> Calculating International Flag...\")\n",
    "df_countries = pd.read_csv(os.path.join(DATA_PATH, 'countries.txt'), usecols=['nct_id', 'name'], **AACT_PARAMS)\n",
    "country_counts = df_countries.groupby('nct_id')['name'].nunique().reset_index(name='cnt')\n",
    "df = df.merge(country_counts, on='nct_id', how='left')\n",
    "df['is_international'] = (df['cnt'] > 1).astype(int)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. GENERATE AUDIT REPORT\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\">>> Generating Report: {OUTPUT_REPORT}...\")\n",
    "\n",
    "fields_to_audit = {\n",
    "    \"OFFICIAL_TITLE\": \"official_title\",\n",
    "    \"CRITERIA\": \"criteria\",\n",
    "    \"KEYWORDS (Grouped)\": \"txt_keywords\",\n",
    "    \"INTERVENTION_NAMES\": \"txt_intervention_names\",\n",
    "    \"INTERVENTION_DESC\": \"txt_intervention_desc\",\n",
    "    \"IS_INTERNATIONAL (Flag)\": \"is_international\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(DATA_PATH, OUTPUT_REPORT), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"================================================================\\n\")\n",
    "    f.write(\"TEXT STRATEGY AUDIT (Filtered Cohort)\\n\")\n",
    "    f.write(f\"Total Trials: {len(df)}\\n\")\n",
    "    f.write(\"================================================================\\n\\n\")\n",
    "\n",
    "    for label, col in fields_to_audit.items():\n",
    "        f.write(f\"FIELD: {label}\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "        if col not in df.columns:\n",
    "            f.write(\"âŒ MISSING (Not found in dataframe)\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        # Stats\n",
    "        missing = df[col].isna().sum()\n",
    "        fill_rate = 100 - ((missing / len(df)) * 100)\n",
    "\n",
    "        f.write(f\"Fill Rate: {fill_rate:.2f}%\\n\")\n",
    "\n",
    "        if col == 'is_international':\n",
    "            dist = df[col].value_counts(normalize=True)\n",
    "            f.write(f\"Distribution: No (0)={dist.get(0,0):.1%}, Yes (1)={dist.get(1,0):.1%}\\n\")\n",
    "        else:\n",
    "            # Text Stats\n",
    "            # Fill NA with empty for length calc\n",
    "            series = df[col].fillna(\"\").astype(str)\n",
    "            avg_chars = series.apply(len).mean()\n",
    "            avg_words = series.apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "            f.write(f\"Avg Length: {avg_chars:.0f} chars ({avg_words:.0f} words)\\n\")\n",
    "\n",
    "            # Recommendation Logic\n",
    "            rec = \"???\"\n",
    "            if avg_words < 5: rec = \"âš ï¸ TOO SHORT / SPARSE\"\n",
    "            elif avg_words < 50: rec = \"ðŸ·ï¸ KEYWORD CANDIDATE (XGBoost/TF-IDF)\"\n",
    "            else: rec = \"ðŸ§  COMPLEXITY CANDIDATE (BERT)\"\n",
    "\n",
    "            f.write(f\"VERDICT:    {rec}\\n\")\n",
    "\n",
    "            # Samples\n",
    "            f.write(\"Samples:\\n\")\n",
    "            non_empty = df[df[col].notna()][col]\n",
    "            if len(non_empty) > 0:\n",
    "                for val in non_empty.sample(3).values:\n",
    "                    preview = str(val)[:100].replace('\\n', ' ')\n",
    "                    f.write(f\"  - {preview}...\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\">>> DONE. Please upload 'text_strategy_audit.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db21c8",
=======
   "id": "ca2280a2",
>>>>>>> dfbc731f8e1b989d0dcef7a3be4b7eec80d79b0b
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
