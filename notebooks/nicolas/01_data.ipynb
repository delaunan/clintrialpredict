{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e610104d",
   "metadata": {},
   "source": [
    "# Clinical Trial Risk Engine: Data Engineering Pipeline\n",
    "\n",
    "**Project:** Premature Termination Risk Prediction\n",
    "**Output Artifact:** `project_data.csv`\n",
    "**Scope:** Interventional Drug Trials (Phases 1, 2, and 3).\n",
    "**Temporal Filter:** Trials starting between 2000 and the present (plus 2 years).\n",
    "\n",
    "## Data Dictionary and Feature Rationale\n",
    "\n",
    "This dataset aggregates data from the AACT (Aggregate Analysis of ClinicalTrials.gov) database to predict trial termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be47c1e",
   "metadata": {},
   "source": [
    "### 1. Target Variable\n",
    "*   **`target`** (Binary): The classification target.\n",
    "    *   `0`: **Completed**. The trial concluded according to protocol.\n",
    "    *   `1`: **Failed**. The trial was Terminated, Withdrawn, or Suspended.\n",
    "*   **`overall_status`** (String): The raw status label. *Note: This column is retained for validation but must be dropped prior to training to prevent data leakage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faa5bd",
   "metadata": {},
   "source": [
    "### 2. Operational Features (Complexity Proxies)\n",
    "*   **`num_facilities`** (Integer): The count of distinct recruiting sites. Higher counts correlate with increased operational complexity and cost.\n",
    "*   **`num_countries`** (Integer): The count of unique countries involved. Indicates regulatory complexity (e.g., FDA, EMA, PMDA coordination).\n",
    "*   **`phase_ordinal`** (Float): A numeric mapping of the trial phase (Phase 1=1.0, Phase 3=3.0). This is the primary proxy for trial magnitude and resource requirements.\n",
    "*   **`number_of_arms`** (Integer): The number of intervention groups. Indicates protocol complexity.\n",
    "*   **`start_year`** (Integer): The year the trial began. Used to capture temporal trends in clinical research standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cbe6b",
   "metadata": {},
   "source": [
    "### 3. Eligibility and Protocol Features\n",
    "*   **`criteria`** (Text): The full inclusion and exclusion criteria. This unstructured text contains high-value signals regarding protocol restrictiveness.\n",
    "*   **`gender`** (Categorical): Indicates if the trial is restricted by sex (Female, Male, or All).\n",
    "*   **`healthy_volunteers`** (Binary): Indicates if healthy participants are accepted. Often distinguishes Phase 1 safety trials (Yes) from Phase 2/3 efficacy trials (No).\n",
    "*   **`adult`, `child`, `older_adult`** (Binary): Pre-calculated flags indicating the target age groups. Used in place of raw numeric age limits to reduce noise and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8870908",
   "metadata": {},
   "source": [
    "### 4. Scientific and Design Features\n",
    "*   **`therapeutic_area`** (Categorical): High-level medical classification (e.g., Oncology, Cardiology).\n",
    "*   **`therapeutic_subgroup_name`** (Categorical): Granular disease classification (e.g., Neoplasms by Site).\n",
    "*   **`intervention_model`** (Categorical): The strategy for assigning interventions (e.g., Parallel, Crossover, Single Group).\n",
    "*   **`masking`** (Categorical): The level of blinding (e.g., Double, Quadruple, None).\n",
    "*   **`allocation`** (Categorical): Indicates if participants are randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9dc0c",
   "metadata": {},
   "source": [
    "### 5. Environmental Context (Competition)\n",
    "*   **`competition_niche`** (Integer): The count of concurrent trials with the *same* Phase and *same* Therapeutic Subgroup. Represents direct competition for specific patient populations.\n",
    "*   **`competition_broad`** (Integer): The count of concurrent trials within the *same* Therapeutic Area. Represents broader resource saturation.\n",
    "*   **`covid_exposure`** (Binary): Indicates if the trial was active during the 2019-2021 global disruption window.\n",
    "\n",
    "### 6. Sponsor Information\n",
    "*   **`agency_class`** (Categorical): The type of lead sponsor (Industry, NIH, Other). Industry trials typically have different risk profiles compared to academic or government-funded studies.\n",
    "*   **`includes_us`** (Binary): Indicates if the trial has at least one site in the United States, implying FDA regulatory oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e3b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473b2012",
   "metadata": {},
   "source": [
    "### 1. Configuration and Path Management\n",
    "This block establishes the environment settings. It also defines safe loading parameters to handle special characters in the raw AACT text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a72c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DATA_PATH set to: /home/delaunan/code/delaunan/clintrialpredict/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PATH SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "print(f\">>> DATA_PATH set to: {os.path.abspath(DATA_PATH)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af4db2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Setup Complete. Ready to process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_FILE = 'project_data.csv'\n",
    "\n",
    "# ROBUST LOADING PARAMETERS\n",
    "AACT_LOAD_PARAMS = {\n",
    "    \"sep\": \"|\",\n",
    "    \"dtype\": str,\n",
    "    \"header\": 0,\n",
    "    \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL,\n",
    "    \"low_memory\": False,\n",
    "    \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "print(\">>> Setup Complete. Ready to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86173d",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Cohort Filtering\n",
    "This step defines the study cohort. We apply strict inclusion and exclusion criteria to ensure data quality:\n",
    "1.  **Study Type:** Retain only `INTERVENTIONAL` trials.\n",
    "2.  **Intervention:** Retain only `DRUG` or `BIOLOGICAL` trials.\n",
    "3.  **Status:** Retain only definitive outcomes (`COMPLETED`, `TERMINATED`, `WITHDRAWN`, `SUSPENDED`).\n",
    "4.  **Phase:** Exclude Phase 0 and Phase 4 to focus on the core development pipeline.\n",
    "5.  **Temporal Validity:** Filter for trials starting between 2000 and the near future, removing invalid dates (e.g., 1900) and placeholder future records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0b261a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Studies & Applying Filters...\n",
      "   - Core Cohort Size (Phases 1-3, Years 2000-2027): 119201 trials\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. THE FUNNEL: LOADING & FILTERING (Updated with Year Filter)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Loading Studies & Applying Filters...\")\n",
    "\n",
    "# A. Load Studies\n",
    "cols_studies = [\n",
    "    'nct_id', 'overall_status', 'study_type', 'phase',\n",
    "    'start_date', 'start_date_type',\n",
    "    'number_of_arms', 'official_title', 'why_stopped'\n",
    "]\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'studies.txt'), usecols=cols_studies, **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Filter: Interventional Only\n",
    "df = df[df['study_type'] == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "# C. Filter: Drugs Only\n",
    "df_int = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **AACT_LOAD_PARAMS)\n",
    "drug_ids = df_int[df_int['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]['nct_id'].unique()\n",
    "df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "# D. Filter: Closed Statuses Only\n",
    "allowed_statuses = ['COMPLETED', 'TERMINATED', 'WITHDRAWN', 'SUSPENDED']\n",
    "df = df[df['overall_status'].isin(allowed_statuses)]\n",
    "\n",
    "# E. Filter: Exclude Phase 0 and Phase 4 (Refined Scope)\n",
    "excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA']\n",
    "df = df[~df['phase'].isin(excluded_phases)]\n",
    "\n",
    "# F. Create Target & Fix Dates\n",
    "df['target'] = df['overall_status'].apply(lambda x: 0 if x == 'COMPLETED' else 1)\n",
    "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "df['start_year'] = df['start_date'].dt.year\n",
    "\n",
    "# --- NEW FILTER: VALID YEARS ONLY ---\n",
    "# Drop 1900 (Errors) and Future Dates (Invalid for training)\n",
    "current_year = pd.Timestamp.now().year\n",
    "df = df[df['start_year'].between(2000, current_year + 2)]\n",
    "\n",
    "print(f\"   - Core Cohort Size (Phases 1-3, Years 2000-{current_year+2}): {len(df)} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9029d8",
   "metadata": {},
   "source": [
    "### 3. Medical Hierarchy Integration\n",
    "This block enriches the dataset with standardized medical classifications. By mapping `nct_id` to the MeSH (Medical Subject Headings) hierarchy, we derive:\n",
    "*   **`therapeutic_area`:** The broad medical category.\n",
    "*   **`therapeutic_subgroup`:** The specific disease family.\n",
    "This hierarchical approach allows the model to learn risk patterns associated with specific medical fields (e.g., Oncology vs. Infectious Diseases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dd03ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Attaching Medical Hierarchy...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. MEDICAL HIERARCHY & SUBGROUPS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Attaching Medical Hierarchy...\")\n",
    "\n",
    "# A. Load Smart Lookup (Best Term per Trial)\n",
    "df_smart = pd.read_csv(os.path.join(DATA_PATH, 'smart_pathology_lookup.csv'))\n",
    "df = df.merge(df_smart, on='nct_id', how='left')\n",
    "\n",
    "# B. Fill Missing\n",
    "df['therapeutic_area'] = df['therapeutic_area'].fillna('Other/Unclassified')\n",
    "df['best_pathology'] = df['best_pathology'].fillna('Unknown')\n",
    "\n",
    "# C. Create Subgroup Code (Level 2 Hierarchy)\n",
    "# Logic: Take first 7 chars of tree number (e.g., C04.588.180 -> C04.588)\n",
    "df['therapeutic_subgroup'] = df['tree_number'].astype(str).apply(\n",
    "    lambda x: x[:7] if pd.notna(x) and len(x) >= 7 else 'Unknown'\n",
    ")\n",
    "\n",
    "# D. Map Subgroup Code to Name (Optional but good for Explainability)\n",
    "# We load the full lookup to get the name for \"C04.588\"\n",
    "df_lookup = pd.read_csv(os.path.join(DATA_PATH, 'mesh_lookup.csv'), sep='|')\n",
    "code_to_name = pd.Series(df_lookup.mesh_term.values, index=df_lookup.tree_number).to_dict()\n",
    "df['therapeutic_subgroup_name'] = df['therapeutic_subgroup'].map(code_to_name).fillna('Unknown Subgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37499ac",
   "metadata": {},
   "source": [
    "### 4. Competition Intensity Calculation\n",
    "This block calculates \"Crowding\" metrics to quantify the competitive environment. We define competition using a 3-year rolling window (Start Year, +1, +2).\n",
    "*   **`competition_broad`:** Measures saturation within the general therapeutic area.\n",
    "*   **`competition_niche`:** Measures direct competition for the specific patient population (same Subgroup and Phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66e0fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Calculating Competition Intensity (Dual Level)...\n",
      "   - Created 'competition_broad' and 'competition_niche'\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. DUAL-LEVEL CROWDING (Niche vs Broad) - UPDATED\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Calculating Competition Intensity (Dual Level)...\")\n",
    "\n",
    "# A. Standardize Phase for Grouping\n",
    "# Group \"Phase 1/2\" with \"Phase 2\" for competition purposes\n",
    "phase_group_map = {\n",
    "    'PHASE1': 'PHASE1', 'PHASE1/PHASE2': 'PHASE2',\n",
    "    'PHASE2': 'PHASE2', 'PHASE2/PHASE3': 'PHASE3', 'PHASE3': 'PHASE3'\n",
    "}\n",
    "df['phase_group'] = df['phase'].map(phase_group_map).fillna('UNKNOWN')\n",
    "\n",
    "# --- LEVEL 1: BROAD COMPETITION (Area + Phase) ---\n",
    "# How many trials in \"Oncology\" + \"Phase 3\" started in this window?\n",
    "grid_broad = df.groupby(['start_year', 'therapeutic_area', 'phase_group']).size().reset_index(name='count')\n",
    "dict_broad = dict(zip(zip(grid_broad['start_year'], grid_broad['therapeutic_area'], grid_broad['phase_group']), grid_broad['count']))\n",
    "\n",
    "def get_broad_crowding(row):\n",
    "    y, area, ph = row['start_year'], row['therapeutic_area'], row['phase_group']\n",
    "    if pd.isna(y): return 0\n",
    "    # Sum Year 0, +1, +2\n",
    "    return dict_broad.get((y, area, ph), 0) + dict_broad.get((y+1, area, ph), 0) + dict_broad.get((y+2, area, ph), 0)\n",
    "\n",
    "df['competition_broad'] = df.apply(get_broad_crowding, axis=1)\n",
    "\n",
    "# --- LEVEL 2: NICHE COMPETITION (Subgroup + Phase) ---\n",
    "# How many trials in \"Gastrointestinal Neoplasms\" + \"Phase 3\" started in this window?\n",
    "grid_niche = df.groupby(['start_year', 'therapeutic_subgroup', 'phase_group']).size().reset_index(name='count')\n",
    "dict_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup'], grid_niche['phase_group']), grid_niche['count']))\n",
    "\n",
    "def get_niche_crowding(row):\n",
    "    y, sub, ph = row['start_year'], row['therapeutic_subgroup'], row['phase_group']\n",
    "    if pd.isna(y) or sub == 'Unknown': return 0\n",
    "    # Sum Year 0, +1, +2\n",
    "    return dict_niche.get((y, sub, ph), 0) + dict_niche.get((y+1, sub, ph), 0) + dict_niche.get((y+2, sub, ph), 0)\n",
    "\n",
    "df['competition_niche'] = df.apply(get_niche_crowding, axis=1)\n",
    "\n",
    "df.drop(columns=['phase_group'], inplace=True)\n",
    "print(\"   - Created 'competition_broad' and 'competition_niche'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862785a4",
   "metadata": {},
   "source": [
    "### 5. Protocol Details and Eligibility\n",
    "This block extracts key protocol design features.\n",
    "*   **Eligibility Flags:** We incorporate categorical flags (`gender`, `healthy_volunteers`, `adult`, `child`, `older_adult`) to characterize the study population.\n",
    "*   **Text Data:** We extract the full `criteria` text block for downstream Natural Language Processing (NLP).\n",
    "*   **Endpoints:** We calculate the number of primary endpoints as a proxy for scientific complexity.\n",
    "*   **Analysis Data:** We extract `min_p_value` for post-hoc analysis (excluded from training to prevent leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edc1c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Extracting Eligibility & Endpoints...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. PROTOCOL DETAILS (Eligibility, Endpoints) - UPDATED\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Extracting Eligibility & Endpoints...\")\n",
    "\n",
    "# A. Load Eligibility Fields (No Age Parsing)\n",
    "# We rely on the pre-calculated flags (adult/child/older_adult) instead of parsing numbers.\n",
    "cols_elig = [\n",
    "    'nct_id',\n",
    "    'criteria',\n",
    "    'gender', 'healthy_volunteers',\n",
    "    'adult', 'child', 'older_adult'\n",
    "]\n",
    "\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'),\n",
    "                      usecols=cols_elig,\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "\n",
    "# B. Merge into Main DataFrame\n",
    "df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "# C. Endpoint Counts (Existing Logic)\n",
    "df_calc = pd.read_csv(os.path.join(DATA_PATH, 'calculated_values.txt'),\n",
    "                      usecols=['nct_id', 'number_of_primary_outcomes_to_measure'],\n",
    "                      **AACT_LOAD_PARAMS)\n",
    "df = df.merge(df_calc, on='nct_id', how='left')\n",
    "df['num_primary_endpoints'] = pd.to_numeric(df['number_of_primary_outcomes_to_measure'], errors='coerce').fillna(1)\n",
    "\n",
    "# D. P-Values (Analysis Only - Existing Logic)\n",
    "df_outcomes = pd.read_csv(os.path.join(DATA_PATH, 'outcomes.txt'), usecols=['id', 'nct_id', 'outcome_type'], **AACT_LOAD_PARAMS)\n",
    "prim_ids = df_outcomes[df_outcomes['outcome_type'] == 'PRIMARY']['id'].unique()\n",
    "\n",
    "df_an = pd.read_csv(os.path.join(DATA_PATH, 'outcome_analyses.txt'), usecols=['outcome_id', 'p_value'], **AACT_LOAD_PARAMS)\n",
    "df_an = df_an[df_an['outcome_id'].isin(prim_ids)]\n",
    "df_an['p_value_num'] = pd.to_numeric(df_an['p_value'], errors='coerce')\n",
    "\n",
    "min_p = df_an.groupby('outcome_id')['p_value_num'].min().reset_index()\n",
    "min_p = min_p.merge(df_outcomes[['id', 'nct_id']], left_on='outcome_id', right_on='id')\n",
    "trial_p = min_p.groupby('nct_id')['p_value_num'].min().reset_index(name='min_p_value')\n",
    "\n",
    "df = df.merge(trial_p, on='nct_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f895009",
   "metadata": {},
   "source": [
    "### 6. Operational Proxies and Sponsor Data\n",
    "This block merges operational and administrative features:\n",
    "*   **Phase Mapping:** Converts text phases to a numeric ordinal scale (1.0‚Äì3.0). Invalid phases are filtered out.\n",
    "*   **Geography:** Calculates the number of facilities and countries, and flags US-based trials.\n",
    "*   **Sponsor:** Identifies the lead agency class (e.g., Industry vs. Other).\n",
    "*   **External Factors:** Calculates `covid_exposure` based on the trial's start date relative to the pandemic window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "836335e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Merging Operational Features & Calculating COVID Exposure...\n",
      "   - Filtered out invalid phases (0.0).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6. OPERATIONAL PROXIES, SPONSORS & EXTERNAL FACTORS (COVID)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Merging Operational Features & Calculating COVID Exposure...\")\n",
    "\n",
    "# A. Phase Ordinal\n",
    "phase_map = {'PHASE1': 1, 'PHASE1/PHASE2': 1.5, 'PHASE2': 2, 'PHASE2/PHASE3': 2.5, 'PHASE3': 3}\n",
    "df['phase_ordinal'] = df['phase'].map(phase_map).fillna(0)\n",
    "\n",
    "# --- NEW FILTER: DROP UNKNOWN PHASES ---\n",
    "# We only want ordinal 1.0 to 3.0. Drop 0.0.\n",
    "df = df[df['phase_ordinal'] > 0]\n",
    "\n",
    "# B. COVID Exposure\n",
    "df['covid_exposure'] = df['start_year'].between(2019, 2021).astype(int)\n",
    "\n",
    "# C. Facilities (Raw Count)\n",
    "df_fac = pd.read_csv(os.path.join(DATA_PATH, 'facilities.txt'), usecols=['nct_id', 'id'], **AACT_LOAD_PARAMS)\n",
    "fac_counts = df_fac.groupby('nct_id')['id'].count().reset_index(name='num_facilities')\n",
    "df = df.merge(fac_counts, on='nct_id', how='left')\n",
    "df['num_facilities'] = df['num_facilities'].fillna(1).astype(int)\n",
    "\n",
    "# D. Countries\n",
    "df_countries = pd.read_csv(os.path.join(DATA_PATH, 'countries.txt'), usecols=['nct_id', 'name'], **AACT_LOAD_PARAMS)\n",
    "country_stats = df_countries.groupby('nct_id').agg(\n",
    "    num_countries=('name', 'nunique'),\n",
    "    includes_us=('name', lambda x: 1 if 'United States' in x.values else 0)\n",
    ").reset_index()\n",
    "df = df.merge(country_stats, on='nct_id', how='left')\n",
    "df['num_countries'] = df['num_countries'].fillna(1).astype(int)\n",
    "df['includes_us'] = df['includes_us'].fillna(0).astype(int)\n",
    "\n",
    "# E. Sponsors & Design\n",
    "df_sponsors = pd.read_csv(os.path.join(DATA_PATH, 'sponsors.txt'), **AACT_LOAD_PARAMS)\n",
    "df_lead = df_sponsors[df_sponsors['lead_or_collaborator'] == 'lead'][['nct_id', 'agency_class']].drop_duplicates('nct_id')\n",
    "df = df.merge(df_lead, on='nct_id', how='left')\n",
    "\n",
    "cols_des = ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose']\n",
    "df_des = pd.read_csv(os.path.join(DATA_PATH, 'designs.txt'), usecols=cols_des, **AACT_LOAD_PARAMS)\n",
    "df = df.merge(df_des, on='nct_id', how='left')\n",
    "\n",
    "print(\"   - Filtered out invalid phases (0.0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810d7c",
   "metadata": {},
   "source": [
    "### 7. Text Integration and Data Export\n",
    "This final processing step merges the remaining unstructured text fields (`brief_summary`, `detailed_description`) and cleans up temporary technical columns. The final dataset is exported as `project_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "116f7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Merging Text & Saving...\n",
      "\n",
      ">>> SUCCESS: Final Dataset saved to project_data.csv\n",
      "    Rows: 105884\n",
      "    Columns: 35\n",
      "    New Features: 'competition_intensity', 'min_age', 'max_age', 'min_p_value'\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. TEXT MERGE & FINAL SAVE\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Merging Text & Saving...\")\n",
    "\n",
    "# A. Text Data\n",
    "df_brief = pd.read_csv(os.path.join(DATA_PATH, 'brief_summaries.txt'), usecols=['nct_id', 'description'], **AACT_LOAD_PARAMS)\n",
    "df_brief.rename(columns={'description': 'brief_summary'}, inplace=True)\n",
    "df = df.merge(df_brief, on='nct_id', how='left')\n",
    "\n",
    "df_detail = pd.read_csv(os.path.join(DATA_PATH, 'detailed_descriptions.txt'), usecols=['nct_id', 'description'], **AACT_LOAD_PARAMS)\n",
    "df_detail.rename(columns={'description': 'detailed_description'}, inplace=True)\n",
    "df = df.merge(df_detail, on='nct_id', how='left')\n",
    "\n",
    "# B. Cleanup\n",
    "# Drop technical columns\n",
    "df.drop(columns=['start_date', 'start_date_type', 'tree_number', 'number_of_primary_outcomes_to_measure'], inplace=True, errors='ignore')\n",
    "\n",
    "# C. Save\n",
    "df.to_csv(os.path.join(DATA_PATH, OUTPUT_FILE), index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "print(f\"\\n>>> SUCCESS: Final Dataset saved to {OUTPUT_FILE}\")\n",
    "print(f\"    Rows: {len(df)}\")\n",
    "print(f\"    Columns: {len(df.columns)}\")\n",
    "print(f\"    New Features: 'competition_intensity', 'min_age', 'max_age', 'min_p_value'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf3d97",
   "metadata": {},
   "source": [
    "### 8. Quality Assurance and Encoding Strategy\n",
    "This block performs a comprehensive audit of the final dataset. It analyzes missing values, cardinality, and data types to generate an automated **Encoding Strategy Report**. This report provides specific recommendations for the machine learning pipeline (e.g., which fields require One-Hot Encoding, Target Encoding, or NLP transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e61e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running Dataset Audit...\n",
      ">>> STARTING ENRICHED AUDIT ON: project_data.csv...\n",
      "   - Loaded 105,884 rows.\n",
      "SUCCESS: Audit report saved to audit_dataset_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. FINAL DATASET AUDIT & ENCODING STRATEGY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Running Dataset Audit...\")\n",
    "\n",
    "INPUT_FILE = 'project_data.csv'\n",
    "OUTPUT_REPORT = 'audit_dataset_report.txt'\n",
    "\n",
    "# Define the Schema & Groups\n",
    "FEATURE_GROUPS = {\n",
    "    \"1. TARGET\": [\"target\", \"overall_status\"],\n",
    "    \"2. NUMERIC (Scale & Log)\": [\n",
    "        \"num_facilities\", \"num_countries\", \"number_of_arms\",\n",
    "        \"start_year\", \"competition_niche\", \"competition_broad\"\n",
    "    ],\n",
    "    \"3. ORDINAL (Keep Numeric)\": [\"phase_ordinal\"],\n",
    "    \"4. CATEGORICAL (One-Hot Encode)\": [\n",
    "        \"gender\", \"healthy_volunteers\", \"adult\", \"child\", \"older_adult\",\n",
    "        \"agency_class\", \"includes_us\", \"covid_exposure\",\n",
    "        \"masking\", \"allocation\", \"intervention_model\", \"primary_purpose\",\n",
    "        \"therapeutic_area\"\n",
    "    ],\n",
    "    \"5. HIGH CARDINALITY (Target Encode)\": [\n",
    "        \"therapeutic_subgroup_name\", \"best_pathology\"\n",
    "    ],\n",
    "    \"6. TEXT (NLP / BERT)\": [\n",
    "        \"official_title\", \"criteria\", \"brief_summary\", \"detailed_description\"\n",
    "    ],\n",
    "    \"7. EXCLUDED (Leakage/Analysis)\": [\"min_p_value\", \"why_stopped\"]\n",
    "}\n",
    "\n",
    "def recommend_encoding(col, dtype, unique_count, is_text=False):\n",
    "    \"\"\"Logic to suggest the best Scikit-Learn transformer.\"\"\"\n",
    "    if col == 'target': return \"LABEL (Do not process)\"\n",
    "    if col in ['overall_status', 'why_stopped', 'min_p_value']: return \"DROP (Leakage/Analysis)\"\n",
    "\n",
    "    if is_text:\n",
    "        return \"NLP (BERT Embeddings or TF-IDF)\"\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(dtype):\n",
    "        if unique_count < 10: return \"ORDINAL / PASSTHROUGH\"\n",
    "        return \"NUMERIC (StandardScaler + Log1p if skewed)\"\n",
    "\n",
    "    if unique_count <= 2: return \"BINARY (One-Hot drop='if_binary')\"\n",
    "    if unique_count < 50: return \"ONE-HOT ENCODING\"\n",
    "    return \"TARGET ENCODING (High Cardinality)\"\n",
    "\n",
    "def run_enriched_audit():\n",
    "    print(f\">>> STARTING ENRICHED AUDIT ON: {INPUT_FILE}...\")\n",
    "\n",
    "    file_path = os.path.join(DATA_PATH, INPUT_FILE)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[ERROR] File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    print(f\"   - Loaded {len(df):,} rows.\")\n",
    "\n",
    "    with open(os.path.join(DATA_PATH, OUTPUT_REPORT), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"================================================================\\n\")\n",
    "        f.write(f\"FINAL DATASET AUDIT & STRATEGY REPORT\\n\")\n",
    "        f.write(f\"File: {INPUT_FILE}\\n\")\n",
    "        f.write(f\"Rows: {len(df):,}\\n\")\n",
    "        f.write(f\"Cols: {len(df.columns)}\\n\")\n",
    "        f.write(\"================================================================\\n\\n\")\n",
    "\n",
    "        for category, columns in FEATURE_GROUPS.items():\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"GROUP: {category}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    f.write(f\"\\n[MISSING] '{col}' not found.\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Stats\n",
    "                missing = df[col].isna().sum()\n",
    "                missing_pct = (missing / len(df)) * 100\n",
    "                unique = df[col].nunique()\n",
    "                dtype = df[col].dtype\n",
    "\n",
    "                # Determine Strategy\n",
    "                is_text = col in FEATURE_GROUPS[\"6. TEXT (NLP / BERT)\"]\n",
    "                strategy = recommend_encoding(col, dtype, unique, is_text)\n",
    "\n",
    "                f.write(f\"\\n>>> FIELD: {col.upper()}\\n\")\n",
    "                f.write(f\"    Type: {dtype} | Unique: {unique} | Missing: {missing_pct:.2f}%\\n\")\n",
    "                f.write(f\"    STRATEGY: {strategy}\\n\")\n",
    "\n",
    "                # Content Analysis\n",
    "                if is_text:\n",
    "                    avg_words = df[col].astype(str).apply(lambda x: len(x.split()) if pd.notna(x) else 0).mean()\n",
    "                    f.write(f\"    - Avg Length: {avg_words:.0f} words\\n\")\n",
    "\n",
    "                elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    stats = df[col].describe()\n",
    "                    zeros = (df[col] == 0).sum()\n",
    "                    f.write(f\"    - Stats: Mean={stats['mean']:.2f}, Min={stats['min']}, Max={stats['max']}\\n\")\n",
    "                    f.write(f\"    - Zeros: {zeros} ({zeros/len(df):.1%})\\n\")\n",
    "                    if col != 'target':\n",
    "                        corr = df[[col, 'target']].corr().iloc[0,1]\n",
    "                        f.write(f\"    - Correlation w/ Target: {corr:.4f}\\n\")\n",
    "\n",
    "                else:\n",
    "                    # Categorical Distribution\n",
    "                    top_n = df[col].value_counts().head(5)\n",
    "                    f.write(\"    - Top Values:\\n\")\n",
    "                    for val, count in top_n.items():\n",
    "                        f.write(f\"      * {str(val)[:30]:<30} : {count} ({count/len(df):.1%})\\n\")\n",
    "\n",
    "    print(f\"SUCCESS: Audit report saved to {OUTPUT_REPORT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_enriched_audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915adc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> STARTING TEXT STRATEGY AUDIT...\n",
      ">>> Data Path: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      ">>> Building Filtered Cohort (Interventional Drugs, Phase 1-3, 2000+)...\n",
      "   - Final Cohort Size: 119201 trials\n",
      ">>> Loading Text Candidates...\n",
      ">>> Calculating International Flag...\n",
      ">>> Generating Report: text_strategy_audit.txt...\n",
      ">>> DONE. Please upload 'text_strategy_audit.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & PATHS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Robust path finding\n",
    "if os.path.exists(\"data\"):\n",
    "    DATA_PATH = \"data\"\n",
    "elif os.path.exists(\"../../data\"):\n",
    "    DATA_PATH = \"../../data\"\n",
    "else:\n",
    "    DATA_PATH = \"/home/delaunan/code/delaunan/clintrialpredict/data\"\n",
    "\n",
    "OUTPUT_REPORT = \"text_strategy_audit.txt\"\n",
    "\n",
    "AACT_PARAMS = {\n",
    "    \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "    \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "}\n",
    "\n",
    "print(f\">>> STARTING TEXT STRATEGY AUDIT...\")\n",
    "print(f\">>> Data Path: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. BUILD THE COHORT (Apply Filters)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Building Filtered Cohort (Interventional Drugs, Phase 1-3, 2000+)...\")\n",
    "\n",
    "# Load Studies\n",
    "cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date', 'official_title']\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'studies.txt'), usecols=cols_studies, **AACT_PARAMS)\n",
    "\n",
    "# Filters\n",
    "df = df[df['study_type'] == 'INTERVENTIONAL']\n",
    "df = df[df['overall_status'].isin(['COMPLETED', 'TERMINATED', 'WITHDRAWN', 'SUSPENDED'])]\n",
    "df = df[~df['phase'].isin(['EARLY_PHASE1', 'PHASE4', 'NA'])]\n",
    "\n",
    "# Date Filter\n",
    "df['start_year'] = pd.to_datetime(df['start_date'], errors='coerce').dt.year\n",
    "current_year = pd.Timestamp.now().year\n",
    "df = df[df['start_year'].between(2000, current_year + 2)]\n",
    "\n",
    "# Drug Filter\n",
    "df_int = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'), usecols=['nct_id', 'intervention_type'], **AACT_PARAMS)\n",
    "drug_ids = df_int[df_int['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]['nct_id'].unique()\n",
    "df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "print(f\"   - Final Cohort Size: {len(df)} trials\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. LOAD & MERGE CANDIDATE FIELDS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> Loading Text Candidates...\")\n",
    "\n",
    "# A. Criteria (The Gold Standard for Complexity)\n",
    "df_elig = pd.read_csv(os.path.join(DATA_PATH, 'eligibilities.txt'), usecols=['nct_id', 'criteria'], **AACT_PARAMS)\n",
    "df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "# B. Keywords (For XGBoost Tags)\n",
    "df_keys = pd.read_csv(os.path.join(DATA_PATH, 'keywords.txt'), usecols=['nct_id', 'name'], **AACT_PARAMS)\n",
    "keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "\n",
    "# C. Intervention Details (Name = Tag, Desc = Complexity)\n",
    "df_int_det = pd.read_csv(os.path.join(DATA_PATH, 'interventions.txt'),\n",
    "                         usecols=['nct_id', 'intervention_type', 'name', 'description'],\n",
    "                         **AACT_PARAMS)\n",
    "# Filter for drugs only in the details\n",
    "df_int_det = df_int_det[df_int_det['intervention_type'].str.upper().isin(['DRUG', 'BIOLOGICAL'])]\n",
    "\n",
    "int_names = df_int_det.groupby('nct_id')['name'].apply(lambda x: \" | \".join(x.dropna().astype(str))).reset_index(name='txt_intervention_names')\n",
    "int_desc = df_int_det.groupby('nct_id')['description'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_intervention_desc')\n",
    "\n",
    "df = df.merge(int_names, on='nct_id', how='left')\n",
    "df = df.merge(int_desc, on='nct_id', how='left')\n",
    "\n",
    "# D. International Flag (Calculated)\n",
    "print(\">>> Calculating International Flag...\")\n",
    "df_countries = pd.read_csv(os.path.join(DATA_PATH, 'countries.txt'), usecols=['nct_id', 'name'], **AACT_PARAMS)\n",
    "country_counts = df_countries.groupby('nct_id')['name'].nunique().reset_index(name='cnt')\n",
    "df = df.merge(country_counts, on='nct_id', how='left')\n",
    "df['is_international'] = (df['cnt'] > 1).astype(int)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. GENERATE AUDIT REPORT\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\">>> Generating Report: {OUTPUT_REPORT}...\")\n",
    "\n",
    "fields_to_audit = {\n",
    "    \"OFFICIAL_TITLE\": \"official_title\",\n",
    "    \"CRITERIA\": \"criteria\",\n",
    "    \"KEYWORDS (Grouped)\": \"txt_keywords\",\n",
    "    \"INTERVENTION_NAMES\": \"txt_intervention_names\",\n",
    "    \"INTERVENTION_DESC\": \"txt_intervention_desc\",\n",
    "    \"IS_INTERNATIONAL (Flag)\": \"is_international\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(DATA_PATH, OUTPUT_REPORT), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"================================================================\\n\")\n",
    "    f.write(\"TEXT STRATEGY AUDIT (Filtered Cohort)\\n\")\n",
    "    f.write(f\"Total Trials: {len(df)}\\n\")\n",
    "    f.write(\"================================================================\\n\\n\")\n",
    "\n",
    "    for label, col in fields_to_audit.items():\n",
    "        f.write(f\"FIELD: {label}\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "        if col not in df.columns:\n",
    "            f.write(\"‚ùå MISSING (Not found in dataframe)\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        # Stats\n",
    "        missing = df[col].isna().sum()\n",
    "        fill_rate = 100 - ((missing / len(df)) * 100)\n",
    "\n",
    "        f.write(f\"Fill Rate: {fill_rate:.2f}%\\n\")\n",
    "\n",
    "        if col == 'is_international':\n",
    "            dist = df[col].value_counts(normalize=True)\n",
    "            f.write(f\"Distribution: No (0)={dist.get(0,0):.1%}, Yes (1)={dist.get(1,0):.1%}\\n\")\n",
    "        else:\n",
    "            # Text Stats\n",
    "            # Fill NA with empty for length calc\n",
    "            series = df[col].fillna(\"\").astype(str)\n",
    "            avg_chars = series.apply(len).mean()\n",
    "            avg_words = series.apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "            f.write(f\"Avg Length: {avg_chars:.0f} chars ({avg_words:.0f} words)\\n\")\n",
    "\n",
    "            # Recommendation Logic\n",
    "            rec = \"???\"\n",
    "            if avg_words < 5: rec = \"‚ö†Ô∏è TOO SHORT / SPARSE\"\n",
    "            elif avg_words < 50: rec = \"üè∑Ô∏è KEYWORD CANDIDATE (XGBoost/TF-IDF)\"\n",
    "            else: rec = \"üß† COMPLEXITY CANDIDATE (BERT)\"\n",
    "\n",
    "            f.write(f\"VERDICT:    {rec}\\n\")\n",
    "\n",
    "            # Samples\n",
    "            f.write(\"Samples:\\n\")\n",
    "            non_empty = df[df[col].notna()][col]\n",
    "            if len(non_empty) > 0:\n",
    "                for val in non_empty.sample(3).values:\n",
    "                    preview = str(val)[:100].replace('\\n', ' ')\n",
    "                    f.write(f\"  - {preview}...\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\">>> DONE. Please upload 'text_strategy_audit.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db21c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
