{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44cbd06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679236db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b92c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING PREDICTION COHORT GENERATION ---\n",
      ">>> 1. Loading Full Cohort (2005-2025) for Competition Calculation...\n",
      "    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\n",
      "    -> Calculating Competition...\n",
      "    Full Cohort Size: 98143 trials.\n",
      ">>> 2. Filtering for Prediction Set (2024-2025, Active/Pending Statuses)...\n",
      "    Prediction Cohort: 10422 trials (Active/Pending Phase 2/3, 2024-2025)\n",
      ">>> 3. Engineering Remaining Features for Prediction...\n",
      "    -> Engineering Sponsor Tiers...\n",
      "    -> Engineering Protocol Complexity (Calculating Age Flags)...\n",
      "    -> Engineering Agent Type (Bulletproof Classifier)...\n",
      "    -> Engineering Smart Patterns (Rigor & Strictness)...\n",
      "    -> Engineering Safe Protocol Features...\n",
      "    -> Preparing Text Features...\n",
      "    -> Skipping embedding attachment for parallel workflow.\n",
      "    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\n",
      ">>> Saved 10422 rows to /home/delaunan/code/delaunan/clintrialpredict/data/data_predict.csv\n",
      "\n",
      "[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION (THE ONLY LINE YOU MUST VERIFY)\n",
    "# ====================================================================\n",
    "# This MUST be the absolute path to the FOLDER containing your AACT .txt files.\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING PREDICTION COHORT GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS (Full Definition)\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    \"\"\"\n",
    "    A specialized loader for generating the PROSPECTIVE prediction dataset (2024-2025).\n",
    "    It ensures feature consistency and excludes all post-hoc/leakage features.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "\n",
    "        # --- STRATEGY A: PERFECT ---\n",
    "        self.params_perfect = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "        # --- STRATEGY B: ROBUST ---\n",
    "        self.params_robust = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"   [!] Warning: File not found {filename}. Features will be empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except Exception as e:\n",
    "            print(f\"   [!] Formatting error in {filename}. Switching to Robust Mode...\")\n",
    "            try:\n",
    "                return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except Exception as e2:\n",
    "                print(f\"   [x] CRITICAL: Could not load {filename}. Error: {e2}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "    # --- STEP 1: LOAD FULL COHORT FOR TIME-SENSITIVE FEATURES ---\n",
    "\n",
    "    def load_full_cohort_for_competition(self, min_year: int = 2005, max_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the entire relevant dataset (2005-2025) to calculate time-sensitive\n",
    "        features (Competition) on the complete historical context before splitting.\n",
    "        \"\"\"\n",
    "        print(f\">>> 1. Loading Full Cohort ({min_year}-{max_year}) for Competition Calculation...\")\n",
    "\n",
    "        # 1. Load Core Columns\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase',\n",
    "                        'start_date', 'number_of_arms',\n",
    "                        'official_title', 'why_stopped',\n",
    "                        'has_dmc', 'is_fda_regulated_drug', 'brief_title']\n",
    "\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Critical Error: 'studies.txt' failed to load.\")\n",
    "\n",
    "        # 2. Apply Core Filters (Interventional, Drug, Phase 2/3)\n",
    "        df = df[df['study_type'].str.upper() == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "        cols_int = ['nct_id', 'intervention_type', 'name']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            drug_ids = df_int[df_int['intervention_type'].str.upper().isin(target_types)]['nct_id'].unique()\n",
    "            df = df[df['nct_id'].isin(drug_ids)]\n",
    "            self.df_drugs = df_int[df_int['intervention_type'].str.upper().isin(target_types + ['DIETARY SUPPLEMENT', 'OTHER'])].copy()\n",
    "\n",
    "        excluded_phases = ['EARLY_PHASE1', 'PHASE1', 'PHASE4', 'NA']\n",
    "        df = df[~df['phase'].astype(str).str.upper().isin(excluded_phases)]\n",
    "        df = df.dropna(subset=['phase'])\n",
    "\n",
    "        # 3. Apply Full Date Range Filter (2005-2025)\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "        df = df[df['start_year'].between(min_year, max_year)].copy()\n",
    "\n",
    "        # 4. Calculate Time-Sensitive Features (Hierarchy and Competition)\n",
    "        df = self._attach_medical_hierarchy(df)\n",
    "        df = self._calculate_competition(df)\n",
    "\n",
    "        print(f\"    Full Cohort Size: {len(df)} trials.\")\n",
    "        return df.copy()\n",
    "\n",
    "\n",
    "    # --- STEP 2: FILTER AND FINALIZE PREDICTION SET ---\n",
    "\n",
    "    def load_and_clean_prediction_set(self, df_full_cohort: pd.DataFrame, start_year_predict: int = 2024) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters the full cohort for the Prediction Set: Active/Pending Statuses, Year >= 2024.\n",
    "        \"\"\"\n",
    "        df = df_full_cohort.copy()\n",
    "        print(f\">>> 2. Filtering for Prediction Set ({start_year_predict}-2025, Active/Pending Statuses)...\")\n",
    "\n",
    "        # 1. Filter: Date Range (2024-2025)\n",
    "        df = df[df['start_year'] >= start_year_predict].copy()\n",
    "\n",
    "        # 2. Filter: Status (Active/Pending Only)\n",
    "        ACTIVE_STATUSES = [\n",
    "            'RECRUITING', 'NOT_YET_RECRUITING', 'ACTIVE_NOT_RECRUITING',\n",
    "            'ENROLLING_BY_INVITATION', 'WITHHELD', 'UNKNOWN'\n",
    "        ]\n",
    "\n",
    "        df_upper = df['overall_status'].astype(str).str.upper()\n",
    "        df = df[df_upper.isin(ACTIVE_STATUSES)].copy()\n",
    "\n",
    "        # 3. Filter: COVID Sanitizer (Kept for consistency)\n",
    "        if 'why_stopped' in df.columns:\n",
    "            covid_keywords = ['covid', 'pandemic', 'coronavirus', 'sars-cov-2', 'logistical reasons']\n",
    "            mask_covid = df['why_stopped'].fillna('').astype(str).str.lower().apply(\n",
    "                lambda x: any(k in x for k in covid_keywords)\n",
    "            )\n",
    "            if mask_covid.sum() > 0:\n",
    "                print(f\"    [Sanitizer] Dropping {mask_covid.sum()} trials terminated due to COVID/Logistics.\")\n",
    "                df = df[~mask_covid]\n",
    "\n",
    "        # 4. Target: The prediction set has NO target.\n",
    "        if 'target' in df.columns:\n",
    "             df.drop(columns=['target'], inplace=True)\n",
    "\n",
    "        print(f\"    Prediction Cohort: {len(df)} trials (Active/Pending Phase 2/3, {start_year_predict}-2025)\")\n",
    "\n",
    "        # 5. Add remaining features and user-facing text\n",
    "        df = self.add_features_for_prediction(df)\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    # --- NEW PREDICTION FEATURE FUNCTION (STEP 3) ---\n",
    "\n",
    "    def add_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds remaining features (non-time-sensitive) and user-facing fields.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        print(\">>> 3. Engineering Remaining Features for Prediction...\")\n",
    "\n",
    "        # 1. Operational Flags\n",
    "        df['covid_exposure'] = df['start_year'].between(2019, 2022).astype(int)\n",
    "\n",
    "        # 2. Geography (SAFE: Only is_us)\n",
    "        df_countries = self._safe_load('countries.txt', cols=['nct_id', 'name'])\n",
    "        if not df_countries.empty:\n",
    "            us_trials = df_countries[df_countries['name'] == 'United States']['nct_id'].unique()\n",
    "            df['includes_us'] = df['nct_id'].isin(us_trials).astype(int)\n",
    "        else:\n",
    "            df['includes_us'] = 0\n",
    "\n",
    "        # 3. Merge Standard Metadata\n",
    "        df = self._merge_file(df, 'designs.txt', ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose'])\n",
    "        # NOTE: This line adds 'number_of_primary_outcomes_to_measure'\n",
    "        df = self._merge_file(df, 'calculated_values.txt', ['nct_id', 'number_of_primary_outcomes_to_measure'])\n",
    "\n",
    "        # 4. Sponsor Engineering\n",
    "        df = self._engineer_sponsor_features(df)\n",
    "\n",
    "        # 5. Complexity Engineering (Loads eligibility.txt and 'criteria' column)\n",
    "        df = self._engineer_complexity(df)\n",
    "\n",
    "        # 6. Agent Type\n",
    "        df = self._engineer_agent_type(df)\n",
    "\n",
    "        # 7. Smart Patterns (Rigor & Strictness)\n",
    "        df = self._engineer_smart_patterns(df)\n",
    "\n",
    "        # 8. Safe Features (DMC)\n",
    "        df = self._engineer_safe_features(df)\n",
    "\n",
    "        # 9. Text Features (Creates txt_tags, txt_criteria)\n",
    "        df = self._prepare_text(df)\n",
    "\n",
    "        # 10. ATTACH EMBEDDINGS (TEMPORARILY SKIPPED FOR COLLEAGUE'S WORKFLOW)\n",
    "        print(\"    -> Skipping embedding attachment for parallel workflow.\")\n",
    "\n",
    "        # 11. Attach User-Facing Text Fields\n",
    "        df = self._load_user_facing_text(df)\n",
    "\n",
    "        # --- CLEANUP (FIXED LOGIC) ---\n",
    "        old_col = 'number_of_primary_outcomes_to_measure'\n",
    "        new_col = 'num_primary_endpoints'\n",
    "\n",
    "        if old_col in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            # Convert to numeric and fill NaNs in the new column\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(1)\n",
    "        else:\n",
    "            # If the column was never loaded, create it with the default value 1\n",
    "            df[new_col] = 1\n",
    "        # -----------------------------\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- NEW HELPER FUNCTION FOR UI/USER-FACING DATA ---\n",
    "\n",
    "    def _load_user_facing_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads text fields that are useful for the UI but not necessarily for the model.\n",
    "        \"\"\"\n",
    "        print(\"    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\")\n",
    "\n",
    "        # 1. Brief Summary\n",
    "        cols_brief = ['nct_id', 'description']\n",
    "        df_brief = self._safe_load('brief_summaries.txt', cols=cols_brief)\n",
    "        if not df_brief.empty:\n",
    "            df_brief = df_brief.rename(columns={'description': 'brief_summary_text'})\n",
    "            df = df.merge(df_brief.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['brief_summary_text'] = \"\"\n",
    "\n",
    "        # 2. Detailed Description\n",
    "        cols_detailed = ['nct_id', 'description']\n",
    "        df_detailed = self._safe_load('detailed_descriptions.txt', cols=cols_detailed)\n",
    "        if not df_detailed.empty:\n",
    "            df_detailed = df_detailed.rename(columns={'description': 'detailed_description_text'})\n",
    "            df = df.merge(df_detailed.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['detailed_description_text'] = \"\"\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- ALL OTHER NECESSARY HELPER METHODS (KEPT FOR FUNCTIONALITY) ---\n",
    "\n",
    "    def _engineer_smart_patterns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Smart Patterns (Rigor & Strictness)...\")\n",
    "        def get_masking_score(val):\n",
    "            val = str(val).lower()\n",
    "            if 'quadruple' in val: return 3\n",
    "            if 'double' in val: return 2\n",
    "            if 'single' in val: return 1\n",
    "            return 0\n",
    "        def get_allocation_score(val):\n",
    "            return 1 if 'randomized' in str(val).lower() else 0\n",
    "        def get_model_score(val):\n",
    "            val = str(val).lower()\n",
    "            return 1 if 'crossover' in val or 'factorial' in val else 0\n",
    "        df['score_masking'] = df['masking'].apply(get_masking_score)\n",
    "        df['score_allocation'] = df['allocation'].apply(get_allocation_score)\n",
    "        df['score_model'] = df['intervention_model'].apply(get_model_score)\n",
    "        df['design_rigor_score'] = df['score_masking'] + df['score_allocation'] + df['score_model']\n",
    "        df['is_gender_restricted'] = df['gender'].apply(lambda x: 0 if str(x).lower() == 'all' else 1)\n",
    "        df['is_sick_only'] = df['healthy_volunteers'].apply(lambda x: 1 if str(x).lower() == 'no' else 0)\n",
    "        for col in ['child', 'adult', 'older_adult']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(lambda x: 1 if x.lower() in ['true', '1', 'yes'] else 0)\n",
    "            else:\n",
    "                df[col] = 1\n",
    "        df['eligibility_strictness_score'] = (\n",
    "            df['is_gender_restricted'] +\n",
    "            df['is_sick_only'] +\n",
    "            (1 - df['child']) +\n",
    "            (1 - df['older_adult'])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _engineer_agent_type(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Agent Type (Bulletproof Classifier)...\")\n",
    "        if self.df_drugs.empty:\n",
    "            df['agent_category'] = 'UNKNOWN'\n",
    "            return df\n",
    "        type_priority = {'GENETIC': 1,'BIOLOGICAL': 2,'DRUG': 3,'DIETARY SUPPLEMENT': 4,'OTHER': 5}\n",
    "        df_int = self.df_drugs.copy()\n",
    "        df_int['type_upper'] = df_int['intervention_type'].str.upper()\n",
    "        df_int['priority'] = df_int['type_upper'].map(lambda x: type_priority.get(x, 5))\n",
    "        df_int = df_int.sort_values('priority')\n",
    "        best_types = df_int.drop_duplicates('nct_id')[['nct_id', 'type_upper', 'name']]\n",
    "        df = df.merge(best_types, on='nct_id', how='left')\n",
    "        def classify_molecule(row):\n",
    "            itype = str(row['type_upper'])\n",
    "            name = str(row['name']).lower()\n",
    "            if 'placebo' in name: return 'PLACEBO_CTRL'\n",
    "            if itype == 'GENETIC': return 'GENE_THERAPY'\n",
    "            if any(x in name for x in ['car-t', 'chimeric antigen', 'autologous', 'allogeneic', 't-cell', 'nk cell']):\n",
    "                return 'CELL_THERAPY'\n",
    "            if name.endswith('cel'): return 'CELL_THERAPY'\n",
    "            if any(x in name for x in ['crispr', 'cas9', 'mrna', 'sirna', 'antisense', 'oligonucleotide', 'plasmid', 'vector', 'aav']):\n",
    "                return 'RNA_GENE_THERAPY'\n",
    "            if itype == 'BIOLOGICAL': return 'BIOLOGIC'\n",
    "            if 'mab' in name:\n",
    "                if 'adc' in name or 'conjugate' in name: return 'ANTIBODY_DRUG_CONJUGATE'\n",
    "                return 'MONOCLONAL_ANTIBODY'\n",
    "            if name.endswith('cept'): return 'BIOLOGIC_FUSION'\n",
    "            if 'vaccine' in name: return 'VACCINE'\n",
    "            if any(x in name for x in ['interferon', 'interleukin', 'cytokine']): return 'IMMUNOTHERAPY'\n",
    "            if name.endswith('ib') or 'ib ' in name:\n",
    "                if 'tinib' in name: return 'KINASE_INHIBITOR_TYROSINE'\n",
    "                if 'parib' in name: return 'PARP_INHIBITOR'\n",
    "                if 'lisib' in name: return 'PI3K_INHIBITOR'\n",
    "                return 'TARGETED_KINASE_INHIBITOR'\n",
    "            if 'vastatin' in name: return 'STATIN_CHOLESTEROL'\n",
    "            if name.endswith('stat') or 'stat ' in name: return 'ENZYME_INHIBITOR'\n",
    "            if name.endswith('degib'): return 'HEDGEHOG_INHIBITOR'\n",
    "            if name.endswith('clax'): return 'BCL2_INHIBITOR'\n",
    "            chemo_stems = ['platin', 'taxel', 'rubicin', 'fluorouracil', 'gemcitabine',\n",
    "                           'cyclophosphamide', 'methotrexate', 'etoposide', 'vincristine', 'vinblastine']\n",
    "            if any(x in name for x in chemo_stems):\n",
    "                return 'CHEMOTHERAPY'\n",
    "            return 'SMALL_MOLECULE_OTHER'\n",
    "        df['agent_category'] = df.apply(classify_molecule, axis=1)\n",
    "        df.drop(columns=['type_upper', 'priority', 'name_y'], inplace=True, errors='ignore')\n",
    "        if 'name_x' in df.columns: df.rename(columns={'name_x': 'official_title'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _engineer_safe_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Safe Protocol Features...\")\n",
    "        for col in ['has_dmc', 'is_fda_regulated_drug']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(\n",
    "                    lambda x: 1 if x.lower() in ['true', 't', '1', 'yes'] else 0\n",
    "                )\n",
    "            else:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def _attach_medical_hierarchy(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\")\n",
    "        cols_bridge = ['nct_id', 'mesh_term']\n",
    "        df_bridge = self._safe_load('browse_conditions.txt', cols=cols_bridge)\n",
    "        if df_bridge.empty:\n",
    "            cols_cond = ['nct_id', 'name']\n",
    "            df_bridge = self._safe_load('conditions.txt', cols=cols_cond)\n",
    "            if not df_bridge.empty:\n",
    "                df_bridge.rename(columns={'name': 'mesh_term'}, inplace=True)\n",
    "        mesh_path = os.path.join(self.data_path, 'mesh_lookup.csv')\n",
    "        df_dictionary = pd.DataFrame()\n",
    "        if os.path.exists(mesh_path):\n",
    "            try:\n",
    "                df_dictionary = pd.read_csv(mesh_path, sep='|', on_bad_lines='skip')\n",
    "                if 'mesh_term' in df_dictionary.columns and 'therapeutic_area' in df_dictionary.columns:\n",
    "                    df_dictionary = df_dictionary[['mesh_term', 'therapeutic_area']].drop_duplicates()\n",
    "                    df_dictionary.rename(columns={'therapeutic_area': 'lookup_area'}, inplace=True)\n",
    "                else:\n",
    "                    df_dictionary = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"       [!] Error reading mesh_lookup.csv: {e}\")\n",
    "        if not df_bridge.empty:\n",
    "            if not df_dictionary.empty:\n",
    "                df_full_mesh = df_bridge.merge(df_dictionary, on='mesh_term', how='left')\n",
    "            else:\n",
    "                df_full_mesh = df_bridge\n",
    "                df_full_mesh['lookup_area'] = np.nan\n",
    "            df_grouped = df_full_mesh.groupby('nct_id').agg({\n",
    "                'mesh_term': 'first',\n",
    "                'lookup_area': 'first'\n",
    "            }).reset_index()\n",
    "            df = df.merge(df_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['mesh_term'] = np.nan\n",
    "            df['lookup_area'] = np.nan\n",
    "        smart_path = os.path.join(self.data_path, 'smart_pathology_lookup.csv')\n",
    "        if os.path.exists(smart_path):\n",
    "            try:\n",
    "                df_smart = pd.read_csv(smart_path)\n",
    "                if 'nct_id' in df_smart.columns:\n",
    "                    df = df.merge(df_smart, on='nct_id', how='left')\n",
    "            except:\n",
    "                pass\n",
    "        if 'best_pathology' not in df.columns: df['best_pathology'] = np.nan\n",
    "        if 'therapeutic_area' not in df.columns: df['therapeutic_area'] = np.nan\n",
    "        def get_final_category(row):\n",
    "            if pd.notna(row.get('best_pathology')) and str(row.get('best_pathology')) != 'Unknown':\n",
    "                return row['best_pathology']\n",
    "            if pd.notna(row.get('lookup_area')) and str(row.get('lookup_area')) != 'Other/Unclassified':\n",
    "                return row['lookup_area']\n",
    "            if pd.notna(row.get('mesh_term')):\n",
    "                return row['mesh_term']\n",
    "            if pd.notna(row.get('therapeutic_area')) and str(row.get('therapeutic_area')) != 'Other':\n",
    "                return row['therapeutic_area']\n",
    "            return 'Unclassified Condition'\n",
    "        df['therapeutic_subgroup_name'] = df.apply(get_final_category, axis=1)\n",
    "        df.drop(columns=['mesh_term', 'lookup_area'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _engineer_sponsor_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Sponsor Tiers...\")\n",
    "        cols_needed = ['nct_id', 'lead_or_collaborator', 'name', 'agency_class']\n",
    "        df_sponsors = self._safe_load('sponsors.txt', cols=cols_needed)\n",
    "        if df_sponsors.empty:\n",
    "            df['sponsor_tier'] = 'TIER_2_OTHER'\n",
    "            df['sponsor_clean'] = 'UNKNOWN'\n",
    "            df['agency_class'] = 'UNKNOWN'\n",
    "            return df\n",
    "        leads = df_sponsors[df_sponsors['lead_or_collaborator'].str.lower() == 'lead'][['nct_id', 'name', 'agency_class']]\n",
    "        leads = leads.rename(columns={'name': 'lead_sponsor'})\n",
    "        leads = leads.drop_duplicates('nct_id')\n",
    "        df = df.merge(leads, on='nct_id', how='left')\n",
    "        df['lead_sponsor'] = df['lead_sponsor'].fillna('UNKNOWN')\n",
    "        df['agency_class'] = df['agency_class'].fillna('UNKNOWN')\n",
    "        clean_col = df['lead_sponsor'].astype(str).str.lower().str.strip()\n",
    "        legal_pattern = r'[.,]|\\binc\\b|\\bltd\\b|\\bllc\\b|\\bcorp\\b|\\bgmbh\\b|\\bsa\\b|\\bplc\\b'\n",
    "        clean_col = clean_col.str.replace(legal_pattern, '', regex=True).str.strip()\n",
    "        mappings = {\n",
    "            'Pfizer': ['pfizer', 'wyeth', 'hospira'], 'GSK': ['glaxo', 'gsk', 'smithkline'],\n",
    "            'Novartis': ['novartis', 'sandoz'], 'AstraZeneca': ['astrazeneca', 'medimmune'],\n",
    "            'Merck': ['merck', 'msd'], 'Roche': ['roche', 'genentech', 'hoffmann'],\n",
    "            'Sanofi': ['sanofi', 'aventis', 'genzyme'], 'J&J': ['johnson & johnson', 'janssen'],\n",
    "            'Bayer': ['bayer', 'monsanto'], 'Boehringer': ['boehringer'],\n",
    "            'BMS': ['bristol-myers', 'squibb', 'celgene'], 'Lilly': ['lilly'],\n",
    "            'Abbott': ['abbott', 'abbvie'], 'Amgen': ['amgen'],\n",
    "            'Takeda': ['takeda', 'shire'], 'Gilead': ['gilead'],\n",
    "            'Novo Nordisk': ['novo nordisk'], 'NIH': ['national cancer institute', 'nci', 'national institutes of health', 'nih']\n",
    "        }\n",
    "        final_names = clean_col.copy()\n",
    "        for std, keys in mappings.items():\n",
    "            pattern = '|'.join(keys)\n",
    "            mask = clean_col.str.contains(pattern, case=False, regex=True)\n",
    "            final_names.loc[mask] = std\n",
    "        df['sponsor_clean'] = final_names\n",
    "        def get_tier(name):\n",
    "            if name in mappings.keys(): return 'TIER_1_GIANT'\n",
    "            return 'TIER_2_OTHER'\n",
    "        df['sponsor_tier'] = df['sponsor_clean'].apply(get_tier)\n",
    "        return df\n",
    "\n",
    "    def _engineer_complexity(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Protocol Complexity (Calculating Age Flags)...\")\n",
    "        cols_needed = ['nct_id', 'criteria', 'gender', 'healthy_volunteers', 'minimum_age', 'maximum_age']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_needed)\n",
    "        if df_elig.empty:\n",
    "            df['criteria_len_log'] = 0\n",
    "            for c in ['gender', 'healthy_volunteers', 'adult', 'child', 'older_adult']: df[c] = 0\n",
    "            return df\n",
    "        df_elig = df_elig.drop_duplicates('nct_id')\n",
    "        df = df.merge(df_elig, on='nct_id', how='left')\n",
    "        df['criteria_len_log'] = np.log1p(df['criteria'].astype(str).str.len().fillna(0))\n",
    "        df['healthy_volunteers'] = df['healthy_volunteers'].astype(str).str.lower().apply(\n",
    "            lambda x: 'no' if x in ['f', 'false', '0', 'no', 'nan'] else 'yes'\n",
    "        )\n",
    "        df['gender'] = df['gender'].fillna('UNKNOWN')\n",
    "        def parse_age_to_years(val, default_val):\n",
    "            if pd.isna(val) or str(val).lower() in ['n/a', 'nan', '', 'none']: return default_val\n",
    "            try:\n",
    "                match = re.search(r'(\\d+(\\.\\d+)?)', str(val))\n",
    "                if not match: return default_val\n",
    "                num = float(match.group(1))\n",
    "                text = str(val).lower()\n",
    "                if 'month' in text: num /= 12.0\n",
    "                elif 'week' in text: num /= 52.0\n",
    "                elif 'day' in text: num /= 365.0\n",
    "                elif 'hour' in text: num /= 8760.0\n",
    "                return num\n",
    "            except:\n",
    "                return default_val\n",
    "        df['min_age_years'] = df['minimum_age'].apply(lambda x: parse_age_to_years(x, 0.0))\n",
    "        df['max_age_years'] = df['maximum_age'].apply(lambda x: parse_age_to_years(x, 100.0))\n",
    "        df['child'] = (df['min_age_years'] < 18).astype(int)\n",
    "        df['adult'] = ((df['max_age_years'] >= 18) & (df['min_age_years'] < 65)).astype(int)\n",
    "        df['older_adult'] = (df['max_age_years'] > 65).astype(int)\n",
    "        df.drop(columns=['minimum_age', 'maximum_age', 'min_age_years', 'max_age_years'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _calculate_competition(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Calculating Competition...\")\n",
    "        try:\n",
    "            req_cols = ['start_year', 'therapeutic_area', 'phase']\n",
    "            if not all(col in df.columns for col in req_cols):\n",
    "                df['competition_broad'] = 0\n",
    "                df['competition_niche'] = 0\n",
    "                return df\n",
    "            grid = df.groupby(['start_year', 'therapeutic_area']).size().reset_index(name='count')\n",
    "            lookup = dict(zip(zip(grid['start_year'], grid['therapeutic_area']), grid['count']))\n",
    "            def get_comp(row):\n",
    "                y, area = row['start_year'], row['therapeutic_area']\n",
    "                return lookup.get((y, area), 0) + lookup.get((y-1, area), 0)\n",
    "            df['competition_broad'] = df.apply(get_comp, axis=1)\n",
    "            if 'therapeutic_subgroup_name' in df.columns:\n",
    "                grid_niche = df.groupby(['start_year', 'therapeutic_subgroup_name']).size().reset_index(name='count')\n",
    "                lookup_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup_name']), grid_niche['count']))\n",
    "                def get_niche(row):\n",
    "                    y, sub = row['start_year'], row['therapeutic_subgroup_name']\n",
    "                    return lookup_niche.get((y, sub), 0) + lookup_niche.get((y-1, sub), 0)\n",
    "                df['competition_niche'] = df.apply(get_niche, axis=1)\n",
    "            else:\n",
    "                df['competition_niche'] = 0\n",
    "        except:\n",
    "            df['competition_broad'] = 0\n",
    "            df['competition_niche'] = 0\n",
    "        return df\n",
    "\n",
    "    def _prepare_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Preparing Text Features...\")\n",
    "        df_keys = self._safe_load('keywords.txt', cols=['nct_id', 'name'])\n",
    "        if not df_keys.empty:\n",
    "            keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "            df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_keywords'] = \"\"\n",
    "        if not self.df_drugs.empty:\n",
    "            int_names = self.df_drugs.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_int_names')\n",
    "            df = df.merge(int_names, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_int_names'] = \"\"\n",
    "        text_cols = ['official_title', 'txt_keywords', 'txt_int_names', 'criteria']\n",
    "        for c in text_cols:\n",
    "            if c in df.columns: df[c] = df[c].fillna(\"\")\n",
    "        df['txt_tags'] = (df['official_title'] + \" \" + df['txt_keywords'] + \" \" + df['txt_int_names'])\n",
    "        if 'criteria' in df.columns:\n",
    "            df['txt_criteria'] = df['criteria']\n",
    "            df.drop(columns=['txt_keywords', 'txt_int_names'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    # NOTE: _attach_embeddings is now a placeholder function\n",
    "    def _attach_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Skipping embedding attachment for parallel workflow.\")\n",
    "        return df\n",
    "\n",
    "    # NOTE: _attach_p_values is ONLY called in the original training pipeline (not in prediction)\n",
    "    def _attach_p_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # This function is not used in the prediction pipeline\n",
    "        return df\n",
    "\n",
    "    def _merge_file(self, df: pd.DataFrame, filename: str, cols: list, filter_col: Optional[str] = None, filter_val: Optional[str] = None) -> pd.DataFrame:\n",
    "        try:\n",
    "            aux = self._safe_load(filename, cols=cols + ([filter_col] if filter_col else []))\n",
    "            if aux.empty: return df\n",
    "            if filter_col:\n",
    "                aux = aux[aux[filter_col] == filter_val].drop(columns=[filter_col])\n",
    "            aux = aux.drop_duplicates('nct_id')\n",
    "            return df.merge(aux, on='nct_id', how='left')\n",
    "        except:\n",
    "            return df\n",
    "\n",
    "    def save(self, df: pd.DataFrame, filename: str = 'data_predict.csv'):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION CODE (The part that runs the process)\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # 1. Instantiate the loader\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # --- STEP 1: Load and Calculate Time-Sensitive Features on the Full Cohort (2005-2025) ---\n",
    "    df_full = loader.load_full_cohort_for_competition(min_year=2005, max_year=2025)\n",
    "\n",
    "    # --- STEP 2: Generate Prediction Set (2024-2025, Active/Pending) ---\n",
    "    # Note: start_year_predict=2024 enforces the new requirement\n",
    "    df_predict = loader.load_and_clean_prediction_set(df_full, start_year_predict=2024)\n",
    "\n",
    "    # 3. Save the final dataset for prediction\n",
    "    loader.save(df_predict)\n",
    "\n",
    "    print(\"\\n[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[CRITICAL FAILURE] Data loading stopped: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check if 'studies.txt' is present in the DATA_PATH folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[UNEXPECTED FAILURE] An unexpected error occurred: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check file paths, file names, and data integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3573c53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDICTION DATASET AUDIT: data_predict.csv\n",
      "================================================================================\n",
      "Training Set Size (project_data.csv): 48294 rows x 147 cols\n",
      "Prediction Set Size (data_predict.csv): 10422 rows x 49 cols\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================== COLUMN COMPARISON ==============================\n",
      "Total Common Columns: 44\n",
      "Total NEW Columns in Prediction Set: 5\n",
      "Total MISSING Columns from Training Set: 103\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[A] NEW COLUMNS (User-Facing/NLP Fields)\n",
      "--------------------------------------------------------------------------------\n",
      "- brief_summary_text\n",
      "- brief_title\n",
      "- criteria\n",
      "- detailed_description_text\n",
      "- official_title\n",
      "\n",
      "[B] MISSING COLUMNS (Expected Exclusions)\n",
      "--------------------------------------------------------------------------------\n",
      "- emb_0 (WARNING: Missing from Prediction Set)\n",
      "- emb_1 (WARNING: Missing from Prediction Set)\n",
      "- emb_10 (WARNING: Missing from Prediction Set)\n",
      "- emb_11 (WARNING: Missing from Prediction Set)\n",
      "- emb_12 (WARNING: Missing from Prediction Set)\n",
      "- emb_13 (WARNING: Missing from Prediction Set)\n",
      "- emb_14 (WARNING: Missing from Prediction Set)\n",
      "- emb_15 (WARNING: Missing from Prediction Set)\n",
      "- emb_16 (WARNING: Missing from Prediction Set)\n",
      "- emb_17 (WARNING: Missing from Prediction Set)\n",
      "- emb_18 (WARNING: Missing from Prediction Set)\n",
      "- emb_19 (WARNING: Missing from Prediction Set)\n",
      "- emb_2 (WARNING: Missing from Prediction Set)\n",
      "- emb_20 (WARNING: Missing from Prediction Set)\n",
      "- emb_21 (WARNING: Missing from Prediction Set)\n",
      "- emb_22 (WARNING: Missing from Prediction Set)\n",
      "- emb_23 (WARNING: Missing from Prediction Set)\n",
      "- emb_24 (WARNING: Missing from Prediction Set)\n",
      "- emb_25 (WARNING: Missing from Prediction Set)\n",
      "- emb_26 (WARNING: Missing from Prediction Set)\n",
      "- emb_27 (WARNING: Missing from Prediction Set)\n",
      "- emb_28 (WARNING: Missing from Prediction Set)\n",
      "- emb_29 (WARNING: Missing from Prediction Set)\n",
      "- emb_3 (WARNING: Missing from Prediction Set)\n",
      "- emb_30 (WARNING: Missing from Prediction Set)\n",
      "- emb_31 (WARNING: Missing from Prediction Set)\n",
      "- emb_32 (WARNING: Missing from Prediction Set)\n",
      "- emb_33 (WARNING: Missing from Prediction Set)\n",
      "- emb_34 (WARNING: Missing from Prediction Set)\n",
      "- emb_35 (WARNING: Missing from Prediction Set)\n",
      "- emb_36 (WARNING: Missing from Prediction Set)\n",
      "- emb_37 (WARNING: Missing from Prediction Set)\n",
      "- emb_38 (WARNING: Missing from Prediction Set)\n",
      "- emb_39 (WARNING: Missing from Prediction Set)\n",
      "- emb_4 (WARNING: Missing from Prediction Set)\n",
      "- emb_40 (WARNING: Missing from Prediction Set)\n",
      "- emb_41 (WARNING: Missing from Prediction Set)\n",
      "- emb_42 (WARNING: Missing from Prediction Set)\n",
      "- emb_43 (WARNING: Missing from Prediction Set)\n",
      "- emb_44 (WARNING: Missing from Prediction Set)\n",
      "- emb_45 (WARNING: Missing from Prediction Set)\n",
      "- emb_46 (WARNING: Missing from Prediction Set)\n",
      "- emb_47 (WARNING: Missing from Prediction Set)\n",
      "- emb_48 (WARNING: Missing from Prediction Set)\n",
      "- emb_49 (WARNING: Missing from Prediction Set)\n",
      "- emb_5 (WARNING: Missing from Prediction Set)\n",
      "- emb_50 (WARNING: Missing from Prediction Set)\n",
      "- emb_51 (WARNING: Missing from Prediction Set)\n",
      "- emb_52 (WARNING: Missing from Prediction Set)\n",
      "- emb_53 (WARNING: Missing from Prediction Set)\n",
      "- emb_54 (WARNING: Missing from Prediction Set)\n",
      "- emb_55 (WARNING: Missing from Prediction Set)\n",
      "- emb_56 (WARNING: Missing from Prediction Set)\n",
      "- emb_57 (WARNING: Missing from Prediction Set)\n",
      "- emb_58 (WARNING: Missing from Prediction Set)\n",
      "- emb_59 (WARNING: Missing from Prediction Set)\n",
      "- emb_6 (WARNING: Missing from Prediction Set)\n",
      "- emb_60 (WARNING: Missing from Prediction Set)\n",
      "- emb_61 (WARNING: Missing from Prediction Set)\n",
      "- emb_62 (WARNING: Missing from Prediction Set)\n",
      "- emb_63 (WARNING: Missing from Prediction Set)\n",
      "- emb_64 (WARNING: Missing from Prediction Set)\n",
      "- emb_65 (WARNING: Missing from Prediction Set)\n",
      "- emb_66 (WARNING: Missing from Prediction Set)\n",
      "- emb_67 (WARNING: Missing from Prediction Set)\n",
      "- emb_68 (WARNING: Missing from Prediction Set)\n",
      "- emb_69 (WARNING: Missing from Prediction Set)\n",
      "- emb_7 (WARNING: Missing from Prediction Set)\n",
      "- emb_70 (WARNING: Missing from Prediction Set)\n",
      "- emb_71 (WARNING: Missing from Prediction Set)\n",
      "- emb_72 (WARNING: Missing from Prediction Set)\n",
      "- emb_73 (WARNING: Missing from Prediction Set)\n",
      "- emb_74 (WARNING: Missing from Prediction Set)\n",
      "- emb_75 (WARNING: Missing from Prediction Set)\n",
      "- emb_76 (WARNING: Missing from Prediction Set)\n",
      "- emb_77 (WARNING: Missing from Prediction Set)\n",
      "- emb_78 (WARNING: Missing from Prediction Set)\n",
      "- emb_79 (WARNING: Missing from Prediction Set)\n",
      "- emb_8 (WARNING: Missing from Prediction Set)\n",
      "- emb_80 (WARNING: Missing from Prediction Set)\n",
      "- emb_81 (WARNING: Missing from Prediction Set)\n",
      "- emb_82 (WARNING: Missing from Prediction Set)\n",
      "- emb_83 (WARNING: Missing from Prediction Set)\n",
      "- emb_84 (WARNING: Missing from Prediction Set)\n",
      "- emb_85 (WARNING: Missing from Prediction Set)\n",
      "- emb_86 (WARNING: Missing from Prediction Set)\n",
      "- emb_87 (WARNING: Missing from Prediction Set)\n",
      "- emb_88 (WARNING: Missing from Prediction Set)\n",
      "- emb_89 (WARNING: Missing from Prediction Set)\n",
      "- emb_9 (WARNING: Missing from Prediction Set)\n",
      "- emb_90 (WARNING: Missing from Prediction Set)\n",
      "- emb_91 (WARNING: Missing from Prediction Set)\n",
      "- emb_92 (WARNING: Missing from Prediction Set)\n",
      "- emb_93 (WARNING: Missing from Prediction Set)\n",
      "- emb_94 (WARNING: Missing from Prediction Set)\n",
      "- emb_95 (WARNING: Missing from Prediction Set)\n",
      "- emb_96 (WARNING: Missing from Prediction Set)\n",
      "- emb_97 (WARNING: Missing from Prediction Set)\n",
      "- emb_98 (WARNING: Missing from Prediction Set)\n",
      "- emb_99 (WARNING: Missing from Prediction Set)\n",
      "- min_p_value (EXPECTED: Target/Leakage Feature)\n",
      "- scientific_success (EXPECTED: Target/Leakage Feature)\n",
      "- target (EXPECTED: Target/Leakage Feature)\n",
      "\n",
      "============================== DATA QUALITY & COMPLETENESS ========================\n",
      "| Feature                      | Type    | Nulls (%)   |   Unique | Flag     |\n",
      "|:-----------------------------|:--------|:------------|---------:|:---------|\n",
      "| detailed_description_text    | object  | 43.12%      |     5902 |          |\n",
      "| allocation                   | object  | 31.14%      |        2 |          |\n",
      "| tree_number                  | object  | 21.10%      |      628 |          |\n",
      "| therapeutic_area             | object  | 20.68%      |       23 | CRITICAL |\n",
      "| best_pathology               | object  | 20.68%      |      660 |          |\n",
      "| why_stopped                  | float64 | 100.00%     |        0 |          |\n",
      "| number_of_arms               | float64 | 0.15%       |       24 |          |\n",
      "| brief_summary_text           | object  | 0.00%       |    10364 |          |\n",
      "| sponsor_tier                 | object  | 0.00%       |        2 | CRITICAL |\n",
      "| overall_status               | object  | 0.00%       |        5 | CRITICAL |\n",
      "| phase                        | object  | 0.00%       |        4 |          |\n",
      "| primary_purpose              | object  | 0.00%       |        8 |          |\n",
      "| score_allocation             | int64   | 0.00%       |        2 |          |\n",
      "| score_masking                | int64   | 0.00%       |        4 |          |\n",
      "| score_model                  | int64   | 0.00%       |        2 |          |\n",
      "| sponsor_clean                | object  | 0.00%       |     3415 |          |\n",
      "| start_date                   | object  | 0.00%       |      693 |          |\n",
      "| brief_title                  | object  | 0.00%       |    10415 |          |\n",
      "| start_year                   | float64 | 0.00%       |        2 |          |\n",
      "| study_type                   | object  | 0.00%       |        1 |          |\n",
      "| older_adult                  | int64   | 0.00%       |        2 |          |\n",
      "| therapeutic_subgroup_name    | object  | 0.00%       |      661 |          |\n",
      "| criteria                     | object  | 0.00%       |    10377 |          |\n",
      "| txt_criteria                 | object  | 0.00%       |    10377 |          |\n",
      "| txt_tags                     | object  | 0.00%       |    10394 |          |\n",
      "| adult                        | int64   | 0.00%       |        2 |          |\n",
      "| num_primary_endpoints        | int64   | 0.00%       |       40 | CRITICAL |\n",
      "| agency_class                 | object  | 0.00%       |        8 |          |\n",
      "| nct_id                       | object  | 0.00%       |    10422 |          |\n",
      "| agent_category               | object  | 0.00%       |       19 | CRITICAL |\n",
      "| child                        | int64   | 0.00%       |        2 |          |\n",
      "| competition_broad            | int64   | 0.00%       |       43 | CRITICAL |\n",
      "| competition_niche            | int64   | 0.00%       |      107 |          |\n",
      "| covid_exposure               | int64   | 0.00%       |        1 |          |\n",
      "| criteria_len_log             | float64 | 0.00%       |     5548 | CRITICAL |\n",
      "| design_rigor_score           | int64   | 0.00%       |        6 | CRITICAL |\n",
      "| eligibility_strictness_score | int64   | 0.00%       |        5 |          |\n",
      "| gender                       | object  | 0.00%       |        3 |          |\n",
      "| has_dmc                      | int64   | 0.00%       |        2 |          |\n",
      "| healthy_volunteers           | object  | 0.00%       |        2 |          |\n",
      "| includes_us                  | int64   | 0.00%       |        2 |          |\n",
      "| intervention_model           | object  | 0.00%       |        5 |          |\n",
      "| is_fda_regulated_drug        | int64   | 0.00%       |        2 |          |\n",
      "| is_gender_restricted         | int64   | 0.00%       |        2 |          |\n",
      "| is_sick_only                 | int64   | 0.00%       |        2 |          |\n",
      "| lead_sponsor                 | object  | 0.00%       |     3460 |          |\n",
      "| masking                      | object  | 0.00%       |        5 |          |\n",
      "| name                         | object  | 0.00%       |     7364 |          |\n",
      "| official_title               | object  | 0.00%       |    10376 |          |\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL DATA COMPLETENESS SCORE: 95.17%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================== STATISTICAL DRIFT CHECK (Train vs. Predict) =============\n",
      "| Feature               |   Train Mean |   Predict Mean | Difference (%)   |\n",
      "|:----------------------|-------------:|---------------:|:-----------------|\n",
      "| design_rigor_score    |         1.8  |           1.5  | -16.49%          |\n",
      "| competition_broad     |       840.4  |        1638.98 | 95.02%           |\n",
      "| competition_niche     |       147.96 |         550.27 | 271.89%          |\n",
      "| num_primary_endpoints |         1.74 |           1.83 | 5.10%            |\n",
      "| number_of_arms        |         2.28 |           2.17 | -5.04%           |\n",
      "| criteria_len_log      |         7.38 |           7.78 | 5.51%            |\n",
      "\n",
      "============================== CATEGORICAL DRIFT CHECK (Top 5 Categories) ===========\n",
      "\n",
      "--- Feature: agent_category ---\n",
      "| agent_category            |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:--------------------------|------------:|--------------:|-------------:|\n",
      "| MONOCLONAL_ANTIBODY       |   0.0603802 |     0.146421  |    0.0860409 |\n",
      "| KINASE_INHIBITOR_TYROSINE |   0         |     0.0436576 |    0.0436576 |\n",
      "| PLACEBO_CTRL              |   0.0575227 |     0.0364613 |   -0.0210613 |\n",
      "| BIOLOGIC                  |   0.117323  |     0.0934562 |   -0.0238669 |\n",
      "| CHEMOTHERAPY              |   0.0376651 |     0         |   -0.0376651 |\n",
      "| SMALL_MOLECULE_OTHER      |   0.640473  |     0.588467  |   -0.0520062 |\n",
      "\n",
      "--- Feature: therapeutic_area ---\n",
      "| therapeutic_area              |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:------------------------------|------------:|--------------:|-------------:|\n",
      "| Oncology                      |   0.331796  |     0.46232   |   0.130524   |\n",
      "| Immunology                    |   0         |     0.0524979 |   0.0524979  |\n",
      "| Cardiovascular                |   0.0754379 |     0.0706423 |  -0.00479555 |\n",
      "| Neurology                     |   0.122485  |     0.100641  |  -0.0218435  |\n",
      "| Infections (Bacterial/Fungal) |   0.112377  |     0.0711262 |  -0.0412504  |\n",
      "| Metabolic                     |   0.061254  |     0         |  -0.061254   |\n",
      "\n",
      "--- Feature: phase ---\n",
      "| phase         |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:--------------|------------:|--------------:|-------------:|\n",
      "| PHASE1/PHASE2 |   0.108067  |     0.162349  |   0.0542816  |\n",
      "| PHASE2        |   0.499089  |     0.526482  |   0.0273935  |\n",
      "| PHASE2/PHASE3 |   0.0455543 |     0.0499904 |   0.00443609 |\n",
      "| PHASE3        |   0.34729   |     0.261178  |  -0.0861112  |\n",
      "\n",
      "--- Feature: sponsor_tier ---\n",
      "| sponsor_tier   |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:---------------|------------:|--------------:|-------------:|\n",
      "| TIER_2_OTHER   |    0.742618 |     0.901746  |     0.159128 |\n",
      "| TIER_1_GIANT   |    0.257382 |     0.0982537 |    -0.159128 |\n",
      "\n",
      "[REPORT SAVED] Detailed audit report saved to: /home/delaunan/code/delaunan/clintrialpredict/data/data_audit_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "# This MUST match the path used to generate the files\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "TRAIN_FILE = 'project_data.csv'\n",
    "PREDICT_FILE = 'data_predict.csv'\n",
    "AUDIT_REPORT_FILE = 'data_audit_report.txt'\n",
    "\n",
    "# ====================================================================\n",
    "# 1. LOAD DATASETS\n",
    "# ====================================================================\n",
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILE))\n",
    "    df_predict = pd.read_csv(os.path.join(DATA_PATH, PREDICT_FILE))\n",
    "\n",
    "    # Ensure all columns are treated as strings for initial comparison\n",
    "    train_cols = set(df_train.columns.astype(str))\n",
    "    predict_cols = set(df_predict.columns.astype(str))\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"PREDICTION DATASET AUDIT: {PREDICT_FILE}\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Training Set Size (project_data.csv): {len(df_train)} rows x {len(train_cols)} cols\")\n",
    "    report.append(f\"Prediction Set Size (data_predict.csv): {len(df_predict)} rows x {len(predict_cols)} cols\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    report.append(f\"CRITICAL ERROR: File not found. Check DATA_PATH and filenames. Error: {e}\")\n",
    "    print(\"\\n\".join(report))\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    report.append(f\"CRITICAL ERROR: Failed to load CSVs. Error: {e}\")\n",
    "    print(\"\\n\".join(report))\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 2. COLUMN COMPARISON\n",
    "# ====================================================================\n",
    "\n",
    "# Identify column sets\n",
    "common_cols = sorted(list(train_cols.intersection(predict_cols)))\n",
    "new_cols = sorted(list(predict_cols.difference(train_cols)))\n",
    "missing_cols = sorted(list(train_cols.difference(predict_cols)))\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" COLUMN COMPARISON \" + \"=\" * 30)\n",
    "report.append(f\"Total Common Columns: {len(common_cols)}\")\n",
    "report.append(f\"Total NEW Columns in Prediction Set: {len(new_cols)}\")\n",
    "report.append(f\"Total MISSING Columns from Training Set: {len(missing_cols)}\")\n",
    "report.append(\"-\" * 80)\n",
    "\n",
    "report.append(\"\\n[A] NEW COLUMNS (User-Facing/NLP Fields)\")\n",
    "report.append(\"--------------------------------------------------------------------------------\")\n",
    "if new_cols:\n",
    "    for col in new_cols:\n",
    "        report.append(f\"- {col}\")\n",
    "else:\n",
    "    report.append(\"- NONE (Unexpected - should have UI/NLP fields)\")\n",
    "\n",
    "report.append(\"\\n[B] MISSING COLUMNS (Expected Exclusions)\")\n",
    "report.append(\"--------------------------------------------------------------------------------\")\n",
    "if missing_cols:\n",
    "    for col in missing_cols:\n",
    "        # Flag expected leakage columns\n",
    "        if col in ['target', 'min_p_value', 'scientific_success']:\n",
    "            report.append(f\"- {col} (EXPECTED: Target/Leakage Feature)\")\n",
    "        else:\n",
    "            report.append(f\"- {col} (WARNING: Missing from Prediction Set)\")\n",
    "else:\n",
    "    report.append(\"- NONE (All training features are present or explicitly excluded)\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 3. DATA QUALITY & COMPLETENESS (Prediction Set)\n",
    "# ====================================================================\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" DATA QUALITY & COMPLETENESS \" + \"=\" * 24)\n",
    "\n",
    "# Define critical features for production quality check\n",
    "CRITICAL_FEATURES = [\n",
    "    'design_rigor_score', 'agent_category', 'competition_broad',\n",
    "    'sponsor_tier', 'num_primary_endpoints', 'criteria_len_log',\n",
    "    'therapeutic_area', 'overall_status'\n",
    "]\n",
    "\n",
    "quality_data = []\n",
    "total_rows = len(df_predict)\n",
    "total_missing_cells = 0\n",
    "\n",
    "for col in common_cols + new_cols:\n",
    "    dtype = df_predict[col].dtype\n",
    "    n_null = df_predict[col].isnull().sum()\n",
    "    pct_null = (n_null / total_rows) * 100\n",
    "    n_unique = df_predict[col].nunique()\n",
    "\n",
    "    total_missing_cells += n_null\n",
    "\n",
    "    is_critical = \"CRITICAL\" if col in CRITICAL_FEATURES else \"\"\n",
    "\n",
    "    quality_data.append({\n",
    "        'Feature': col,\n",
    "        'Type': dtype,\n",
    "        'Nulls (%)': f\"{pct_null:.2f}%\",\n",
    "        'Unique': n_unique,\n",
    "        'Flag': is_critical\n",
    "    })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_data).sort_values(by='Nulls (%)', ascending=False)\n",
    "report.append(df_quality.to_markdown(index=False))\n",
    "\n",
    "# Overall Completeness Score\n",
    "total_cells = total_rows * len(df_predict.columns)\n",
    "completeness_score = 100 * (1 - (total_missing_cells / total_cells))\n",
    "report.append(\"-\" * 80)\n",
    "report.append(f\"OVERALL DATA COMPLETENESS SCORE: {completeness_score:.2f}%\")\n",
    "report.append(\"-\" * 80)\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 4. STATISTICAL COMPARISON (DATA DRIFT CHECK)\n",
    "# ====================================================================\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" STATISTICAL DRIFT CHECK (Train vs. Predict) \" + \"=\" * 13)\n",
    "\n",
    "# Select key numerical features for comparison\n",
    "NUMERIC_FEATURES = [\n",
    "    'design_rigor_score', 'competition_broad', 'competition_niche',\n",
    "    'num_primary_endpoints', 'number_of_arms', 'criteria_len_log'\n",
    "]\n",
    "\n",
    "drift_data = []\n",
    "for col in NUMERIC_FEATURES:\n",
    "    if col in common_cols:\n",
    "        train_mean = df_train[col].mean()\n",
    "        predict_mean = df_predict[col].mean()\n",
    "        diff_pct = 100 * (predict_mean - train_mean) / train_mean\n",
    "\n",
    "        drift_data.append({\n",
    "            'Feature': col,\n",
    "            'Train Mean': f\"{train_mean:.2f}\",\n",
    "            'Predict Mean': f\"{predict_mean:.2f}\",\n",
    "            'Difference (%)': f\"{diff_pct:.2f}%\"\n",
    "        })\n",
    "\n",
    "df_drift = pd.DataFrame(drift_data)\n",
    "report.append(df_drift.to_markdown(index=False))\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" CATEGORICAL DRIFT CHECK (Top 5 Categories) \" + \"=\" * 11)\n",
    "\n",
    "# Categorical comparison for the strongest features\n",
    "CATEGORICAL_FEATURES = ['agent_category', 'therapeutic_area', 'phase', 'sponsor_tier']\n",
    "\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in common_cols:\n",
    "        train_counts = df_train[col].value_counts(normalize=True).head(5)\n",
    "        predict_counts = df_predict[col].value_counts(normalize=True).head(5)\n",
    "\n",
    "        combined = pd.concat([train_counts, predict_counts], axis=1, keys=['Train_Pct', 'Predict_Pct']).fillna(0)\n",
    "        combined['Difference'] = combined['Predict_Pct'] - combined['Train_Pct']\n",
    "\n",
    "        report.append(f\"\\n--- Feature: {col} ---\")\n",
    "        report.append(combined.sort_values(by='Difference', ascending=False).to_markdown())\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 5. FINAL OUTPUT\n",
    "# ====================================================================\n",
    "\n",
    "final_report = \"\\n\".join(report)\n",
    "print(final_report)\n",
    "\n",
    "# Save the report to a file\n",
    "with open(os.path.join(DATA_PATH, AUDIT_REPORT_FILE), 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(f\"\\n[REPORT SAVED] Detailed audit report saved to: {os.path.join(DATA_PATH, AUDIT_REPORT_FILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9e9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING FULL HISTORY NLP CRITERIA GENERATION ---\n",
      ">>> 1. Loading FULL HISTORY NLP Cohort (2005-2025)...\n",
      "    Final NLP Cohort Size: 137724 trials (Full History, Phase 1/2/3)\n",
      ">>> Saved 137724 rows to /home/delaunan/code/delaunan/clintrialpredict/data/nlp_criteria_full_history_2005_2025.csv\n",
      "\n",
      "[SUCCESS] Full History NLP criteria set ready for colleague at: /home/delaunan/code/delaunan/clintrialpredict/data/nlp_criteria_full_history_2005_2025.csv\n",
      "WARNING: This file is large and contains the entire 2005-2025 Phase 1/2/3 cohort. BioBERT processing will take significant time/resources.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION (THE ONLY LINE YOU MUST VERIFY)\n",
    "# ====================================================================\n",
    "# This MUST be the absolute path to the FOLDER containing your AACT .txt files.\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "NLP_OUTPUT_FILE = 'nlp_criteria_full_history_2005_2025.csv'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING FULL HISTORY NLP CRITERIA GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS (Full Definition)\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    \"\"\"\n",
    "    A specialized loader for generating the full historical NLP criteria dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "\n",
    "        # --- STRATEGY A: PERFECT ---\n",
    "        self.params_perfect = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "        # --- STRATEGY B: ROBUST ---\n",
    "        self.params_robust = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"   [!] Warning: File not found {filename}. Features will be empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except Exception as e:\n",
    "            print(f\"   [!] Formatting error in {filename}. Switching to Robust Mode...\")\n",
    "            try:\n",
    "                return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except Exception as e2:\n",
    "                print(f\"   [x] CRITICAL: Could not load {filename}. Error: {e2}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "    # --- MAIN FUNCTION: LOAD FULL HISTORY NLP CRITERIA ---\n",
    "\n",
    "    def load_full_history_nlp_set(self, min_year: int = 2005, max_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the full historical dataset (2005-2025) for NLP embedding generation,\n",
    "        including Phase 1 trials to maximize data for BioBERT training.\n",
    "        \"\"\"\n",
    "        print(f\">>> 1. Loading FULL HISTORY NLP Cohort ({min_year}-{max_year})...\")\n",
    "\n",
    "        # 1. Load Core Columns (Minimal set for filtering)\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date']\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Critical Error: 'studies.txt' failed to load.\")\n",
    "\n",
    "        # 2. Apply Core Filters (Interventional, Drug)\n",
    "        df = df[df['study_type'].str.upper() == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "        cols_int = ['nct_id', 'intervention_type']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            drug_ids = df_int[df_int['intervention_type'].str.upper().isin(target_types)]['nct_id'].unique()\n",
    "            df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "        # 3. CRITICAL CHANGE: Phase Filter (Keep Phase 1, Phase 2, Phase 3)\n",
    "        # We only exclude EARLY_PHASE1, PHASE4, and NA\n",
    "        excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA']\n",
    "        df = df[~df['phase'].astype(str).str.upper().isin(excluded_phases)]\n",
    "        df = df.dropna(subset=['phase'])\n",
    "\n",
    "        # 4. Apply Date Filter (2005-2025)\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "        df = df[df['start_year'].between(min_year, max_year)].copy()\n",
    "\n",
    "        # 5. CRITICAL: Load Eligibility Criteria Text\n",
    "        cols_elig = ['nct_id', 'criteria']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_elig)\n",
    "\n",
    "        if df_elig.empty:\n",
    "            print(\"    [!] Warning: 'eligibilities.txt' failed to load. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame(columns=['nct_id', 'criteria'])\n",
    "\n",
    "        # 6. Final Merge and Cleanup\n",
    "        # Merge on nct_id to get the criteria for the filtered studies\n",
    "        df_final = df[['nct_id']].merge(df_elig.drop_duplicates('nct_id'), on='nct_id', how='inner')\n",
    "\n",
    "\n",
    "        # Rename column for clarity in the final output file\n",
    "        df_final.rename(columns={'criteria': 'txt_criteria'}, inplace=True)\n",
    "\n",
    "        print(f\"    Final NLP Cohort Size: {len(df_final)} trials (Full History, Phase 1/2/3)\")\n",
    "        return df_final[['nct_id', 'txt_criteria']].copy()\n",
    "\n",
    "    def save(self, df: pd.DataFrame, filename: str = 'project_data.csv'):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION CODE\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # 1. Instantiate the loader\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # 2. Generate the NLP-focused dataset (2005-2025, including Phase 1)\n",
    "    df_nlp_full = loader.load_full_history_nlp_set(min_year=2005, max_year=2025)\n",
    "\n",
    "    # 3. Save the final dataset\n",
    "    loader.save(df_nlp_full, filename=NLP_OUTPUT_FILE)\n",
    "\n",
    "    print(f\"\\n[SUCCESS] Full History NLP criteria set ready for colleague at: {os.path.join(DATA_PATH, NLP_OUTPUT_FILE)}\")\n",
    "    print(\"WARNING: This file is large and contains the entire 2005-2025 Phase 1/2/3 cohort. BioBERT processing will take significant time/resources.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[CRITICAL FAILURE] Data loading stopped: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check if 'studies.txt' is present in the DATA_PATH folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[UNEXPECTED FAILURE] An unexpected error occurred: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check file paths, file names, and data integrity.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
