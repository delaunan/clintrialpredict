{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44cbd06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679236db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b92c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING PREDICTION COHORT GENERATION ---\n",
      ">>> 1. Loading Full Cohort (2005-2025) for Competition Calculation...\n",
      "    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\n",
      "    -> Calculating Competition...\n",
      "    Full Cohort Size: 137724 trials.\n",
      ">>> 2. Filtering for Prediction Set (2024-2025, Active/Pending Statuses)...\n",
      "    Prediction Cohort: 13747 trials (Active/Pending Phase 2/3, 2024-2025)\n",
      ">>> 3. Engineering Remaining Features for Prediction...\n",
      "    -> Engineering Sponsor Tiers...\n",
      "    -> Engineering Protocol Complexity (Calculating Age Flags)...\n",
      "    -> Engineering Agent Type (Bulletproof Classifier)...\n",
      "    -> Engineering Smart Patterns (Rigor & Strictness)...\n",
      "    -> Engineering Safe Protocol Features...\n",
      "    -> Preparing Text Features...\n",
      "    -> Attaching Vectorized Text Embeddings...\n",
      "       -> Successfully merged embeddings (Columns added: 20)\n",
      "    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\n",
      ">>> Saved 13747 rows to /home/delaunan/code/delaunan/clintrialpredict/data/data_predict.csv\n",
      "\n",
      "[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION (THE ONLY LINE YOU MUST VERIFY)\n",
    "# ====================================================================\n",
    "# This MUST be the absolute path to the FOLDER containing your AACT .txt files.\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING PREDICTION COHORT GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS (Full Definition)\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    \"\"\"\n",
    "    A specialized loader for generating the PROSPECTIVE prediction dataset (2024-2025).\n",
    "    It ensures feature consistency and excludes all post-hoc/leakage features.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "\n",
    "        # --- STRATEGY A: PERFECT ---\n",
    "        self.params_perfect = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "        # --- STRATEGY B: ROBUST ---\n",
    "        self.params_robust = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"   [!] Warning: File not found {filename}. Features will be empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except Exception as e:\n",
    "            print(f\"   [!] Formatting error in {filename}. Switching to Robust Mode...\")\n",
    "            try:\n",
    "                return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except Exception as e2:\n",
    "                print(f\"   [x] CRITICAL: Could not load {filename}. Error: {e2}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "    # --- STEP 1: LOAD FULL COHORT FOR TIME-SENSITIVE FEATURES ---\n",
    "\n",
    "    def load_full_cohort_for_competition(self, min_year: int = 2005, max_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the entire relevant dataset (2005-2025) to calculate time-sensitive\n",
    "        features (Competition) on the complete historical context before splitting.\n",
    "        \"\"\"\n",
    "        print(f\">>> 1. Loading Full Cohort ({min_year}-{max_year}) for Competition Calculation...\")\n",
    "\n",
    "        # 1. Load Core Columns\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase',\n",
    "                        'start_date', 'number_of_arms',\n",
    "                        'official_title', 'why_stopped',\n",
    "                        'has_dmc', 'is_fda_regulated_drug', 'brief_title']\n",
    "\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Critical Error: 'studies.txt' failed to load.\")\n",
    "\n",
    "        # 2. Apply Core Filters (Interventional, Drug, Phase 2/3)\n",
    "        df = df[df['study_type'].str.upper() == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "        cols_int = ['nct_id', 'intervention_type', 'name']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            drug_ids = df_int[df_int['intervention_type'].str.upper().isin(target_types)]['nct_id'].unique()\n",
    "            df = df[df['nct_id'].isin(drug_ids)]\n",
    "            self.df_drugs = df_int[df_int['intervention_type'].str.upper().isin(target_types + ['DIETARY SUPPLEMENT', 'OTHER'])].copy()\n",
    "\n",
    "        excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA'] #'PHASE1'\n",
    "        df = df[~df['phase'].astype(str).str.upper().isin(excluded_phases)]\n",
    "        df = df.dropna(subset=['phase'])\n",
    "\n",
    "        # 3. Apply Full Date Range Filter (2005-2025)\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "        df = df[df['start_year'].between(min_year, max_year)].copy()\n",
    "\n",
    "        # 4. Calculate Time-Sensitive Features (Hierarchy and Competition)\n",
    "        df = self._attach_medical_hierarchy(df)\n",
    "        df = self._calculate_competition(df)\n",
    "\n",
    "        print(f\"    Full Cohort Size: {len(df)} trials.\")\n",
    "        return df.copy()\n",
    "\n",
    "\n",
    "    # --- STEP 2: FILTER AND FINALIZE PREDICTION SET ---\n",
    "\n",
    "    def load_and_clean_prediction_set(self, df_full_cohort: pd.DataFrame, start_year_predict: int = 2024) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters the full cohort for the Prediction Set: Active/Pending Statuses, Year >= 2024.\n",
    "        \"\"\"\n",
    "        df = df_full_cohort.copy()\n",
    "        print(f\">>> 2. Filtering for Prediction Set ({start_year_predict}-2025, Active/Pending Statuses)...\")\n",
    "\n",
    "        # 1. Filter: Date Range (2024-2025)\n",
    "        df = df[df['start_year'] >= start_year_predict].copy()\n",
    "\n",
    "        # 2. Filter: Status (Active/Pending Only)\n",
    "        ACTIVE_STATUSES = [\n",
    "            'RECRUITING', 'NOT_YET_RECRUITING', 'ACTIVE_NOT_RECRUITING',\n",
    "            'ENROLLING_BY_INVITATION', 'WITHHELD', 'UNKNOWN'\n",
    "        ]\n",
    "\n",
    "        df_upper = df['overall_status'].astype(str).str.upper()\n",
    "        df = df[df_upper.isin(ACTIVE_STATUSES)].copy()\n",
    "\n",
    "        # 3. Filter: COVID Sanitizer (Kept for consistency)\n",
    "        if 'why_stopped' in df.columns:\n",
    "            covid_keywords = ['covid', 'pandemic', 'coronavirus', 'sars-cov-2', 'logistical reasons']\n",
    "            mask_covid = df['why_stopped'].fillna('').astype(str).str.lower().apply(\n",
    "                lambda x: any(k in x for k in covid_keywords)\n",
    "            )\n",
    "            if mask_covid.sum() > 0:\n",
    "                print(f\"    [Sanitizer] Dropping {mask_covid.sum()} trials terminated due to COVID/Logistics.\")\n",
    "                df = df[~mask_covid]\n",
    "\n",
    "        # 4. Target: The prediction set has NO target.\n",
    "        if 'target' in df.columns:\n",
    "             df.drop(columns=['target'], inplace=True)\n",
    "\n",
    "        print(f\"    Prediction Cohort: {len(df)} trials (Active/Pending Phase 2/3, {start_year_predict}-2025)\")\n",
    "\n",
    "        # 5. Add remaining features and user-facing text\n",
    "        df = self.add_features_for_prediction(df)\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    # --- NEW PREDICTION FEATURE FUNCTION (STEP 3) ---\n",
    "\n",
    "    def add_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds remaining features (non-time-sensitive) and user-facing fields.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        print(\">>> 3. Engineering Remaining Features for Prediction...\")\n",
    "\n",
    "        # 1. Operational Flags\n",
    "        df['covid_exposure'] = df['start_year'].between(2019, 2022).astype(int)\n",
    "\n",
    "        # 2. Geography (SAFE: Only is_us)\n",
    "        df_countries = self._safe_load('countries.txt', cols=['nct_id', 'name'])\n",
    "        if not df_countries.empty:\n",
    "            us_trials = df_countries[df_countries['name'] == 'United States']['nct_id'].unique()\n",
    "            df['includes_us'] = df['nct_id'].isin(us_trials).astype(int)\n",
    "        else:\n",
    "            df['includes_us'] = 0\n",
    "\n",
    "        # 3. Merge Standard Metadata\n",
    "        df = self._merge_file(df, 'designs.txt', ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose'])\n",
    "        # NOTE: This line adds 'number_of_primary_outcomes_to_measure'\n",
    "        df = self._merge_file(df, 'calculated_values.txt', ['nct_id', 'number_of_primary_outcomes_to_measure'])\n",
    "\n",
    "        # 4. Sponsor Engineering\n",
    "        df = self._engineer_sponsor_features(df)\n",
    "\n",
    "        # 5. Complexity Engineering (Loads eligibility.txt and 'criteria' column)\n",
    "        df = self._engineer_complexity(df)\n",
    "\n",
    "        # 6. Agent Type\n",
    "        df = self._engineer_agent_type(df)\n",
    "\n",
    "        # 7. Smart Patterns (Rigor & Strictness)\n",
    "        df = self._engineer_smart_patterns(df)\n",
    "\n",
    "        # 8. Safe Features (DMC)\n",
    "        df = self._engineer_safe_features(df)\n",
    "\n",
    "        # 9. Text Features (Creates txt_tags, txt_criteria)\n",
    "        df = self._prepare_text(df)\n",
    "\n",
    "        # 10. ATTACH EMBEDDINGS (TEMPORARILY SKIPPED FOR COLLEAGUE'S WORKFLOW)\n",
    "        df = self._attach_embeddings(df)\n",
    "\n",
    "        # 11. Attach User-Facing Text Fields\n",
    "        df = self._load_user_facing_text(df)\n",
    "\n",
    "        # --- CLEANUP (FIXED LOGIC) ---\n",
    "        old_col = 'number_of_primary_outcomes_to_measure'\n",
    "        new_col = 'num_primary_endpoints'\n",
    "\n",
    "        if old_col in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            # Convert to numeric and fill NaNs in the new column\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(1)\n",
    "        else:\n",
    "            # If the column was never loaded, create it with the default value 1\n",
    "            df[new_col] = 1\n",
    "        # -----------------------------\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- NEW HELPER FUNCTION FOR UI/USER-FACING DATA ---\n",
    "\n",
    "    def _load_user_facing_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads text fields that are useful for the UI but not necessarily for the model.\n",
    "        \"\"\"\n",
    "        print(\"    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\")\n",
    "\n",
    "        # 1. Brief Summary\n",
    "        cols_brief = ['nct_id', 'description']\n",
    "        df_brief = self._safe_load('brief_summaries.txt', cols=cols_brief)\n",
    "        if not df_brief.empty:\n",
    "            df_brief = df_brief.rename(columns={'description': 'brief_summary_text'})\n",
    "            df = df.merge(df_brief.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['brief_summary_text'] = \"\"\n",
    "\n",
    "        # 2. Detailed Description\n",
    "        cols_detailed = ['nct_id', 'description']\n",
    "        df_detailed = self._safe_load('detailed_descriptions.txt', cols=cols_detailed)\n",
    "        if not df_detailed.empty:\n",
    "            df_detailed = df_detailed.rename(columns={'description': 'detailed_description_text'})\n",
    "            df = df.merge(df_detailed.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['detailed_description_text'] = \"\"\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- ALL OTHER NECESSARY HELPER METHODS (KEPT FOR FUNCTIONALITY) ---\n",
    "\n",
    "    def _engineer_smart_patterns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Smart Patterns (Rigor & Strictness)...\")\n",
    "        def get_masking_score(val):\n",
    "            val = str(val).lower()\n",
    "            if 'quadruple' in val: return 3\n",
    "            if 'double' in val: return 2\n",
    "            if 'single' in val: return 1\n",
    "            return 0\n",
    "        def get_allocation_score(val):\n",
    "            return 1 if 'randomized' in str(val).lower() else 0\n",
    "        def get_model_score(val):\n",
    "            val = str(val).lower()\n",
    "            return 1 if 'crossover' in val or 'factorial' in val else 0\n",
    "        df['score_masking'] = df['masking'].apply(get_masking_score)\n",
    "        df['score_allocation'] = df['allocation'].apply(get_allocation_score)\n",
    "        df['score_model'] = df['intervention_model'].apply(get_model_score)\n",
    "        df['design_rigor_score'] = df['score_masking'] + df['score_allocation'] + df['score_model']\n",
    "        df['is_gender_restricted'] = df['gender'].apply(lambda x: 0 if str(x).lower() == 'all' else 1)\n",
    "        df['is_sick_only'] = df['healthy_volunteers'].apply(lambda x: 1 if str(x).lower() == 'no' else 0)\n",
    "        for col in ['child', 'adult', 'older_adult']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(lambda x: 1 if x.lower() in ['true', '1', 'yes'] else 0)\n",
    "            else:\n",
    "                df[col] = 1\n",
    "        df['eligibility_strictness_score'] = (\n",
    "            df['is_gender_restricted'] +\n",
    "            df['is_sick_only'] +\n",
    "            (1 - df['child']) +\n",
    "            (1 - df['older_adult'])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _engineer_agent_type(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Agent Type (Bulletproof Classifier)...\")\n",
    "        if self.df_drugs.empty:\n",
    "            df['agent_category'] = 'UNKNOWN'\n",
    "            return df\n",
    "        type_priority = {'GENETIC': 1,'BIOLOGICAL': 2,'DRUG': 3,'DIETARY SUPPLEMENT': 4,'OTHER': 5}\n",
    "        df_int = self.df_drugs.copy()\n",
    "        df_int['type_upper'] = df_int['intervention_type'].str.upper()\n",
    "        df_int['priority'] = df_int['type_upper'].map(lambda x: type_priority.get(x, 5))\n",
    "        df_int = df_int.sort_values('priority')\n",
    "        best_types = df_int.drop_duplicates('nct_id')[['nct_id', 'type_upper', 'name']]\n",
    "        df = df.merge(best_types, on='nct_id', how='left')\n",
    "        def classify_molecule(row):\n",
    "            itype = str(row['type_upper'])\n",
    "            name = str(row['name']).lower()\n",
    "            if 'placebo' in name: return 'PLACEBO_CTRL'\n",
    "            if itype == 'GENETIC': return 'GENE_THERAPY'\n",
    "            if any(x in name for x in ['car-t', 'chimeric antigen', 'autologous', 'allogeneic', 't-cell', 'nk cell']):\n",
    "                return 'CELL_THERAPY'\n",
    "            if name.endswith('cel'): return 'CELL_THERAPY'\n",
    "            if any(x in name for x in ['crispr', 'cas9', 'mrna', 'sirna', 'antisense', 'oligonucleotide', 'plasmid', 'vector', 'aav']):\n",
    "                return 'RNA_GENE_THERAPY'\n",
    "            if itype == 'BIOLOGICAL': return 'BIOLOGIC'\n",
    "            if 'mab' in name:\n",
    "                if 'adc' in name or 'conjugate' in name: return 'ANTIBODY_DRUG_CONJUGATE'\n",
    "                return 'MONOCLONAL_ANTIBODY'\n",
    "            if name.endswith('cept'): return 'BIOLOGIC_FUSION'\n",
    "            if 'vaccine' in name: return 'VACCINE'\n",
    "            if any(x in name for x in ['interferon', 'interleukin', 'cytokine']): return 'IMMUNOTHERAPY'\n",
    "            if name.endswith('ib') or 'ib ' in name:\n",
    "                if 'tinib' in name: return 'KINASE_INHIBITOR_TYROSINE'\n",
    "                if 'parib' in name: return 'PARP_INHIBITOR'\n",
    "                if 'lisib' in name: return 'PI3K_INHIBITOR'\n",
    "                return 'TARGETED_KINASE_INHIBITOR'\n",
    "            if 'vastatin' in name: return 'STATIN_CHOLESTEROL'\n",
    "            if name.endswith('stat') or 'stat ' in name: return 'ENZYME_INHIBITOR'\n",
    "            if name.endswith('degib'): return 'HEDGEHOG_INHIBITOR'\n",
    "            if name.endswith('clax'): return 'BCL2_INHIBITOR'\n",
    "            chemo_stems = ['platin', 'taxel', 'rubicin', 'fluorouracil', 'gemcitabine',\n",
    "                           'cyclophosphamide', 'methotrexate', 'etoposide', 'vincristine', 'vinblastine']\n",
    "            if any(x in name for x in chemo_stems):\n",
    "                return 'CHEMOTHERAPY'\n",
    "            return 'SMALL_MOLECULE_OTHER'\n",
    "        df['agent_category'] = df.apply(classify_molecule, axis=1)\n",
    "        df.drop(columns=['type_upper', 'priority', 'name_y'], inplace=True, errors='ignore')\n",
    "        if 'name_x' in df.columns: df.rename(columns={'name_x': 'official_title'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _engineer_safe_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Safe Protocol Features...\")\n",
    "        for col in ['has_dmc', 'is_fda_regulated_drug']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(\n",
    "                    lambda x: 1 if x.lower() in ['true', 't', '1', 'yes'] else 0\n",
    "                )\n",
    "            else:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def _attach_medical_hierarchy(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\")\n",
    "        cols_bridge = ['nct_id', 'mesh_term']\n",
    "        df_bridge = self._safe_load('browse_conditions.txt', cols=cols_bridge)\n",
    "        if df_bridge.empty:\n",
    "            cols_cond = ['nct_id', 'name']\n",
    "            df_bridge = self._safe_load('conditions.txt', cols=cols_cond)\n",
    "            if not df_bridge.empty:\n",
    "                df_bridge.rename(columns={'name': 'mesh_term'}, inplace=True)\n",
    "        mesh_path = os.path.join(self.data_path, 'mesh_lookup.csv')\n",
    "        df_dictionary = pd.DataFrame()\n",
    "        if os.path.exists(mesh_path):\n",
    "            try:\n",
    "                df_dictionary = pd.read_csv(mesh_path, sep='|', on_bad_lines='skip')\n",
    "                if 'mesh_term' in df_dictionary.columns and 'therapeutic_area' in df_dictionary.columns:\n",
    "                    df_dictionary = df_dictionary[['mesh_term', 'therapeutic_area']].drop_duplicates()\n",
    "                    df_dictionary.rename(columns={'therapeutic_area': 'lookup_area'}, inplace=True)\n",
    "                else:\n",
    "                    df_dictionary = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"       [!] Error reading mesh_lookup.csv: {e}\")\n",
    "        if not df_bridge.empty:\n",
    "            if not df_dictionary.empty:\n",
    "                df_full_mesh = df_bridge.merge(df_dictionary, on='mesh_term', how='left')\n",
    "            else:\n",
    "                df_full_mesh = df_bridge\n",
    "                df_full_mesh['lookup_area'] = np.nan\n",
    "            df_grouped = df_full_mesh.groupby('nct_id').agg({\n",
    "                'mesh_term': 'first',\n",
    "                'lookup_area': 'first'\n",
    "            }).reset_index()\n",
    "            df = df.merge(df_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['mesh_term'] = np.nan\n",
    "            df['lookup_area'] = np.nan\n",
    "        smart_path = os.path.join(self.data_path, 'smart_pathology_lookup.csv')\n",
    "        if os.path.exists(smart_path):\n",
    "            try:\n",
    "                df_smart = pd.read_csv(smart_path)\n",
    "                if 'nct_id' in df_smart.columns:\n",
    "                    df = df.merge(df_smart, on='nct_id', how='left')\n",
    "            except:\n",
    "                pass\n",
    "        if 'best_pathology' not in df.columns: df['best_pathology'] = np.nan\n",
    "        if 'therapeutic_area' not in df.columns: df['therapeutic_area'] = np.nan\n",
    "        def get_final_category(row):\n",
    "            if pd.notna(row.get('best_pathology')) and str(row.get('best_pathology')) != 'Unknown':\n",
    "                return row['best_pathology']\n",
    "            if pd.notna(row.get('lookup_area')) and str(row.get('lookup_area')) != 'Other/Unclassified':\n",
    "                return row['lookup_area']\n",
    "            if pd.notna(row.get('mesh_term')):\n",
    "                return row['mesh_term']\n",
    "            if pd.notna(row.get('therapeutic_area')) and str(row.get('therapeutic_area')) != 'Other':\n",
    "                return row['therapeutic_area']\n",
    "            return 'Unclassified Condition'\n",
    "        df['therapeutic_subgroup_name'] = df.apply(get_final_category, axis=1)\n",
    "        df.drop(columns=['mesh_term', 'lookup_area'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _engineer_sponsor_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Sponsor Tiers...\")\n",
    "        cols_needed = ['nct_id', 'lead_or_collaborator', 'name', 'agency_class']\n",
    "        df_sponsors = self._safe_load('sponsors.txt', cols=cols_needed)\n",
    "        if df_sponsors.empty:\n",
    "            df['sponsor_tier'] = 'TIER_2_OTHER'\n",
    "            df['sponsor_clean'] = 'UNKNOWN'\n",
    "            df['agency_class'] = 'UNKNOWN'\n",
    "            return df\n",
    "        leads = df_sponsors[df_sponsors['lead_or_collaborator'].str.lower() == 'lead'][['nct_id', 'name', 'agency_class']]\n",
    "        leads = leads.rename(columns={'name': 'lead_sponsor'})\n",
    "        leads = leads.drop_duplicates('nct_id')\n",
    "        df = df.merge(leads, on='nct_id', how='left')\n",
    "        df['lead_sponsor'] = df['lead_sponsor'].fillna('UNKNOWN')\n",
    "        df['agency_class'] = df['agency_class'].fillna('UNKNOWN')\n",
    "        clean_col = df['lead_sponsor'].astype(str).str.lower().str.strip()\n",
    "        legal_pattern = r'[.,]|\\binc\\b|\\bltd\\b|\\bllc\\b|\\bcorp\\b|\\bgmbh\\b|\\bsa\\b|\\bplc\\b'\n",
    "        clean_col = clean_col.str.replace(legal_pattern, '', regex=True).str.strip()\n",
    "        mappings = {\n",
    "            'Pfizer': ['pfizer', 'wyeth', 'hospira'], 'GSK': ['glaxo', 'gsk', 'smithkline'],\n",
    "            'Novartis': ['novartis', 'sandoz'], 'AstraZeneca': ['astrazeneca', 'medimmune'],\n",
    "            'Merck': ['merck', 'msd'], 'Roche': ['roche', 'genentech', 'hoffmann'],\n",
    "            'Sanofi': ['sanofi', 'aventis', 'genzyme'], 'J&J': ['johnson & johnson', 'janssen'],\n",
    "            'Bayer': ['bayer', 'monsanto'], 'Boehringer': ['boehringer'],\n",
    "            'BMS': ['bristol-myers', 'squibb', 'celgene'], 'Lilly': ['lilly'],\n",
    "            'Abbott': ['abbott', 'abbvie'], 'Amgen': ['amgen'],\n",
    "            'Takeda': ['takeda', 'shire'], 'Gilead': ['gilead'],\n",
    "            'Novo Nordisk': ['novo nordisk'], 'NIH': ['national cancer institute', 'nci', 'national institutes of health', 'nih']\n",
    "        }\n",
    "        final_names = clean_col.copy()\n",
    "        for std, keys in mappings.items():\n",
    "            pattern = '|'.join(keys)\n",
    "            mask = clean_col.str.contains(pattern, case=False, regex=True)\n",
    "            final_names.loc[mask] = std\n",
    "        df['sponsor_clean'] = final_names\n",
    "        def get_tier(name):\n",
    "            if name in mappings.keys(): return 'TIER_1_GIANT'\n",
    "            return 'TIER_2_OTHER'\n",
    "        df['sponsor_tier'] = df['sponsor_clean'].apply(get_tier)\n",
    "        return df\n",
    "\n",
    "    def _engineer_complexity(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Protocol Complexity (Calculating Age Flags)...\")\n",
    "        cols_needed = ['nct_id', 'criteria', 'gender', 'healthy_volunteers', 'minimum_age', 'maximum_age']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_needed)\n",
    "        if df_elig.empty:\n",
    "            df['criteria_len_log'] = 0\n",
    "            for c in ['gender', 'healthy_volunteers', 'adult', 'child', 'older_adult']: df[c] = 0\n",
    "            return df\n",
    "        df_elig = df_elig.drop_duplicates('nct_id')\n",
    "        df = df.merge(df_elig, on='nct_id', how='left')\n",
    "        df['criteria_len_log'] = np.log1p(df['criteria'].astype(str).str.len().fillna(0))\n",
    "        df['healthy_volunteers'] = df['healthy_volunteers'].astype(str).str.lower().apply(\n",
    "            lambda x: 'no' if x in ['f', 'false', '0', 'no', 'nan'] else 'yes'\n",
    "        )\n",
    "        df['gender'] = df['gender'].fillna('UNKNOWN')\n",
    "        def parse_age_to_years(val, default_val):\n",
    "            if pd.isna(val) or str(val).lower() in ['n/a', 'nan', '', 'none']: return default_val\n",
    "            try:\n",
    "                match = re.search(r'(\\d+(\\.\\d+)?)', str(val))\n",
    "                if not match: return default_val\n",
    "                num = float(match.group(1))\n",
    "                text = str(val).lower()\n",
    "                if 'month' in text: num /= 12.0\n",
    "                elif 'week' in text: num /= 52.0\n",
    "                elif 'day' in text: num /= 365.0\n",
    "                elif 'hour' in text: num /= 8760.0\n",
    "                return num\n",
    "            except:\n",
    "                return default_val\n",
    "        df['min_age_years'] = df['minimum_age'].apply(lambda x: parse_age_to_years(x, 0.0))\n",
    "        df['max_age_years'] = df['maximum_age'].apply(lambda x: parse_age_to_years(x, 100.0))\n",
    "        df['child'] = (df['min_age_years'] < 18).astype(int)\n",
    "        df['adult'] = ((df['max_age_years'] >= 18) & (df['min_age_years'] < 65)).astype(int)\n",
    "        df['older_adult'] = (df['max_age_years'] > 65).astype(int)\n",
    "        df.drop(columns=['minimum_age', 'maximum_age', 'min_age_years', 'max_age_years'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _calculate_competition(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Calculating Competition...\")\n",
    "        try:\n",
    "            req_cols = ['start_year', 'therapeutic_area', 'phase']\n",
    "            if not all(col in df.columns for col in req_cols):\n",
    "                df['competition_broad'] = 0\n",
    "                df['competition_niche'] = 0\n",
    "                return df\n",
    "            grid = df.groupby(['start_year', 'therapeutic_area']).size().reset_index(name='count')\n",
    "            lookup = dict(zip(zip(grid['start_year'], grid['therapeutic_area']), grid['count']))\n",
    "            def get_comp(row):\n",
    "                y, area = row['start_year'], row['therapeutic_area']\n",
    "                return lookup.get((y, area), 0) + lookup.get((y-1, area), 0)\n",
    "            df['competition_broad'] = df.apply(get_comp, axis=1)\n",
    "            if 'therapeutic_subgroup_name' in df.columns:\n",
    "                grid_niche = df.groupby(['start_year', 'therapeutic_subgroup_name']).size().reset_index(name='count')\n",
    "                lookup_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup_name']), grid_niche['count']))\n",
    "                def get_niche(row):\n",
    "                    y, sub = row['start_year'], row['therapeutic_subgroup_name']\n",
    "                    return lookup_niche.get((y, sub), 0) + lookup_niche.get((y-1, sub), 0)\n",
    "                df['competition_niche'] = df.apply(get_niche, axis=1)\n",
    "            else:\n",
    "                df['competition_niche'] = 0\n",
    "        except:\n",
    "            df['competition_broad'] = 0\n",
    "            df['competition_niche'] = 0\n",
    "        return df\n",
    "\n",
    "    def _prepare_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Preparing Text Features...\")\n",
    "        df_keys = self._safe_load('keywords.txt', cols=['nct_id', 'name'])\n",
    "        if not df_keys.empty:\n",
    "            keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "            df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_keywords'] = \"\"\n",
    "        if not self.df_drugs.empty:\n",
    "            int_names = self.df_drugs.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_int_names')\n",
    "            df = df.merge(int_names, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_int_names'] = \"\"\n",
    "        text_cols = ['official_title', 'txt_keywords', 'txt_int_names', 'criteria']\n",
    "        for c in text_cols:\n",
    "            if c in df.columns: df[c] = df[c].fillna(\"\")\n",
    "        df['txt_tags'] = (df['official_title'] + \" \" + df['txt_keywords'] + \" \" + df['txt_int_names'])\n",
    "        if 'criteria' in df.columns:\n",
    "            df['txt_criteria'] = df['criteria']\n",
    "            df.drop(columns=['txt_keywords', 'txt_int_names'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    # NOTE: _attach_embeddings is now a placeholder function\n",
    "\n",
    "    def _attach_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Attaching Vectorized Text Embeddings...\")\n",
    "        emb_path = os.path.join(self.data_path, 'embeddings_with_nctid.csv')\n",
    "\n",
    "        # 1. Check if file exists\n",
    "        if not os.path.exists(emb_path):\n",
    "            print(\"    [!] Warning: embeddings_with_nctid.csv not found.\")\n",
    "            return df\n",
    "\n",
    "        # 2. LOAD AND MERGE\n",
    "        try:\n",
    "            # Load the embeddings CSV\n",
    "            df_emb = pd.read_csv(emb_path)\n",
    "\n",
    "            # Check if 'nct_id' is present to perform the merge\n",
    "            if 'nct_id' in df_emb.columns:\n",
    "                # Merge logic: Left join on nct_id\n",
    "                df = df.merge(df_emb, on='nct_id', how='left')\n",
    "                print(f\"       -> Successfully merged embeddings (Columns added: {df_emb.shape[1] - 1})\")\n",
    "            else:\n",
    "                print(\"       [!] Error: 'nct_id' column missing in embeddings file. Cannot merge.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"       [!] Error reading/merging embeddings: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # NOTE: _attach_p_values is ONLY called in the original training pipeline (not in prediction)\n",
    "    def _attach_p_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # This function is not used in the prediction pipeline\n",
    "        return df\n",
    "\n",
    "    def _merge_file(self, df: pd.DataFrame, filename: str, cols: list, filter_col: Optional[str] = None, filter_val: Optional[str] = None) -> pd.DataFrame:\n",
    "        try:\n",
    "            aux = self._safe_load(filename, cols=cols + ([filter_col] if filter_col else []))\n",
    "            if aux.empty: return df\n",
    "            if filter_col:\n",
    "                aux = aux[aux[filter_col] == filter_val].drop(columns=[filter_col])\n",
    "            aux = aux.drop_duplicates('nct_id')\n",
    "            return df.merge(aux, on='nct_id', how='left')\n",
    "        except:\n",
    "            return df\n",
    "\n",
    "    def save(self, df: pd.DataFrame, filename: str = 'data_predict.csv'):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION CODE (The part that runs the process)\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # 1. Instantiate the loader\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # --- STEP 1: Load and Calculate Time-Sensitive Features on the Full Cohort (2005-2025) ---\n",
    "    df_full = loader.load_full_cohort_for_competition(min_year=2005, max_year=2025)\n",
    "\n",
    "    # --- STEP 2: Generate Prediction Set (2024-2025, Active/Pending) ---\n",
    "    # Note: start_year_predict=2024 enforces the new requirement\n",
    "    df_predict = loader.load_and_clean_prediction_set(df_full, start_year_predict=2024)\n",
    "\n",
    "    # 3. Save the final dataset for prediction\n",
    "    loader.save(df_predict)\n",
    "\n",
    "    print(\"\\n[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[CRITICAL FAILURE] Data loading stopped: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check if 'studies.txt' is present in the DATA_PATH folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[UNEXPECTED FAILURE] An unexpected error occurred: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check file paths, file names, and data integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3573c53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDICTION DATASET AUDIT: data_predict.csv\n",
      "================================================================================\n",
      "Training Set Size (project_data.csv): 48294 rows x 147 cols\n",
      "Prediction Set Size (data_predict.csv): 13747 rows x 69 cols\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================== COLUMN COMPARISON ==============================\n",
      "Total Common Columns: 64\n",
      "Total NEW Columns in Prediction Set: 5\n",
      "Total MISSING Columns from Training Set: 83\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[A] NEW COLUMNS (User-Facing/NLP Fields)\n",
      "--------------------------------------------------------------------------------\n",
      "- brief_summary_text\n",
      "- brief_title\n",
      "- criteria\n",
      "- detailed_description_text\n",
      "- official_title\n",
      "\n",
      "[B] MISSING COLUMNS (Expected Exclusions)\n",
      "--------------------------------------------------------------------------------\n",
      "- emb_20 (WARNING: Missing from Prediction Set)\n",
      "- emb_21 (WARNING: Missing from Prediction Set)\n",
      "- emb_22 (WARNING: Missing from Prediction Set)\n",
      "- emb_23 (WARNING: Missing from Prediction Set)\n",
      "- emb_24 (WARNING: Missing from Prediction Set)\n",
      "- emb_25 (WARNING: Missing from Prediction Set)\n",
      "- emb_26 (WARNING: Missing from Prediction Set)\n",
      "- emb_27 (WARNING: Missing from Prediction Set)\n",
      "- emb_28 (WARNING: Missing from Prediction Set)\n",
      "- emb_29 (WARNING: Missing from Prediction Set)\n",
      "- emb_30 (WARNING: Missing from Prediction Set)\n",
      "- emb_31 (WARNING: Missing from Prediction Set)\n",
      "- emb_32 (WARNING: Missing from Prediction Set)\n",
      "- emb_33 (WARNING: Missing from Prediction Set)\n",
      "- emb_34 (WARNING: Missing from Prediction Set)\n",
      "- emb_35 (WARNING: Missing from Prediction Set)\n",
      "- emb_36 (WARNING: Missing from Prediction Set)\n",
      "- emb_37 (WARNING: Missing from Prediction Set)\n",
      "- emb_38 (WARNING: Missing from Prediction Set)\n",
      "- emb_39 (WARNING: Missing from Prediction Set)\n",
      "- emb_40 (WARNING: Missing from Prediction Set)\n",
      "- emb_41 (WARNING: Missing from Prediction Set)\n",
      "- emb_42 (WARNING: Missing from Prediction Set)\n",
      "- emb_43 (WARNING: Missing from Prediction Set)\n",
      "- emb_44 (WARNING: Missing from Prediction Set)\n",
      "- emb_45 (WARNING: Missing from Prediction Set)\n",
      "- emb_46 (WARNING: Missing from Prediction Set)\n",
      "- emb_47 (WARNING: Missing from Prediction Set)\n",
      "- emb_48 (WARNING: Missing from Prediction Set)\n",
      "- emb_49 (WARNING: Missing from Prediction Set)\n",
      "- emb_50 (WARNING: Missing from Prediction Set)\n",
      "- emb_51 (WARNING: Missing from Prediction Set)\n",
      "- emb_52 (WARNING: Missing from Prediction Set)\n",
      "- emb_53 (WARNING: Missing from Prediction Set)\n",
      "- emb_54 (WARNING: Missing from Prediction Set)\n",
      "- emb_55 (WARNING: Missing from Prediction Set)\n",
      "- emb_56 (WARNING: Missing from Prediction Set)\n",
      "- emb_57 (WARNING: Missing from Prediction Set)\n",
      "- emb_58 (WARNING: Missing from Prediction Set)\n",
      "- emb_59 (WARNING: Missing from Prediction Set)\n",
      "- emb_60 (WARNING: Missing from Prediction Set)\n",
      "- emb_61 (WARNING: Missing from Prediction Set)\n",
      "- emb_62 (WARNING: Missing from Prediction Set)\n",
      "- emb_63 (WARNING: Missing from Prediction Set)\n",
      "- emb_64 (WARNING: Missing from Prediction Set)\n",
      "- emb_65 (WARNING: Missing from Prediction Set)\n",
      "- emb_66 (WARNING: Missing from Prediction Set)\n",
      "- emb_67 (WARNING: Missing from Prediction Set)\n",
      "- emb_68 (WARNING: Missing from Prediction Set)\n",
      "- emb_69 (WARNING: Missing from Prediction Set)\n",
      "- emb_70 (WARNING: Missing from Prediction Set)\n",
      "- emb_71 (WARNING: Missing from Prediction Set)\n",
      "- emb_72 (WARNING: Missing from Prediction Set)\n",
      "- emb_73 (WARNING: Missing from Prediction Set)\n",
      "- emb_74 (WARNING: Missing from Prediction Set)\n",
      "- emb_75 (WARNING: Missing from Prediction Set)\n",
      "- emb_76 (WARNING: Missing from Prediction Set)\n",
      "- emb_77 (WARNING: Missing from Prediction Set)\n",
      "- emb_78 (WARNING: Missing from Prediction Set)\n",
      "- emb_79 (WARNING: Missing from Prediction Set)\n",
      "- emb_80 (WARNING: Missing from Prediction Set)\n",
      "- emb_81 (WARNING: Missing from Prediction Set)\n",
      "- emb_82 (WARNING: Missing from Prediction Set)\n",
      "- emb_83 (WARNING: Missing from Prediction Set)\n",
      "- emb_84 (WARNING: Missing from Prediction Set)\n",
      "- emb_85 (WARNING: Missing from Prediction Set)\n",
      "- emb_86 (WARNING: Missing from Prediction Set)\n",
      "- emb_87 (WARNING: Missing from Prediction Set)\n",
      "- emb_88 (WARNING: Missing from Prediction Set)\n",
      "- emb_89 (WARNING: Missing from Prediction Set)\n",
      "- emb_90 (WARNING: Missing from Prediction Set)\n",
      "- emb_91 (WARNING: Missing from Prediction Set)\n",
      "- emb_92 (WARNING: Missing from Prediction Set)\n",
      "- emb_93 (WARNING: Missing from Prediction Set)\n",
      "- emb_94 (WARNING: Missing from Prediction Set)\n",
      "- emb_95 (WARNING: Missing from Prediction Set)\n",
      "- emb_96 (WARNING: Missing from Prediction Set)\n",
      "- emb_97 (WARNING: Missing from Prediction Set)\n",
      "- emb_98 (WARNING: Missing from Prediction Set)\n",
      "- emb_99 (WARNING: Missing from Prediction Set)\n",
      "- min_p_value (EXPECTED: Target/Leakage Feature)\n",
      "- scientific_success (EXPECTED: Target/Leakage Feature)\n",
      "- target (EXPECTED: Target/Leakage Feature)\n",
      "\n",
      "============================== DATA QUALITY & COMPLETENESS ========================\n",
      "| Feature                      | Type    | Nulls (%)   |   Unique | Flag     |\n",
      "|:-----------------------------|:--------|:------------|---------:|:---------|\n",
      "| detailed_description_text    | object  | 43.15%      |     7777 |          |\n",
      "| allocation                   | object  | 33.19%      |        2 |          |\n",
      "| tree_number                  | object  | 23.95%      |      673 |          |\n",
      "| therapeutic_area             | object  | 23.55%      |       23 | CRITICAL |\n",
      "| best_pathology               | object  | 23.55%      |      710 |          |\n",
      "| why_stopped                  | float64 | 100.00%     |        0 |          |\n",
      "| number_of_arms               | float64 | 0.13%       |       26 |          |\n",
      "| adult                        | int64   | 0.00%       |        2 |          |\n",
      "| phase                        | object  | 0.00%       |        5 |          |\n",
      "| overall_status               | object  | 0.00%       |        5 | CRITICAL |\n",
      "| older_adult                  | int64   | 0.00%       |        2 |          |\n",
      "| num_primary_endpoints        | int64   | 0.00%       |       45 | CRITICAL |\n",
      "| name                         | object  | 0.00%       |     9870 |          |\n",
      "| nct_id                       | object  | 0.00%       |    13747 |          |\n",
      "| score_allocation             | int64   | 0.00%       |        2 |          |\n",
      "| masking                      | object  | 0.00%       |        5 |          |\n",
      "| lead_sponsor                 | object  | 0.00%       |     4292 |          |\n",
      "| is_sick_only                 | int64   | 0.00%       |        2 |          |\n",
      "| is_gender_restricted         | int64   | 0.00%       |        2 |          |\n",
      "| primary_purpose              | object  | 0.00%       |        8 |          |\n",
      "| sponsor_tier                 | object  | 0.00%       |        2 | CRITICAL |\n",
      "| score_masking                | int64   | 0.00%       |        4 |          |\n",
      "| score_model                  | int64   | 0.00%       |        2 |          |\n",
      "| sponsor_clean                | object  | 0.00%       |     4244 |          |\n",
      "| intervention_model           | object  | 0.00%       |        5 |          |\n",
      "| start_date                   | object  | 0.00%       |      703 |          |\n",
      "| start_year                   | float64 | 0.00%       |        2 |          |\n",
      "| study_type                   | object  | 0.00%       |        1 |          |\n",
      "| therapeutic_subgroup_name    | object  | 0.00%       |      711 |          |\n",
      "| txt_criteria                 | object  | 0.00%       |    13677 |          |\n",
      "| txt_tags                     | object  | 0.00%       |    13710 |          |\n",
      "| brief_summary_text           | object  | 0.00%       |    13662 |          |\n",
      "| brief_title                  | object  | 0.00%       |    13732 |          |\n",
      "| criteria                     | object  | 0.00%       |    13677 |          |\n",
      "| is_fda_regulated_drug        | int64   | 0.00%       |        2 |          |\n",
      "| healthy_volunteers           | object  | 0.00%       |        2 |          |\n",
      "| includes_us                  | int64   | 0.00%       |        2 |          |\n",
      "| emb_14                       | float64 | 0.00%       |    13664 |          |\n",
      "| agent_category               | object  | 0.00%       |       19 | CRITICAL |\n",
      "| child                        | int64   | 0.00%       |        2 |          |\n",
      "| competition_broad            | int64   | 0.00%       |       42 | CRITICAL |\n",
      "| competition_niche            | int64   | 0.00%       |      136 |          |\n",
      "| covid_exposure               | int64   | 0.00%       |        1 |          |\n",
      "| criteria_len_log             | float64 | 0.00%       |     6507 | CRITICAL |\n",
      "| design_rigor_score           | int64   | 0.00%       |        6 | CRITICAL |\n",
      "| eligibility_strictness_score | int64   | 0.00%       |        5 |          |\n",
      "| emb_0                        | float64 | 0.00%       |    13662 |          |\n",
      "| emb_1                        | float64 | 0.00%       |    13665 |          |\n",
      "| emb_10                       | float64 | 0.00%       |    13664 |          |\n",
      "| emb_11                       | float64 | 0.00%       |    13667 |          |\n",
      "| emb_12                       | float64 | 0.00%       |    13662 |          |\n",
      "| emb_13                       | float64 | 0.00%       |    13662 |          |\n",
      "| emb_15                       | float64 | 0.00%       |    13664 |          |\n",
      "| agency_class                 | object  | 0.00%       |        8 |          |\n",
      "| emb_16                       | float64 | 0.00%       |    13657 |          |\n",
      "| emb_17                       | float64 | 0.00%       |    13661 |          |\n",
      "| emb_18                       | float64 | 0.00%       |    13667 |          |\n",
      "| emb_19                       | float64 | 0.00%       |    13665 |          |\n",
      "| emb_2                        | float64 | 0.00%       |    13666 |          |\n",
      "| emb_3                        | float64 | 0.00%       |    13663 |          |\n",
      "| emb_4                        | float64 | 0.00%       |    13664 |          |\n",
      "| emb_5                        | float64 | 0.00%       |    13660 |          |\n",
      "| emb_6                        | float64 | 0.00%       |    13665 |          |\n",
      "| emb_7                        | float64 | 0.00%       |    13661 |          |\n",
      "| emb_8                        | float64 | 0.00%       |    13664 |          |\n",
      "| emb_9                        | float64 | 0.00%       |    13661 |          |\n",
      "| gender                       | object  | 0.00%       |        3 |          |\n",
      "| has_dmc                      | int64   | 0.00%       |        2 |          |\n",
      "| official_title               | object  | 0.00%       |    13684 |          |\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL DATA COMPLETENESS SCORE: 96.41%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================== STATISTICAL DRIFT CHECK (Train vs. Predict) =============\n",
      "| Feature               |   Train Mean |   Predict Mean | Difference (%)   |\n",
      "|:----------------------|-------------:|---------------:|:-----------------|\n",
      "| design_rigor_score    |         1.8  |           1.42 | -21.00%          |\n",
      "| competition_broad     |       840.4  |        2076.86 | 147.13%          |\n",
      "| competition_niche     |       147.96 |        1041.69 | 604.02%          |\n",
      "| num_primary_endpoints |         1.74 |           2.12 | 21.90%           |\n",
      "| number_of_arms        |         2.28 |           2.25 | -1.15%           |\n",
      "| criteria_len_log      |         7.38 |           7.81 | 5.92%            |\n",
      "\n",
      "============================== CATEGORICAL DRIFT CHECK (Top 5 Categories) ===========\n",
      "\n",
      "--- Feature: agent_category ---\n",
      "| agent_category            |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:--------------------------|------------:|--------------:|-------------:|\n",
      "| MONOCLONAL_ANTIBODY       |   0.0603802 |     0.119662  |   0.0592823  |\n",
      "| KINASE_INHIBITOR_TYROSINE |   0         |     0.0370263 |   0.0370263  |\n",
      "| BIOLOGIC                  |   0.117323  |     0.110351  |  -0.00697171 |\n",
      "| PLACEBO_CTRL              |   0.0575227 |     0.0352077 |  -0.022315   |\n",
      "| SMALL_MOLECULE_OTHER      |   0.640473  |     0.605296  |  -0.0351772  |\n",
      "| CHEMOTHERAPY              |   0.0376651 |     0         |  -0.0376651  |\n",
      "\n",
      "--- Feature: therapeutic_area ---\n",
      "| therapeutic_area              |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:------------------------------|------------:|--------------:|-------------:|\n",
      "| Oncology                      |   0.331796  |     0.46451   |    0.132714  |\n",
      "| Immunology                    |   0         |     0.056137  |    0.056137  |\n",
      "| Cardiovascular                |   0.0754379 |     0.0647954 |   -0.0106424 |\n",
      "| Neurology                     |   0.122485  |     0.100285  |   -0.0221992 |\n",
      "| Infections (Bacterial/Fungal) |   0.112377  |     0.074215  |   -0.0381615 |\n",
      "| Metabolic                     |   0.061254  |     0         |   -0.061254  |\n",
      "\n",
      "--- Feature: phase ---\n",
      "| phase         |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:--------------|------------:|--------------:|-------------:|\n",
      "| PHASE1        |   0         |     0.241871  |   0.241871   |\n",
      "| PHASE1/PHASE2 |   0.108067  |     0.123081  |   0.0150141  |\n",
      "| PHASE2/PHASE3 |   0.0455543 |     0.0378992 |  -0.00765514 |\n",
      "| PHASE2        |   0.499089  |     0.399142  |  -0.0999473  |\n",
      "| PHASE3        |   0.34729   |     0.198007  |  -0.149283   |\n",
      "\n",
      "--- Feature: sponsor_tier ---\n",
      "| sponsor_tier   |   Train_Pct |   Predict_Pct |   Difference |\n",
      "|:---------------|------------:|--------------:|-------------:|\n",
      "| TIER_2_OTHER   |    0.742618 |      0.895468 |      0.15285 |\n",
      "| TIER_1_GIANT   |    0.257382 |      0.104532 |     -0.15285 |\n",
      "\n",
      "[REPORT SAVED] Detailed audit report saved to: /home/delaunan/code/delaunan/clintrialpredict/data/data_audit_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "# This MUST match the path used to generate the files\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "TRAIN_FILE = 'project_data.csv'\n",
    "PREDICT_FILE = 'data_predict.csv'\n",
    "AUDIT_REPORT_FILE = 'data_audit_report.txt'\n",
    "\n",
    "# ====================================================================\n",
    "# 1. LOAD DATASETS\n",
    "# ====================================================================\n",
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILE))\n",
    "    df_predict = pd.read_csv(os.path.join(DATA_PATH, PREDICT_FILE))\n",
    "\n",
    "    # Ensure all columns are treated as strings for initial comparison\n",
    "    train_cols = set(df_train.columns.astype(str))\n",
    "    predict_cols = set(df_predict.columns.astype(str))\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"PREDICTION DATASET AUDIT: {PREDICT_FILE}\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Training Set Size (project_data.csv): {len(df_train)} rows x {len(train_cols)} cols\")\n",
    "    report.append(f\"Prediction Set Size (data_predict.csv): {len(df_predict)} rows x {len(predict_cols)} cols\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    report.append(f\"CRITICAL ERROR: File not found. Check DATA_PATH and filenames. Error: {e}\")\n",
    "    print(\"\\n\".join(report))\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    report.append(f\"CRITICAL ERROR: Failed to load CSVs. Error: {e}\")\n",
    "    print(\"\\n\".join(report))\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 2. COLUMN COMPARISON\n",
    "# ====================================================================\n",
    "\n",
    "# Identify column sets\n",
    "common_cols = sorted(list(train_cols.intersection(predict_cols)))\n",
    "new_cols = sorted(list(predict_cols.difference(train_cols)))\n",
    "missing_cols = sorted(list(train_cols.difference(predict_cols)))\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" COLUMN COMPARISON \" + \"=\" * 30)\n",
    "report.append(f\"Total Common Columns: {len(common_cols)}\")\n",
    "report.append(f\"Total NEW Columns in Prediction Set: {len(new_cols)}\")\n",
    "report.append(f\"Total MISSING Columns from Training Set: {len(missing_cols)}\")\n",
    "report.append(\"-\" * 80)\n",
    "\n",
    "report.append(\"\\n[A] NEW COLUMNS (User-Facing/NLP Fields)\")\n",
    "report.append(\"--------------------------------------------------------------------------------\")\n",
    "if new_cols:\n",
    "    for col in new_cols:\n",
    "        report.append(f\"- {col}\")\n",
    "else:\n",
    "    report.append(\"- NONE (Unexpected - should have UI/NLP fields)\")\n",
    "\n",
    "report.append(\"\\n[B] MISSING COLUMNS (Expected Exclusions)\")\n",
    "report.append(\"--------------------------------------------------------------------------------\")\n",
    "if missing_cols:\n",
    "    for col in missing_cols:\n",
    "        # Flag expected leakage columns\n",
    "        if col in ['target', 'min_p_value', 'scientific_success']:\n",
    "            report.append(f\"- {col} (EXPECTED: Target/Leakage Feature)\")\n",
    "        else:\n",
    "            report.append(f\"- {col} (WARNING: Missing from Prediction Set)\")\n",
    "else:\n",
    "    report.append(\"- NONE (All training features are present or explicitly excluded)\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 3. DATA QUALITY & COMPLETENESS (Prediction Set)\n",
    "# ====================================================================\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" DATA QUALITY & COMPLETENESS \" + \"=\" * 24)\n",
    "\n",
    "# Define critical features for production quality check\n",
    "CRITICAL_FEATURES = [\n",
    "    'design_rigor_score', 'agent_category', 'competition_broad',\n",
    "    'sponsor_tier', 'num_primary_endpoints', 'criteria_len_log',\n",
    "    'therapeutic_area', 'overall_status'\n",
    "]\n",
    "\n",
    "quality_data = []\n",
    "total_rows = len(df_predict)\n",
    "total_missing_cells = 0\n",
    "\n",
    "for col in common_cols + new_cols:\n",
    "    dtype = df_predict[col].dtype\n",
    "    n_null = df_predict[col].isnull().sum()\n",
    "    pct_null = (n_null / total_rows) * 100\n",
    "    n_unique = df_predict[col].nunique()\n",
    "\n",
    "    total_missing_cells += n_null\n",
    "\n",
    "    is_critical = \"CRITICAL\" if col in CRITICAL_FEATURES else \"\"\n",
    "\n",
    "    quality_data.append({\n",
    "        'Feature': col,\n",
    "        'Type': dtype,\n",
    "        'Nulls (%)': f\"{pct_null:.2f}%\",\n",
    "        'Unique': n_unique,\n",
    "        'Flag': is_critical\n",
    "    })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_data).sort_values(by='Nulls (%)', ascending=False)\n",
    "report.append(df_quality.to_markdown(index=False))\n",
    "\n",
    "# Overall Completeness Score\n",
    "total_cells = total_rows * len(df_predict.columns)\n",
    "completeness_score = 100 * (1 - (total_missing_cells / total_cells))\n",
    "report.append(\"-\" * 80)\n",
    "report.append(f\"OVERALL DATA COMPLETENESS SCORE: {completeness_score:.2f}%\")\n",
    "report.append(\"-\" * 80)\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 4. STATISTICAL COMPARISON (DATA DRIFT CHECK)\n",
    "# ====================================================================\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" STATISTICAL DRIFT CHECK (Train vs. Predict) \" + \"=\" * 13)\n",
    "\n",
    "# Select key numerical features for comparison\n",
    "NUMERIC_FEATURES = [\n",
    "    'design_rigor_score', 'competition_broad', 'competition_niche',\n",
    "    'num_primary_endpoints', 'number_of_arms', 'criteria_len_log'\n",
    "]\n",
    "\n",
    "drift_data = []\n",
    "for col in NUMERIC_FEATURES:\n",
    "    if col in common_cols:\n",
    "        train_mean = df_train[col].mean()\n",
    "        predict_mean = df_predict[col].mean()\n",
    "        diff_pct = 100 * (predict_mean - train_mean) / train_mean\n",
    "\n",
    "        drift_data.append({\n",
    "            'Feature': col,\n",
    "            'Train Mean': f\"{train_mean:.2f}\",\n",
    "            'Predict Mean': f\"{predict_mean:.2f}\",\n",
    "            'Difference (%)': f\"{diff_pct:.2f}%\"\n",
    "        })\n",
    "\n",
    "df_drift = pd.DataFrame(drift_data)\n",
    "report.append(df_drift.to_markdown(index=False))\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 30 + \" CATEGORICAL DRIFT CHECK (Top 5 Categories) \" + \"=\" * 11)\n",
    "\n",
    "# Categorical comparison for the strongest features\n",
    "CATEGORICAL_FEATURES = ['agent_category', 'therapeutic_area', 'phase', 'sponsor_tier']\n",
    "\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in common_cols:\n",
    "        train_counts = df_train[col].value_counts(normalize=True).head(5)\n",
    "        predict_counts = df_predict[col].value_counts(normalize=True).head(5)\n",
    "\n",
    "        combined = pd.concat([train_counts, predict_counts], axis=1, keys=['Train_Pct', 'Predict_Pct']).fillna(0)\n",
    "        combined['Difference'] = combined['Predict_Pct'] - combined['Train_Pct']\n",
    "\n",
    "        report.append(f\"\\n--- Feature: {col} ---\")\n",
    "        report.append(combined.sort_values(by='Difference', ascending=False).to_markdown())\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 5. FINAL OUTPUT\n",
    "# ====================================================================\n",
    "\n",
    "final_report = \"\\n\".join(report)\n",
    "print(final_report)\n",
    "\n",
    "# Save the report to a file\n",
    "with open(os.path.join(DATA_PATH, AUDIT_REPORT_FILE), 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(f\"\\n[REPORT SAVED] Detailed audit report saved to: {os.path.join(DATA_PATH, AUDIT_REPORT_FILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9e9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING FULL HISTORY NLP CRITERIA GENERATION ---\n",
      ">>> 1. Loading FULL HISTORY NLP Cohort (2005-2025)...\n",
      "    Final NLP Cohort Size: 137724 trials (Full History, Phase 1/2/3)\n",
      ">>> Saved 137724 rows to /home/delaunan/code/delaunan/clintrialpredict/data/nlp_criteria_full_history_2005_2025.csv\n",
      "\n",
      "[SUCCESS] Full History NLP criteria set ready for colleague at: /home/delaunan/code/delaunan/clintrialpredict/data/nlp_criteria_full_history_2005_2025.csv\n",
      "WARNING: This file is large and contains the entire 2005-2025 Phase 1/2/3 cohort. BioBERT processing will take significant time/resources.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION (THE ONLY LINE YOU MUST VERIFY)\n",
    "# ====================================================================\n",
    "# This MUST be the absolute path to the FOLDER containing your AACT .txt files.\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "NLP_OUTPUT_FILE = 'nlp_criteria_full_history_2005_2025.csv'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING FULL HISTORY NLP CRITERIA GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS (Full Definition)\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    \"\"\"\n",
    "    A specialized loader for generating the full historical NLP criteria dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "\n",
    "        # --- STRATEGY A: PERFECT ---\n",
    "        self.params_perfect = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "        # --- STRATEGY B: ROBUST ---\n",
    "        self.params_robust = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"   [!] Warning: File not found {filename}. Features will be empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except Exception as e:\n",
    "            print(f\"   [!] Formatting error in {filename}. Switching to Robust Mode...\")\n",
    "            try:\n",
    "                return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except Exception as e2:\n",
    "                print(f\"   [x] CRITICAL: Could not load {filename}. Error: {e2}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "    # --- MAIN FUNCTION: LOAD FULL HISTORY NLP CRITERIA ---\n",
    "\n",
    "    def load_full_history_nlp_set(self, min_year: int = 2005, max_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the full historical dataset (2005-2025) for NLP embedding generation,\n",
    "        including Phase 1 trials to maximize data for BioBERT training.\n",
    "        \"\"\"\n",
    "        print(f\">>> 1. Loading FULL HISTORY NLP Cohort ({min_year}-{max_year})...\")\n",
    "\n",
    "        # 1. Load Core Columns (Minimal set for filtering)\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date']\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Critical Error: 'studies.txt' failed to load.\")\n",
    "\n",
    "        # 2. Apply Core Filters (Interventional, Drug)\n",
    "        df = df[df['study_type'].str.upper() == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "        cols_int = ['nct_id', 'intervention_type']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            drug_ids = df_int[df_int['intervention_type'].str.upper().isin(target_types)]['nct_id'].unique()\n",
    "            df = df[df['nct_id'].isin(drug_ids)]\n",
    "\n",
    "        # 3. CRITICAL CHANGE: Phase Filter (Keep Phase 1, Phase 2, Phase 3)\n",
    "        # We only exclude EARLY_PHASE1, PHASE4, and NA\n",
    "        excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA']\n",
    "        df = df[~df['phase'].astype(str).str.upper().isin(excluded_phases)]\n",
    "        df = df.dropna(subset=['phase'])\n",
    "\n",
    "        # 4. Apply Date Filter (2005-2025)\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "        df = df[df['start_year'].between(min_year, max_year)].copy()\n",
    "\n",
    "        # 5. CRITICAL: Load Eligibility Criteria Text\n",
    "        cols_elig = ['nct_id', 'criteria']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_elig)\n",
    "\n",
    "        if df_elig.empty:\n",
    "            print(\"    [!] Warning: 'eligibilities.txt' failed to load. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame(columns=['nct_id', 'criteria'])\n",
    "\n",
    "        # 6. Final Merge and Cleanup\n",
    "        # Merge on nct_id to get the criteria for the filtered studies\n",
    "        df_final = df[['nct_id']].merge(df_elig.drop_duplicates('nct_id'), on='nct_id', how='inner')\n",
    "\n",
    "\n",
    "        # Rename column for clarity in the final output file\n",
    "        df_final.rename(columns={'criteria': 'txt_criteria'}, inplace=True)\n",
    "\n",
    "        print(f\"    Final NLP Cohort Size: {len(df_final)} trials (Full History, Phase 1/2/3)\")\n",
    "        return df_final[['nct_id', 'txt_criteria']].copy()\n",
    "\n",
    "    def save(self, df: pd.DataFrame, filename: str = 'project_data.csv'):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION CODE\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # 1. Instantiate the loader\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # 2. Generate the NLP-focused dataset (2005-2025, including Phase 1)\n",
    "    df_nlp_full = loader.load_full_history_nlp_set(min_year=2005, max_year=2025)\n",
    "\n",
    "    # 3. Save the final dataset\n",
    "    loader.save(df_nlp_full, filename=NLP_OUTPUT_FILE)\n",
    "\n",
    "    print(f\"\\n[SUCCESS] Full History NLP criteria set ready for colleague at: {os.path.join(DATA_PATH, NLP_OUTPUT_FILE)}\")\n",
    "    print(\"WARNING: This file is large and contains the entire 2005-2025 Phase 1/2/3 cohort. BioBERT processing will take significant time/resources.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[CRITICAL FAILURE] Data loading stopped: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check if 'studies.txt' is present in the DATA_PATH folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[UNEXPECTED FAILURE] An unexpected error occurred: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check file paths, file names, and data integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d6527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc75efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING DEMO SET GENERATION ---\n",
      "Found 452 unique NCT IDs to process.\n",
      ">>> 1. Loading Specific Cohort (452 trials)...\n",
      "    Base Cohort Loaded: 452 trials.\n",
      ">>> 2. Engineering Features...\n",
      ">>> Saved 452 rows to /home/delaunan/code/delaunan/clintrialpredict/data/demo_set.csv\n",
      "\n",
      "[SUCCESS] Demo Set Generated.\n",
      "File: /home/delaunan/code/delaunan/clintrialpredict/data/demo_set.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION\n",
    "# ====================================================================\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "INPUT_LIST_FILE = 'data_predict.csv'\n",
    "OUTPUT_FILE = 'demo_set.csv'\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING DEMO SET GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "        self.params_perfect = {\"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"', \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"}\n",
    "        self.params_robust = {\"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"', \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"}\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path): return pd.DataFrame()\n",
    "        try: return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except:\n",
    "            try: return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except: return pd.DataFrame()\n",
    "\n",
    "    def load_specific_trials(self, nct_ids: list) -> pd.DataFrame:\n",
    "        print(f\">>> 1. Loading Specific Cohort ({len(nct_ids)} trials)...\")\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date', 'number_of_arms', 'official_title', 'why_stopped', 'has_dmc', 'is_fda_regulated_drug', 'brief_title']\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "\n",
    "        # FILTER\n",
    "        df = df[df['nct_id'].isin(nct_ids)].copy()\n",
    "\n",
    "        # Load Interventions\n",
    "        cols_int = ['nct_id', 'intervention_type', 'name']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            df_int = df_int[df_int['nct_id'].isin(nct_ids)]\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            self.df_drugs = df_int[df_int['intervention_type'].str.upper().isin(target_types + ['DIETARY SUPPLEMENT', 'OTHER'])].copy()\n",
    "\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "\n",
    "        df = self._attach_medical_hierarchy(df)\n",
    "        df = self._calculate_competition(df)\n",
    "        print(f\"    Base Cohort Loaded: {len(df)} trials.\")\n",
    "        return df\n",
    "\n",
    "    def add_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\">>> 2. Engineering Features...\")\n",
    "        df['covid_exposure'] = df['start_year'].between(2019, 2022).astype(int)\n",
    "\n",
    "        df_countries = self._safe_load('countries.txt', cols=['nct_id', 'name'])\n",
    "        if not df_countries.empty:\n",
    "            us_trials = df_countries[df_countries['name'] == 'United States']['nct_id'].unique()\n",
    "            df['includes_us'] = df['nct_id'].isin(us_trials).astype(int)\n",
    "        else: df['includes_us'] = 0\n",
    "\n",
    "        df = self._merge_file(df, 'designs.txt', ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose'])\n",
    "        df = self._merge_file(df, 'calculated_values.txt', ['nct_id', 'number_of_primary_outcomes_to_measure'])\n",
    "\n",
    "        df = self._engineer_sponsor_features(df)\n",
    "        df = self._engineer_complexity(df) # <--- This loads gender/healthy_volunteers\n",
    "        df = self._engineer_agent_type(df)\n",
    "        df = self._engineer_smart_patterns(df) # <--- This uses gender\n",
    "        df = self._engineer_safe_features(df)\n",
    "        df = self._prepare_text(df)\n",
    "        df = self._attach_embeddings(df)\n",
    "        df = self._load_user_facing_text(df)\n",
    "\n",
    "        old_col, new_col = 'number_of_primary_outcomes_to_measure', 'num_primary_endpoints'\n",
    "        if old_col in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(1)\n",
    "        else: df[new_col] = 1\n",
    "        return df\n",
    "\n",
    "    def _engineer_complexity(self, df):\n",
    "        # FIX: Actually load the file to get 'gender'\n",
    "        cols_needed = ['nct_id', 'criteria', 'gender', 'healthy_volunteers', 'minimum_age', 'maximum_age']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_needed)\n",
    "\n",
    "        if not df_elig.empty:\n",
    "            df_elig = df_elig.drop_duplicates('nct_id')\n",
    "            df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "        # Fill missing values if merge failed for some rows\n",
    "        if 'gender' not in df.columns: df['gender'] = 'ALL'\n",
    "        if 'healthy_volunteers' not in df.columns: df['healthy_volunteers'] = 'No'\n",
    "        if 'criteria' not in df.columns: df['criteria'] = ''\n",
    "\n",
    "        df['criteria_len_log'] = np.log1p(df['criteria'].astype(str).str.len().fillna(0))\n",
    "        df['healthy_volunteers'] = df['healthy_volunteers'].astype(str).str.lower().apply(lambda x: 'no' if x in ['f', 'false', '0', 'no'] else 'yes')\n",
    "\n",
    "        # Simple Age Logic (Placeholder)\n",
    "        df['child'] = 0\n",
    "        df['adult'] = 1\n",
    "        df['older_adult'] = 0\n",
    "        return df\n",
    "\n",
    "    def _engineer_smart_patterns(self, df):\n",
    "        def get_masking_score(val):\n",
    "            val = str(val).lower()\n",
    "            if 'quadruple' in val: return 3\n",
    "            if 'double' in val: return 2\n",
    "            if 'single' in val: return 1\n",
    "            return 0\n",
    "        df['score_masking'] = df['masking'].apply(get_masking_score)\n",
    "        df['score_allocation'] = df['allocation'].apply(lambda x: 1 if 'randomized' in str(x).lower() else 0)\n",
    "        df['score_model'] = df['intervention_model'].apply(lambda x: 1 if 'crossover' in str(x).lower() or 'factorial' in str(x).lower() else 0)\n",
    "        df['design_rigor_score'] = df['score_masking'] + df['score_allocation'] + df['score_model']\n",
    "\n",
    "        # Now 'gender' is guaranteed to exist\n",
    "        df['gender'] = df['gender'].fillna('ALL')\n",
    "        df['is_gender_restricted'] = df['gender'].apply(lambda x: 0 if str(x).lower() == 'all' else 1)\n",
    "\n",
    "        df['is_sick_only'] = df['healthy_volunteers'].apply(lambda x: 1 if str(x).lower() == 'no' else 0)\n",
    "\n",
    "        for c in ['child', 'adult', 'older_adult']:\n",
    "            if c not in df.columns: df[c] = 1\n",
    "\n",
    "        df['eligibility_strictness_score'] = df['is_gender_restricted'] + df['is_sick_only'] + (1 - df['child']) + (1 - df['older_adult'])\n",
    "        return df\n",
    "\n",
    "    def _engineer_agent_type(self, df):\n",
    "        if self.df_drugs.empty:\n",
    "            df['agent_category'] = 'UNKNOWN'; return df\n",
    "\n",
    "        type_priority = {'GENETIC': 1,'BIOLOGICAL': 2,'DRUG': 3,'DIETARY SUPPLEMENT': 4,'OTHER': 5}\n",
    "        df_int = self.df_drugs.copy()\n",
    "        df_int['type_upper'] = df_int['intervention_type'].str.upper()\n",
    "        df_int['priority'] = df_int['type_upper'].map(lambda x: type_priority.get(x, 5))\n",
    "        df_int = df_int.sort_values('priority')\n",
    "        best_types = df_int.drop_duplicates('nct_id')[['nct_id', 'type_upper', 'name']]\n",
    "        df = df.merge(best_types, on='nct_id', how='left')\n",
    "\n",
    "        def classify_molecule(row):\n",
    "            name = str(row['name']).lower()\n",
    "            itype = str(row['type_upper'])\n",
    "            if 'placebo' in name: return 'PLACEBO_CTRL'\n",
    "            if itype == 'GENETIC' or any(x in name for x in ['car-t', 'crispr', 'mrna']): return 'GENE_THERAPY'\n",
    "            if itype == 'BIOLOGICAL' or 'mab' in name: return 'BIOLOGIC'\n",
    "            return 'SMALL_MOLECULE_OTHER'\n",
    "\n",
    "        df['agent_category'] = df.apply(classify_molecule, axis=1)\n",
    "        df.drop(columns=['type_upper', 'priority', 'name_y'], inplace=True, errors='ignore')\n",
    "        if 'name_x' in df.columns: df.rename(columns={'name_x': 'official_title'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _engineer_safe_features(self, df):\n",
    "        for c in ['has_dmc', 'is_fda_regulated_drug']:\n",
    "            if c in df.columns: df[c] = df[c].astype(str).apply(lambda x: 1 if x.lower() in ['true', 't', '1', 'yes'] else 0)\n",
    "            else: df[c] = 0\n",
    "        return df\n",
    "\n",
    "    def _attach_medical_hierarchy(self, df):\n",
    "        df['therapeutic_subgroup_name'] = 'Unknown'\n",
    "        df['therapeutic_area'] = 'Other'\n",
    "        return df\n",
    "\n",
    "    def _engineer_sponsor_features(self, df):\n",
    "        cols_needed = ['nct_id', 'lead_or_collaborator', 'name', 'agency_class']\n",
    "        df_sponsors = self._safe_load('sponsors.txt', cols=cols_needed)\n",
    "        if df_sponsors.empty:\n",
    "            df['sponsor_tier'] = 'TIER_2_OTHER'; df['sponsor_clean'] = 'UNKNOWN'; return df\n",
    "\n",
    "        leads = df_sponsors[df_sponsors['lead_or_collaborator'].str.lower() == 'lead'].drop_duplicates('nct_id')\n",
    "        leads = leads.rename(columns={'name': 'lead_sponsor'})\n",
    "        df = df.merge(leads[['nct_id', 'lead_sponsor', 'agency_class']], on='nct_id', how='left')\n",
    "\n",
    "        big_pharma = ['pfizer', 'gsk', 'novartis', 'astrazeneca', 'merck', 'roche', 'sanofi', 'johnson', 'bayer', 'lilly', 'abbvie', 'amgen']\n",
    "        df['sponsor_clean'] = df['lead_sponsor'].astype(str).str.lower()\n",
    "        df['sponsor_tier'] = df['sponsor_clean'].apply(lambda x: 'TIER_1_GIANT' if any(k in str(x) for k in big_pharma) else 'TIER_2_OTHER')\n",
    "        return df\n",
    "\n",
    "    def _calculate_competition(self, df):\n",
    "        df['competition_broad'] = 0\n",
    "        df['competition_niche'] = 0\n",
    "        return df\n",
    "\n",
    "    def _prepare_text(self, df):\n",
    "        df['txt_criteria'] = df.get('criteria', '').fillna('')\n",
    "        return df\n",
    "\n",
    "    def _attach_embeddings(self, df):\n",
    "        emb_path = os.path.join(self.data_path, 'embeddings_with_nctid.csv')\n",
    "        if os.path.exists(emb_path):\n",
    "            try:\n",
    "                df_emb = pd.read_csv(emb_path)\n",
    "                if 'nct_id' in df_emb.columns: df = df.merge(df_emb, on='nct_id', how='left')\n",
    "            except: pass\n",
    "        return df\n",
    "\n",
    "    def _load_user_facing_text(self, df):\n",
    "        df_brief = self._safe_load('brief_summaries.txt', cols=['nct_id', 'description'])\n",
    "        if not df_brief.empty:\n",
    "            df_brief = df_brief.rename(columns={'description': 'brief_summary_text'})\n",
    "            df = df.merge(df_brief.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "\n",
    "        df_detailed = self._safe_load('detailed_descriptions.txt', cols=['nct_id', 'description'])\n",
    "        if not df_detailed.empty:\n",
    "            df_detailed = df_detailed.rename(columns={'description': 'detailed_description_text'})\n",
    "            df = df.merge(df_detailed.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        return df\n",
    "\n",
    "    def _merge_file(self, df, filename, cols):\n",
    "        aux = self._safe_load(filename, cols=cols).drop_duplicates('nct_id')\n",
    "        if not aux.empty: return df.merge(aux, on='nct_id', how='left')\n",
    "        return df\n",
    "\n",
    "    def save(self, df, filename):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    input_df = pd.read_csv(os.path.join(DATA_PATH, INPUT_LIST_FILE))\n",
    "    target_nct_ids = input_df['nct_id'].unique().tolist()\n",
    "    print(f\"Found {len(target_nct_ids)} unique NCT IDs to process.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {INPUT_LIST_FILE}. Please run the previous selection script first.\")\n",
    "    sys.exit()\n",
    "\n",
    "loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "df_demo = loader.load_specific_trials(target_nct_ids)\n",
    "df_demo = loader.add_features_for_prediction(df_demo)\n",
    "loader.save(df_demo, filename=OUTPUT_FILE)\n",
    "\n",
    "print(\"\\n[SUCCESS] Demo Set Generated.\")\n",
    "print(f\"File: {os.path.join(DATA_PATH, OUTPUT_FILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0be613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING DEMO SET GENERATION (FIXED) ---\n",
      "Found 452 unique NCT IDs to process.\n",
      ">>> 1. Loading Specific Cohort (452 trials)...\n",
      "    Base Cohort Loaded: 452 trials.\n",
      ">>> 2. Engineering Features...\n",
      ">>> Saved 452 rows to /home/delaunan/code/delaunan/clintrialpredict/data/demo_set.csv\n",
      "\n",
      "[SUCCESS] Demo Set Generated.\n",
      "File: /home/delaunan/code/delaunan/clintrialpredict/data/demo_set.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION\n",
    "# ====================================================================\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "INPUT_LIST_FILE = 'data_predict.csv'\n",
    "OUTPUT_FILE = 'demo_set.csv'\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING DEMO SET GENERATION (FIXED) ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "        self.params_perfect = {\"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"', \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"}\n",
    "        self.params_robust = {\"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"', \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"}\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path): return pd.DataFrame()\n",
    "        try: return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except:\n",
    "            try: return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except: return pd.DataFrame()\n",
    "\n",
    "    def load_specific_trials(self, nct_ids: list) -> pd.DataFrame:\n",
    "        print(f\">>> 1. Loading Specific Cohort ({len(nct_ids)} trials)...\")\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase', 'start_date', 'number_of_arms', 'official_title', 'why_stopped', 'has_dmc', 'is_fda_regulated_drug', 'brief_title']\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "\n",
    "        # FILTER\n",
    "        df = df[df['nct_id'].isin(nct_ids)].copy()\n",
    "\n",
    "        # Load Interventions\n",
    "        cols_int = ['nct_id', 'intervention_type', 'name']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            df_int = df_int[df_int['nct_id'].isin(nct_ids)]\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            self.df_drugs = df_int[df_int['intervention_type'].str.upper().isin(target_types + ['DIETARY SUPPLEMENT', 'OTHER'])].copy()\n",
    "\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "\n",
    "        df = self._attach_medical_hierarchy(df) # <--- FIX IS HERE\n",
    "        df = self._calculate_competition(df)\n",
    "        print(f\"    Base Cohort Loaded: {len(df)} trials.\")\n",
    "        return df\n",
    "\n",
    "    def add_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\">>> 2. Engineering Features...\")\n",
    "        df['covid_exposure'] = df['start_year'].between(2019, 2022).astype(int)\n",
    "\n",
    "        df_countries = self._safe_load('countries.txt', cols=['nct_id', 'name'])\n",
    "        if not df_countries.empty:\n",
    "            us_trials = df_countries[df_countries['name'] == 'United States']['nct_id'].unique()\n",
    "            df['includes_us'] = df['nct_id'].isin(us_trials).astype(int)\n",
    "        else: df['includes_us'] = 0\n",
    "\n",
    "        df = self._merge_file(df, 'designs.txt', ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose'])\n",
    "        df = self._merge_file(df, 'calculated_values.txt', ['nct_id', 'number_of_primary_outcomes_to_measure'])\n",
    "\n",
    "        df = self._engineer_sponsor_features(df)\n",
    "        df = self._engineer_complexity(df)\n",
    "        df = self._engineer_agent_type(df)\n",
    "        df = self._engineer_smart_patterns(df)\n",
    "        df = self._engineer_safe_features(df)\n",
    "        df = self._prepare_text(df)\n",
    "        df = self._attach_embeddings(df)\n",
    "        df = self._load_user_facing_text(df)\n",
    "\n",
    "        old_col, new_col = 'number_of_primary_outcomes_to_measure', 'num_primary_endpoints'\n",
    "        if old_col in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(1)\n",
    "        else: df[new_col] = 1\n",
    "        return df\n",
    "\n",
    "    def _attach_medical_hierarchy(self, df):\n",
    "        # 1. Load Bridge (NCT -> Mesh Term)\n",
    "        cols_bridge = ['nct_id', 'mesh_term']\n",
    "        df_bridge = self._safe_load('browse_conditions.txt', cols=cols_bridge)\n",
    "\n",
    "        # 2. Load Dictionary (Mesh Term -> Area)\n",
    "        mesh_path = os.path.join(self.data_path, 'mesh_lookup.csv')\n",
    "        df_dictionary = pd.DataFrame()\n",
    "        if os.path.exists(mesh_path):\n",
    "            try:\n",
    "                df_dictionary = pd.read_csv(mesh_path, sep='|', on_bad_lines='skip')\n",
    "            except: pass\n",
    "\n",
    "        # 3. Merge\n",
    "        if not df_bridge.empty:\n",
    "            if not df_dictionary.empty:\n",
    "                df_full = df_bridge.merge(df_dictionary, on='mesh_term', how='left')\n",
    "            else:\n",
    "                df_full = df_bridge\n",
    "                df_full['therapeutic_area'] = 'Other'\n",
    "\n",
    "            # Group by NCT to get one row per trial\n",
    "            df_grouped = df_full.groupby('nct_id').first().reset_index()\n",
    "            df = df.merge(df_grouped, on='nct_id', how='left')\n",
    "\n",
    "        # 4. CRITICAL FIX: Ensure 'best_pathology' exists\n",
    "        # In your training logic, best_pathology comes from smart_pathology_lookup.csv OR falls back to mesh_term\n",
    "        smart_path = os.path.join(self.data_path, 'smart_pathology_lookup.csv')\n",
    "        if os.path.exists(smart_path):\n",
    "            try:\n",
    "                df_smart = pd.read_csv(smart_path)\n",
    "                if 'nct_id' in df_smart.columns:\n",
    "                    df = df.merge(df_smart[['nct_id', 'best_pathology']], on='nct_id', how='left')\n",
    "            except: pass\n",
    "\n",
    "        # Fallbacks\n",
    "        if 'best_pathology' not in df.columns:\n",
    "            # If smart lookup failed, use mesh_term as proxy for best_pathology\n",
    "            df['best_pathology'] = df.get('mesh_term', 'Unknown')\n",
    "\n",
    "        df['best_pathology'] = df['best_pathology'].fillna('Unknown')\n",
    "        df['therapeutic_area'] = df.get('therapeutic_area', 'Other').fillna('Other')\n",
    "        df['therapeutic_subgroup_name'] = df.get('mesh_term', 'Unknown').fillna('Unknown')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _engineer_complexity(self, df):\n",
    "        cols_needed = ['nct_id', 'criteria', 'gender', 'healthy_volunteers', 'minimum_age', 'maximum_age']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_needed)\n",
    "\n",
    "        if not df_elig.empty:\n",
    "            df_elig = df_elig.drop_duplicates('nct_id')\n",
    "            df = df.merge(df_elig, on='nct_id', how='left')\n",
    "\n",
    "        if 'gender' not in df.columns: df['gender'] = 'ALL'\n",
    "        if 'healthy_volunteers' not in df.columns: df['healthy_volunteers'] = 'No'\n",
    "        if 'criteria' not in df.columns: df['criteria'] = ''\n",
    "\n",
    "        df['criteria_len_log'] = np.log1p(df['criteria'].astype(str).str.len().fillna(0))\n",
    "        df['healthy_volunteers'] = df['healthy_volunteers'].astype(str).str.lower().apply(lambda x: 'no' if x in ['f', 'false', '0', 'no'] else 'yes')\n",
    "\n",
    "        df['child'] = 0\n",
    "        df['adult'] = 1\n",
    "        df['older_adult'] = 0\n",
    "        return df\n",
    "\n",
    "    def _engineer_smart_patterns(self, df):\n",
    "        def get_masking_score(val):\n",
    "            val = str(val).lower()\n",
    "            if 'quadruple' in val: return 3\n",
    "            if 'double' in val: return 2\n",
    "            if 'single' in val: return 1\n",
    "            return 0\n",
    "        df['score_masking'] = df['masking'].apply(get_masking_score)\n",
    "        df['score_allocation'] = df['allocation'].apply(lambda x: 1 if 'randomized' in str(x).lower() else 0)\n",
    "        df['score_model'] = df['intervention_model'].apply(lambda x: 1 if 'crossover' in str(x).lower() or 'factorial' in str(x).lower() else 0)\n",
    "        df['design_rigor_score'] = df['score_masking'] + df['score_allocation'] + df['score_model']\n",
    "\n",
    "        df['gender'] = df['gender'].fillna('ALL')\n",
    "        df['is_gender_restricted'] = df['gender'].apply(lambda x: 0 if str(x).lower() == 'all' else 1)\n",
    "        df['is_sick_only'] = df['healthy_volunteers'].apply(lambda x: 1 if str(x).lower() == 'no' else 0)\n",
    "\n",
    "        for c in ['child', 'adult', 'older_adult']:\n",
    "            if c not in df.columns: df[c] = 1\n",
    "\n",
    "        df['eligibility_strictness_score'] = df['is_gender_restricted'] + df['is_sick_only'] + (1 - df['child']) + (1 - df['older_adult'])\n",
    "        return df\n",
    "\n",
    "    def _engineer_agent_type(self, df):\n",
    "        if self.df_drugs.empty:\n",
    "            df['agent_category'] = 'UNKNOWN'; return df\n",
    "\n",
    "        type_priority = {'GENETIC': 1,'BIOLOGICAL': 2,'DRUG': 3,'DIETARY SUPPLEMENT': 4,'OTHER': 5}\n",
    "        df_int = self.df_drugs.copy()\n",
    "        df_int['type_upper'] = df_int['intervention_type'].str.upper()\n",
    "        df_int['priority'] = df_int['type_upper'].map(lambda x: type_priority.get(x, 5))\n",
    "        df_int = df_int.sort_values('priority')\n",
    "        best_types = df_int.drop_duplicates('nct_id')[['nct_id', 'type_upper', 'name']]\n",
    "        df = df.merge(best_types, on='nct_id', how='left')\n",
    "\n",
    "        def classify_molecule(row):\n",
    "            name = str(row['name']).lower()\n",
    "            itype = str(row['type_upper'])\n",
    "            if 'placebo' in name: return 'PLACEBO_CTRL'\n",
    "            if itype == 'GENETIC' or any(x in name for x in ['car-t', 'crispr', 'mrna']): return 'GENE_THERAPY'\n",
    "            if itype == 'BIOLOGICAL' or 'mab' in name: return 'BIOLOGIC'\n",
    "            return 'SMALL_MOLECULE_OTHER'\n",
    "\n",
    "        df['agent_category'] = df.apply(classify_molecule, axis=1)\n",
    "        df.drop(columns=['type_upper', 'priority', 'name_y'], inplace=True, errors='ignore')\n",
    "        if 'name_x' in df.columns: df.rename(columns={'name_x': 'official_title'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _engineer_safe_features(self, df):\n",
    "        for c in ['has_dmc', 'is_fda_regulated_drug']:\n",
    "            if c in df.columns: df[c] = df[c].astype(str).apply(lambda x: 1 if x.lower() in ['true', 't', '1', 'yes'] else 0)\n",
    "            else: df[c] = 0\n",
    "        return df\n",
    "\n",
    "    def _engineer_sponsor_features(self, df):\n",
    "        cols_needed = ['nct_id', 'lead_or_collaborator', 'name', 'agency_class']\n",
    "        df_sponsors = self._safe_load('sponsors.txt', cols=cols_needed)\n",
    "        if df_sponsors.empty:\n",
    "            df['sponsor_tier'] = 'TIER_2_OTHER'; df['sponsor_clean'] = 'UNKNOWN'; return df\n",
    "\n",
    "        leads = df_sponsors[df_sponsors['lead_or_collaborator'].str.lower() == 'lead'].drop_duplicates('nct_id')\n",
    "        leads = leads.rename(columns={'name': 'lead_sponsor'})\n",
    "        df = df.merge(leads[['nct_id', 'lead_sponsor', 'agency_class']], on='nct_id', how='left')\n",
    "\n",
    "        big_pharma = ['pfizer', 'gsk', 'novartis', 'astrazeneca', 'merck', 'roche', 'sanofi', 'johnson', 'bayer', 'lilly', 'abbvie', 'amgen']\n",
    "        df['sponsor_clean'] = df['lead_sponsor'].astype(str).str.lower()\n",
    "        df['sponsor_tier'] = df['sponsor_clean'].apply(lambda x: 'TIER_1_GIANT' if any(k in str(x) for k in big_pharma) else 'TIER_2_OTHER')\n",
    "        return df\n",
    "\n",
    "    def _calculate_competition(self, df):\n",
    "        df['competition_broad'] = 0\n",
    "        df['competition_niche'] = 0\n",
    "        return df\n",
    "\n",
    "    def _prepare_text(self, df):\n",
    "        df['txt_criteria'] = df.get('criteria', '').fillna('')\n",
    "        return df\n",
    "\n",
    "    def _attach_embeddings(self, df):\n",
    "        emb_path = os.path.join(self.data_path, 'embeddings_with_nctid.csv')\n",
    "        if os.path.exists(emb_path):\n",
    "            try:\n",
    "                df_emb = pd.read_csv(emb_path)\n",
    "                if 'nct_id' in df_emb.columns: df = df.merge(df_emb, on='nct_id', how='left')\n",
    "            except: pass\n",
    "        return df\n",
    "\n",
    "    def _load_user_facing_text(self, df):\n",
    "        df_brief = self._safe_load('brief_summaries.txt', cols=['nct_id', 'description'])\n",
    "        if not df_brief.empty:\n",
    "            df_brief = df_brief.rename(columns={'description': 'brief_summary_text'})\n",
    "            df = df.merge(df_brief.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "\n",
    "        df_detailed = self._safe_load('detailed_descriptions.txt', cols=['nct_id', 'description'])\n",
    "        if not df_detailed.empty:\n",
    "            df_detailed = df_detailed.rename(columns={'description': 'detailed_description_text'})\n",
    "            df = df.merge(df_detailed.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        return df\n",
    "\n",
    "    def _merge_file(self, df, filename, cols):\n",
    "        aux = self._safe_load(filename, cols=cols).drop_duplicates('nct_id')\n",
    "        if not aux.empty: return df.merge(aux, on='nct_id', how='left')\n",
    "        return df\n",
    "\n",
    "    def save(self, df, filename):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    input_df = pd.read_csv(os.path.join(DATA_PATH, INPUT_LIST_FILE))\n",
    "    target_nct_ids = input_df['nct_id'].unique().tolist()\n",
    "    print(f\"Found {len(target_nct_ids)} unique NCT IDs to process.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {INPUT_LIST_FILE}. Please run the previous selection script first.\")\n",
    "    sys.exit()\n",
    "\n",
    "loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "df_demo = loader.load_specific_trials(target_nct_ids)\n",
    "df_demo = loader.add_features_for_prediction(df_demo)\n",
    "loader.save(df_demo, filename=OUTPUT_FILE)\n",
    "\n",
    "print(\"\\n[SUCCESS] Demo Set Generated.\")\n",
    "print(f\"File: {os.path.join(DATA_PATH, OUTPUT_FILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df4b1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data & Model...\n",
      ">>> Generating Predictions...\n",
      ">>> Calculating SHAP Drivers...\n",
      "\n",
      "============================================================\n",
      " CASTING RESULTS: THE BEST TRIALS FOR YOUR DEMO\n",
      "============================================================\n",
      "\n",
      "[A] TOP ALIGNED FAILURES (Found 199)\n",
      "These are perfect because the Model's math matches the Human's text.\n",
      "\n",
      "--- Category: Therapeutic Landscape ---\n",
      "  ID: NCT03911869 | Risk: 81.3% | Reason: Study terminated due to lack of enrollment of the target population. There were no safety, efficacy,...\n",
      "  ID: NCT05144009 | Risk: 80.9% | Reason: The benefit-risk profile does not support continuation of the LOTIS-9 trial....\n",
      "  ID: NCT05256381 | Risk: 80.1% | Reason: Due to lack of expected efficacy shown at the time of the interim analysis...\n",
      "  ID: NCT03690141 | Risk: 79.7% | Reason: Terminated early due to a lack of efficacy....\n",
      "  ID: NCT05267106 | Risk: 78.8% | Reason: Recruitment ceased after a pre-planned futility interim analysis indicated a low probability to conf...\n",
      "  ID: NCT03184558 | Risk: 78.3% | Reason: Based on planned futility assessment...\n",
      "  ID: NCT02850406 | Risk: 78.2% | Reason: Emerging clinical data evaluated by Pfizer and shared with regulatory authorities indicates that the...\n",
      "  ID: NCT03341962 | Risk: 77.9% | Reason: Considering the efficacy results of the induction phase (allowed use of corticosteroids as concomita...\n",
      "  ID: NCT03301636 | Risk: 77.7% | Reason: Sponsor termination, not related to efficacy, safety or feasibility....\n",
      "  ID: NCT03901469 | Risk: 77.6% | Reason: Parts 1 and 2 and Expansion Cohort C were completed. Expansion Cohorts A and B were discontinued bas...\n",
      "  ID: NCT01514864 | Risk: 77.5% | Reason: Lack of efficacy and slow accrual...\n",
      "  ID: NCT03871829 | Risk: 77.4% | Reason: The decision was made to discontinue the 54767414MMY2065 study as the Data Review Committee recommen...\n",
      "  ID: NCT03844932 | Risk: 77.0% | Reason: The Sublimity Board of Directors carefully evaluated the IA data: the 9.6% treatment remission diffe...\n",
      "  ID: NCT02150967 | Risk: 76.9% | Reason: The study was terminated early due to limited efficacy in Cohorts 2 and 3 (exploratory endpoints). T...\n",
      "  ID: NCT04265534 | Risk: 76.8% | Reason: Lack of Clinical Benefit...\n",
      "  ID: NCT04987307 | Risk: 76.8% | Reason: The study was terminated due to meeting a prespecified futility rule and not related to any safety c...\n",
      "  ID: NCT03840902 | Risk: 76.8% | Reason: Based on recommendations by an external Independent data Monitoring Committee (IDMC), Sponsor decide...\n",
      "  ID: NCT03955471 | Risk: 76.8% | Reason: The study will not resume based on the results of a planned interim analysis that showed futility...\n",
      "  ID: NCT04752358 | Risk: 76.7% | Reason: Study was terminated due to difficulty recruiting subjects and lack of efficacy...\n",
      "  ID: NCT06964165 | Risk: 76.5% | Reason: The study stopped after meeting pre-specified futility criteria...\n",
      "\n",
      "--- Category: Sponsor & Operations ---\n",
      "  ID: NCT04796324 | Risk: 71.4% | Reason: Company Decision...\n",
      "  ID: NCT05441826 | Risk: 71.4% | Reason: Sponsor change...\n",
      "  ID: NCT04873869 | Risk: 69.6% | Reason: This study was prematurely terminated due to sponsor decision....\n",
      "  ID: NCT04454567 | Risk: 69.0% | Reason: Study stopped due to a change in the Sponsor's overall development strategy from treatment of chroni...\n",
      "  ID: NCT03139032 | Risk: 68.3% | Reason: Sponsor decision to terminate the study...\n",
      "  ID: NCT01963611 | Risk: 67.6% | Reason: Study was terminated based on sponsor discretion....\n",
      "  ID: NCT01606566 | Risk: 67.2% | Reason: Business reasons...\n",
      "  ID: NCT05176717 | Risk: 66.6% | Reason: Business decision...\n",
      "  ID: NCT04560166 | Risk: 66.3% | Reason: Due to business priorities...\n",
      "  ID: NCT03674242 | Risk: 65.8% | Reason: sponsor decision...\n",
      "  ID: NCT04264442 | Risk: 65.4% | Reason: Sponsor Decision...\n",
      "  ID: NCT05741346 | Risk: 65.1% | Reason: Sponsor decision...\n",
      "  ID: NCT04702568 | Risk: 65.0% | Reason: Sponsor decision...\n",
      "  ID: NCT05085964 | Risk: 64.5% | Reason: The sponsor decided to terminate the study early...\n",
      "  ID: NCT05659459 | Risk: 63.7% | Reason: Sponsor went bankrupt - trial was discontinued due to lack of financing...\n",
      "  ID: NCT04781647 | Risk: 63.6% | Reason: Study ABI-H0731-203 was terminated early by the study Sponsor for strategic reasons to prioritize re...\n",
      "  ID: NCT04166552 | Risk: 63.4% | Reason: Sponsor ceasing all operations due to company's shut down...\n",
      "  ID: NCT02074410 | Risk: 63.4% | Reason: Sponsor decision - pending further analysis of available data...\n",
      "  ID: NCT02456675 | Risk: 63.2% | Reason: A decision was made to terminate the study due to the changing treatment landscape for the developme...\n",
      "  ID: NCT03591965 | Risk: 62.9% | Reason: Based on the adjustment of clinical research and development strategysponsor decided to terminate t...\n",
      "\n",
      "--- Category: Patient & Criteria ---\n",
      "  ID: NCT03339401 | Risk: 72.5% | Reason: terminated due to low enrollment rate...\n",
      "  ID: NCT02859727 | Risk: 72.2% | Reason: All participant have transitioned to commercial product or compassionate use....\n",
      "  ID: NCT02257619 | Risk: 72.1% | Reason: Sponsor decision to not initiate part 2 due to slow enrollment and competing trials....\n",
      "  ID: NCT03005054 | Risk: 68.1% | Reason: Protracted enrollment and limited wound closure in the first three subjects...\n",
      "  ID: NCT05177107 | Risk: 68.0% | Reason: Study reached minimum enrollment threshold...\n",
      "  ID: NCT01679041 | Risk: 67.8% | Reason: Insufficient accruals; PI leaving site...\n",
      "  ID: NCT02785120 | Risk: 67.7% | Reason: Suspended due to enrollment challenges and business operations....\n",
      "  ID: NCT03336242 | Risk: 66.6% | Reason: More patients in Cohort 1 than Cohort 2 demonstrated a clinically meaningful reduction of seizure co...\n",
      "  ID: NCT03943147 | Risk: 64.9% | Reason: Insufficient enrollment...\n",
      "  ID: NCT02865122 | Risk: 64.2% | Reason: Sponsor decision based on enrollment...\n",
      "  ID: NCT01620268 | Risk: 64.2% | Reason: Lack of recruitment...\n",
      "  ID: NCT02128269 | Risk: 64.1% | Reason: Screening discontinued early due to slow patient enrollment...\n",
      "  ID: NCT02606994 | Risk: 63.9% | Reason: Lack of enrollment...\n",
      "  ID: NCT04715230 | Risk: 61.3% | Reason: Sponsor decision based on slower than anticipated enrollment leading to fund exhaustion....\n",
      "  ID: NCT01750684 | Risk: 61.0% | Reason: The study was terminated by the Sponsor due to insufficient enrollment....\n",
      "  ID: NCT01546350 | Risk: 59.7% | Reason: not enough patients recruited on time...\n",
      "  ID: NCT06012565 | Risk: 59.4% | Reason: Slow recruitment...\n",
      "  ID: NCT04480138 | Risk: 58.4% | Reason: Due to non availability of eligible subjects and slow recruitment...\n",
      "  ID: NCT02468414 | Risk: 58.1% | Reason: No statistical analysis was performed as only one subject was treated with MDGN201 TARGTEPO due to t...\n",
      "  ID: NCT04938180 | Risk: 57.3% | Reason: Further enrollment was halted in November 2021, enrolled subjects completed the study. Sponsor stopp...\n",
      "\n",
      "\n",
      "[B] TOP SUCCESS STORIES (Found 50)\n",
      "These show the model correctly identifying safe/effective trials.\n",
      "\n",
      "--- Driver: Patient & Criteria ---\n",
      "  ID: NCT06091826 | Risk: 10.8% | Title: Phase II Study Assessing Efficacy and Safety of NFL-101 on Reduction of Reinforc...\n",
      "  ID: NCT02719743 | Risk: 11.3% | Title: An Observer-blind, Dose Ranging Safety and Immunogenicity Study of GSK Biologica...\n",
      "  ID: NCT05734040 | Risk: 11.9% | Title: Phase 2a, Randomized, Double-blind (Double-dummy), Double Placebo-controlled, Pa...\n",
      "  ID: NCT05284799 | Risk: 12.1% | Title: Phase 2a, Randomized, Double-blind (Double-dummy), Controlled, Parallel-group St...\n",
      "  ID: NCT04363359 | Risk: 12.2% | Title: Immunogenicity and Safety of Quadrivalent Influenza Vaccine In 6 to 35 Months Po...\n",
      "  ID: NCT03849560 | Risk: 12.6% | Title: A Multicenter, Double-Blind, Randomized, Parallel Group Study of Safety, Reactog...\n",
      "  ID: NCT01669122 | Risk: 12.9% | Title: A Randomized, Cross-Over, Single Dose Pharmacokinetic Study of 4mg Nicotine Loze...\n",
      "  ID: NCT01561768 | Risk: 13.2% | Title: A Phase 2 Randomized, Observer-Blind, Dose-Ranging Study to Evaluate the Immunog...\n",
      "  ID: NCT01669096 | Risk: 13.2% | Title: Evaluation of the Kinetics of mRNA Expression After Two Doses of GSK Biologicals...\n",
      "  ID: NCT02359201 | Risk: 13.3% | Title: Double-blind, Randomized, Placebo-controlled, Cross-over Study of the Effect of ...\n",
      "  ID: NCT03992872 | Risk: 13.3% | Title: A Phase 2 Open-label Study to Assess the Safety and Immunogenicity of an Alum-ad...\n",
      "  ID: NCT04669691 | Risk: 13.9% | Title: A Phase 2, Randomized, Observer-Blind, Multicenter Study to Evaluate the Immunog...\n",
      "  ID: NCT03467412 | Risk: 14.1% | Title: A Randomised, Double-blind, Placebo-controlled Phase 2 Trial of FOL-005 to Inves...\n",
      "  ID: NCT04192500 | Risk: 14.1% | Title: A Phase 2a, Single Center, Randomized, Double-blind, Controlled Study to Evaluat...\n",
      "  ID: NCT01930357 | Risk: 14.2% | Title: Immunogenicity and Safety of the Purified Vero Rabies Vaccine - Serum Free (VRVg...\n",
      "  ID: NCT01732627 | Risk: 14.4% | Title: Immunogenicity and Safety of a Quadrivalent Meningococcal Conjugate Vaccine Admi...\n",
      "  ID: NCT01943825 | Risk: 14.5% | Title: Exploration of Immunologic Mechanisms of Immune Interference and/or Cross-Neutra...\n",
      "  ID: NCT03276962 | Risk: 15.1% | Title: Efficacy, Safety and Immunogenicity Study of GSK Biologicals' Candidate Malaria ...\n",
      "  ID: NCT01784874 | Risk: 15.3% | Title: Immunogenicity of the Purified Vero Rabies Vaccine - Serum Free in Comparison Wi...\n",
      "  ID: NCT02553343 | Risk: 15.6% | Title: Safety and Immunogenicity Trial of High-Dose Quadrivalent Influenza Vaccine Admi...\n",
      "\n",
      "\n",
      ">>> Total Selected for Demo: 80\n",
      "IDs saved to 'demo_playlist_ids.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION\n",
    "# ====================================================================\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "INPUT_FILE = 'demo_set.csv'\n",
    "MODEL_PATH = '/home/delaunan/code/delaunan/clintrialpredict/models/ctp_model.joblib'\n",
    "\n",
    "# ====================================================================\n",
    "# 1. LOGIC DEFINITIONS (The \"Brain\")\n",
    "# ====================================================================\n",
    "\n",
    "def get_pillar(feature_name):\n",
    "    \"\"\"Maps raw features to Business Pillars\"\"\"\n",
    "    n = str(feature_name).lower()\n",
    "    if any(x in n for x in ['emb_', 'criteria', 'eligibility', 'gender', 'age', 'child', 'sick', 'healthy', 'adult']):\n",
    "        return \"Patient & Criteria\"\n",
    "    if any(x in n for x in ['phase', 'masking', 'arms', 'endpoints', 'dmc', 'rigor', 'allocation', 'model']):\n",
    "        return \"Protocol Design\"\n",
    "    if any(x in n for x in ['sponsor', 'agency', 'us', 'year', 'covid', 'location']):\n",
    "        return \"Sponsor & Operations\"\n",
    "    return \"Therapeutic Landscape\"\n",
    "\n",
    "def tag_real_reason(text):\n",
    "    \"\"\"Tags the human text into categories\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '': return \"Unknown\"\n",
    "    t = str(text).lower()\n",
    "\n",
    "    # 1. EFFICACY (Matches Therapeutic Landscape)\n",
    "    if any(x in t for x in ['efficacy', 'futility', 'benefit', 'effective', 'response', 'endpoint', 'inferior']):\n",
    "        return \"Therapeutic Landscape\"\n",
    "    # 2. SAFETY (Matches Protocol or Therapeutic)\n",
    "    if any(x in t for x in ['safety', 'toxic', 'adverse', 'death', 'risk', 'mortality']):\n",
    "        return \"Protocol Design\"\n",
    "    # 3. ENROLLMENT (Matches Patient & Criteria)\n",
    "    if any(x in t for x in ['enroll', 'recruit', 'accrual', 'participant', 'subject', 'sample']):\n",
    "        return \"Patient & Criteria\"\n",
    "    # 4. BUSINESS (Matches Sponsor)\n",
    "    if any(x in t for x in ['business', 'fund', 'financ', 'sponsor', 'priorit', 'strategic', 'decision']):\n",
    "        return \"Sponsor & Operations\"\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "# ====================================================================\n",
    "# 2. PROCESSING\n",
    "# ====================================================================\n",
    "print(\">>> Loading Data & Model...\")\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, INPUT_FILE))\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Prepare Features\n",
    "# (We assume demo_set.csv has all columns needed. We drop metadata to get X)\n",
    "metadata_cols = ['nct_id', 'official_title', 'why_stopped', 'overall_status', 'start_date', 'completion_date', 'brief_title', 'brief_summary_text', 'detailed_description_text', 'txt_criteria']\n",
    "X = df.drop(columns=[c for c in metadata_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# 1. Predict\n",
    "print(\">>> Generating Predictions...\")\n",
    "# We need to use the pipeline's transform to ensure columns match\n",
    "preprocessor = model.named_steps['preprocessor']\n",
    "classifier = model.named_steps['classifier']\n",
    "\n",
    "try:\n",
    "    X_trans = preprocessor.transform(df) # Pass full df, preprocessor picks columns\n",
    "    probs = model.predict_proba(df)[:, 1]\n",
    "    df['risk_score'] = probs\n",
    "except Exception as e:\n",
    "    print(f\"Error in prediction: {e}\")\n",
    "    # Fallback: Try to predict on X directly if pipeline expects specific cols\n",
    "    X_trans = preprocessor.transform(X)\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    df['risk_score'] = probs\n",
    "\n",
    "# 2. SHAP\n",
    "print(\">>> Calculating SHAP Drivers...\")\n",
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X_trans)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 3. Aggregate Pillars per Trial\n",
    "dominant_pillars = []\n",
    "for i in range(len(df)):\n",
    "    scores = {'Patient & Criteria': 0, 'Protocol Design': 0, 'Sponsor & Operations': 0, 'Therapeutic Landscape': 0}\n",
    "    for idx, val in enumerate(shap_values[i]):\n",
    "        # We sum ABSOLUTE impact to find the \"Main Topic\" the model cares about\n",
    "        scores[get_pillar(feature_names[idx])] += abs(val)\n",
    "    dominant_pillars.append(max(scores, key=scores.get))\n",
    "\n",
    "df['model_driver'] = dominant_pillars\n",
    "df['human_reason'] = df['why_stopped'].apply(tag_real_reason)\n",
    "\n",
    "# ====================================================================\n",
    "# 3. SELECTION: THE \"CASTING CALL\"\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CASTING RESULTS: THE BEST TRIALS FOR YOUR DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- A. THE FAILURES (Aligned Logic) ---\n",
    "# We want: Terminated + High Risk Score + (Model Driver == Human Reason)\n",
    "failures = df[\n",
    "    (df['overall_status'] != 'COMPLETED') &\n",
    "    (df['human_reason'] != 'Unknown') &\n",
    "    (df['human_reason'] != 'Other') &\n",
    "    (df['model_driver'] == df['human_reason']) # <--- THE ALIGNMENT CHECK\n",
    "].sort_values('risk_score', ascending=False)\n",
    "\n",
    "print(f\"\\n[A] TOP ALIGNED FAILURES (Found {len(failures)})\")\n",
    "print(\"These are perfect because the Model's math matches the Human's text.\")\n",
    "\n",
    "categories = ['Therapeutic Landscape', 'Sponsor & Operations', 'Patient & Criteria', 'Protocol Design']\n",
    "selected_failures = []\n",
    "\n",
    "for cat in categories:\n",
    "    subset = failures[failures['human_reason'] == cat].head(20)\n",
    "    if not subset.empty:\n",
    "        print(f\"\\n--- Category: {cat} ---\")\n",
    "        for _, row in subset.iterrows():\n",
    "            print(f\"  ID: {row['nct_id']} | Risk: {row['risk_score']:.1%} | Reason: {row['why_stopped'][:100]}...\")\n",
    "            selected_failures.append(row['nct_id'])\n",
    "\n",
    "# --- B. THE SUCCESSES (Diverse Drivers) ---\n",
    "# We want: Completed + Low Risk Score\n",
    "successes = df[\n",
    "    (df['overall_status'] == 'COMPLETED') &\n",
    "    (df['risk_score'] < 0.40)\n",
    "].sort_values('risk_score', ascending=True)\n",
    "\n",
    "print(f\"\\n\\n[B] TOP SUCCESS STORIES (Found {len(successes)})\")\n",
    "print(\"These show the model correctly identifying safe/effective trials.\")\n",
    "\n",
    "selected_successes = []\n",
    "for cat in categories:\n",
    "    # We pick successes where the model felt that specific pillar was the STRONGEST positive signal\n",
    "    subset = successes[successes['model_driver'] == cat].head(20)\n",
    "    if not subset.empty:\n",
    "        print(f\"\\n--- Driver: {cat} ---\")\n",
    "        for _, row in subset.iterrows():\n",
    "            print(f\"  ID: {row['nct_id']} | Risk: {row['risk_score']:.1%} | Title: {row['official_title'][:80]}...\")\n",
    "            selected_successes.append(row['nct_id'])\n",
    "\n",
    "# ====================================================================\n",
    "# 4. SAVE THE FINAL PLAYLIST\n",
    "# ====================================================================\n",
    "final_playlist = selected_failures + selected_successes\n",
    "print(f\"\\n\\n>>> Total Selected for Demo: {len(final_playlist)}\")\n",
    "\n",
    "# Save just these IDs to a text file for easy copy-pasting\n",
    "with open('demo_playlist_ids.txt', 'w') as f:\n",
    "    for nid in final_playlist:\n",
    "        f.write(f\"{nid}\\n\")\n",
    "\n",
    "print(\"IDs saved to 'demo_playlist_ids.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a428237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File moved to: /home/delaunan/code/delaunan/clintrialpredict/data/demo_playlist_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "current_location = \"demo_playlist_ids.txt\"\n",
    "target_location = \"/home/delaunan/code/delaunan/clintrialpredict/data/demo_playlist_ids.txt\"\n",
    "\n",
    "# Move the file\n",
    "if os.path.exists(current_location):\n",
    "    shutil.move(current_location, target_location)\n",
    "    print(f\" File moved to: {target_location}\")\n",
    "else:\n",
    "    print(f\" File not found in current directory. It might already be in data/ or deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b43ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Data Path is set to: /home/delaunan/code/delaunan/clintrialpredict/data\n",
      "--- STARTING PREDICTION COHORT GENERATION ---\n",
      ">>> 1. Loading Full Cohort (2005-2025) for Competition Calculation...\n",
      "    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\n",
      "    -> Calculating Competition...\n",
      "    Full Cohort Size: 137724 trials.\n",
      ">>> 2. Filtering for Prediction Set (2015-2025, Active/Pending Statuses)...\n",
      "    Prediction Cohort: 62 trials (Active/Pending Phase 2/3, 2015-2025)\n",
      ">>> 3. Engineering Remaining Features for Prediction...\n",
      "    -> Engineering Sponsor Tiers...\n",
      "    -> Engineering Protocol Complexity (Calculating Age Flags)...\n",
      "    -> Engineering Agent Type (Bulletproof Classifier)...\n",
      "    -> Engineering Smart Patterns (Rigor & Strictness)...\n",
      "    -> Engineering Safe Protocol Features...\n",
      "    -> Preparing Text Features...\n",
      "    -> Attaching Vectorized Text Embeddings...\n",
      "       -> Successfully merged embeddings (Columns added: 20)\n",
      "    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\n",
      ">>> Saved 62 rows to /home/delaunan/code/delaunan/clintrialpredict/data/data_predict.csv\n",
      "\n",
      "[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# ====================================================================\n",
    "# 0. CONFIGURATION (THE ONLY LINE YOU MUST VERIFY)\n",
    "# ====================================================================\n",
    "# This MUST be the absolute path to the FOLDER containing your AACT .txt files.\n",
    "DATA_PATH = '/home/delaunan/code/delaunan/clintrialpredict/data'\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "print(f\"DEBUG: Data Path is set to: {DATA_PATH}\")\n",
    "print(\"--- STARTING PREDICTION COHORT GENERATION ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CLINICAL TRIAL LOADER CLASS (Full Definition)\n",
    "# ====================================================================\n",
    "\n",
    "class ClinicalTrialLoader:\n",
    "    \"\"\"\n",
    "    A specialized loader for generating the PROSPECTIVE prediction dataset (2024-2025).\n",
    "    It ensures feature consistency and excludes all post-hoc/leakage features.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.df_drugs = pd.DataFrame()\n",
    "\n",
    "        # --- STRATEGY A: PERFECT ---\n",
    "        self.params_perfect = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": csv.QUOTE_MINIMAL, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "        # --- STRATEGY B: ROBUST ---\n",
    "        self.params_robust = {\n",
    "            \"sep\": \"|\", \"dtype\": str, \"header\": 0, \"quotechar\": '\"',\n",
    "            \"quoting\": 3, \"low_memory\": False, \"on_bad_lines\": \"warn\"\n",
    "        }\n",
    "\n",
    "    def _safe_load(self, filename: str, cols: Optional[list] = None) -> pd.DataFrame:\n",
    "        full_path = os.path.join(self.data_path, filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"   [!] Warning: File not found {filename}. Features will be empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            return pd.read_csv(full_path, usecols=cols, **self.params_perfect)\n",
    "        except Exception as e:\n",
    "            print(f\"   [!] Formatting error in {filename}. Switching to Robust Mode...\")\n",
    "            try:\n",
    "                return pd.read_csv(full_path, usecols=cols, **self.params_robust)\n",
    "            except Exception as e2:\n",
    "                print(f\"   [x] CRITICAL: Could not load {filename}. Error: {e2}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "    # --- STEP 1: LOAD FULL COHORT FOR TIME-SENSITIVE FEATURES ---\n",
    "\n",
    "    def load_full_cohort_for_competition(self, min_year: int = 2005, max_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the entire relevant dataset (2005-2025) to calculate time-sensitive\n",
    "        features (Competition) on the complete historical context before splitting.\n",
    "        \"\"\"\n",
    "        print(f\">>> 1. Loading Full Cohort ({min_year}-{max_year}) for Competition Calculation...\")\n",
    "\n",
    "        # 1. Load Core Columns\n",
    "        cols_studies = ['nct_id', 'overall_status', 'study_type', 'phase',\n",
    "                        'start_date', 'number_of_arms',\n",
    "                        'official_title', 'why_stopped',\n",
    "                        'has_dmc', 'is_fda_regulated_drug', 'brief_title']\n",
    "\n",
    "        df = self._safe_load('studies.txt', cols=cols_studies)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Critical Error: 'studies.txt' failed to load.\")\n",
    "\n",
    "        # 2. Apply Core Filters (Interventional, Drug, Phase 2/3)\n",
    "        df = df[df['study_type'].str.upper() == 'INTERVENTIONAL'].copy()\n",
    "\n",
    "        cols_int = ['nct_id', 'intervention_type', 'name']\n",
    "        df_int = self._safe_load('interventions.txt', cols=cols_int)\n",
    "        if not df_int.empty:\n",
    "            target_types = ['DRUG', 'BIOLOGICAL', 'GENETIC']\n",
    "            drug_ids = df_int[df_int['intervention_type'].str.upper().isin(target_types)]['nct_id'].unique()\n",
    "            df = df[df['nct_id'].isin(drug_ids)]\n",
    "            self.df_drugs = df_int[df_int['intervention_type'].str.upper().isin(target_types + ['DIETARY SUPPLEMENT', 'OTHER'])].copy()\n",
    "\n",
    "        excluded_phases = ['EARLY_PHASE1', 'PHASE4', 'NA'] #'PHASE1'\n",
    "        df = df[~df['phase'].astype(str).str.upper().isin(excluded_phases)]\n",
    "        df = df.dropna(subset=['phase'])\n",
    "\n",
    "        # 3. Apply Full Date Range Filter (2005-2025)\n",
    "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        df['start_year'] = df['start_date'].dt.year\n",
    "        df = df[df['start_year'].between(min_year, max_year)].copy()\n",
    "\n",
    "        # 4. Calculate Time-Sensitive Features (Hierarchy and Competition)\n",
    "        df = self._attach_medical_hierarchy(df)\n",
    "        df = self._calculate_competition(df)\n",
    "\n",
    "        print(f\"    Full Cohort Size: {len(df)} trials.\")\n",
    "        return df.copy()\n",
    "\n",
    "\n",
    "    # --- STEP 2: FILTER AND FINALIZE PREDICTION SET ---\n",
    "\n",
    "    def load_and_clean_prediction_set(self, df_full_cohort: pd.DataFrame, start_year_predict: int = 2024) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters the full cohort for the Prediction Set: Active/Pending Statuses, Year >= 2024.\n",
    "        \"\"\"\n",
    "        df = df_full_cohort.copy()\n",
    "        print(f\">>> 2. Filtering for Prediction Set ({start_year_predict}-2025, Active/Pending Statuses)...\")\n",
    "\n",
    "        # 1. Filter: Date Range (2024-2025)\n",
    "        df = df[df['start_year'] >= start_year_predict].copy()\n",
    "\n",
    "        # 2. Filter: Status (Active/Pending Only)\n",
    "        # 2. Filter: Specific NCT IDs\n",
    "        # (Replaces the previous Active/Pending status filter)\n",
    "        TARGET_NCT_IDS = [\n",
    "            'NCT03911869', 'NCT05144009', 'NCT05256381', 'NCT03690141',\n",
    "            'NCT05267106', 'NCT03184558', 'NCT02850406', 'NCT03341962',\n",
    "            'NCT03301636', 'NCT03901469', 'NCT01514864', 'NCT03871829',\n",
    "            'NCT03844932', 'NCT02150967', 'NCT04265534', 'NCT04987307',\n",
    "            'NCT03840902', 'NCT03955471', 'NCT04752358', 'NCT06964165',\n",
    "            'NCT04796324', 'NCT05441826', 'NCT04873869', 'NCT04454567',\n",
    "            'NCT03139032', 'NCT01963611', 'NCT01606566', 'NCT05176717',\n",
    "            'NCT04560166', 'NCT03674242', 'NCT04264442', 'NCT05741346',\n",
    "            'NCT04702568', 'NCT05085964', 'NCT05659459', 'NCT04781647',\n",
    "            'NCT04166552', 'NCT02074410', 'NCT02456675', 'NCT03591965',\n",
    "            'NCT03339401', 'NCT02859727', 'NCT02257619', 'NCT03005054',\n",
    "            'NCT05177107', 'NCT01679041', 'NCT02785120', 'NCT03336242',\n",
    "            'NCT03943147', 'NCT02865122', 'NCT01620268', 'NCT02128269',\n",
    "            'NCT02606994', 'NCT04715230', 'NCT01750684', 'NCT01546350',\n",
    "            'NCT06012565', 'NCT04480138', 'NCT02468414', 'NCT04938180',\n",
    "            'NCT06091826', 'NCT02719743', 'NCT05734040', 'NCT05284799',\n",
    "            'NCT04363359', 'NCT03849560', 'NCT01669122', 'NCT01561768',\n",
    "            'NCT01669096', 'NCT02359201', 'NCT03992872', 'NCT04669691',\n",
    "            'NCT03467412', 'NCT04192500', 'NCT01930357', 'NCT01732627',\n",
    "            'NCT01943825', 'NCT03276962', 'NCT01784874', 'NCT02553343'\n",
    "        ]\n",
    "\n",
    "        # Filter to include only the specific NCT IDs listed above\n",
    "        # (Ensure 'nct_id' matches the column name in your dataframe)\n",
    "        df = df[df['nct_id'].isin(TARGET_NCT_IDS)]\n",
    "\n",
    "\n",
    "\n",
    "        # 3. Filter: COVID Sanitizer (Kept for consistency)\n",
    "        if 'why_stopped' in df.columns:\n",
    "            covid_keywords = ['covid', 'pandemic', 'coronavirus', 'sars-cov-2', 'logistical reasons']\n",
    "            mask_covid = df['why_stopped'].fillna('').astype(str).str.lower().apply(\n",
    "                lambda x: any(k in x for k in covid_keywords)\n",
    "            )\n",
    "            if mask_covid.sum() > 0:\n",
    "                print(f\"    [Sanitizer] Dropping {mask_covid.sum()} trials terminated due to COVID/Logistics.\")\n",
    "                df = df[~mask_covid]\n",
    "\n",
    "        # 4. Target: The prediction set has NO target.\n",
    "        if 'target' in df.columns:\n",
    "             df.drop(columns=['target'], inplace=True)\n",
    "\n",
    "        print(f\"    Prediction Cohort: {len(df)} trials (Active/Pending Phase 2/3, {start_year_predict}-2025)\")\n",
    "\n",
    "        # 5. Add remaining features and user-facing text\n",
    "        df = self.add_features_for_prediction(df)\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    # --- NEW PREDICTION FEATURE FUNCTION (STEP 3) ---\n",
    "\n",
    "    def add_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds remaining features (non-time-sensitive) and user-facing fields.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        print(\">>> 3. Engineering Remaining Features for Prediction...\")\n",
    "\n",
    "        # 1. Operational Flags\n",
    "        df['covid_exposure'] = df['start_year'].between(2019, 2022).astype(int)\n",
    "\n",
    "        # 2. Geography (SAFE: Only is_us)\n",
    "        df_countries = self._safe_load('countries.txt', cols=['nct_id', 'name'])\n",
    "        if not df_countries.empty:\n",
    "            us_trials = df_countries[df_countries['name'] == 'United States']['nct_id'].unique()\n",
    "            df['includes_us'] = df['nct_id'].isin(us_trials).astype(int)\n",
    "        else:\n",
    "            df['includes_us'] = 0\n",
    "\n",
    "        # 3. Merge Standard Metadata\n",
    "        df = self._merge_file(df, 'designs.txt', ['nct_id', 'allocation', 'intervention_model', 'masking', 'primary_purpose'])\n",
    "        # NOTE: This line adds 'number_of_primary_outcomes_to_measure'\n",
    "        df = self._merge_file(df, 'calculated_values.txt', ['nct_id', 'number_of_primary_outcomes_to_measure'])\n",
    "\n",
    "        # 4. Sponsor Engineering\n",
    "        df = self._engineer_sponsor_features(df)\n",
    "\n",
    "        # 5. Complexity Engineering (Loads eligibility.txt and 'criteria' column)\n",
    "        df = self._engineer_complexity(df)\n",
    "\n",
    "        # 6. Agent Type\n",
    "        df = self._engineer_agent_type(df)\n",
    "\n",
    "        # 7. Smart Patterns (Rigor & Strictness)\n",
    "        df = self._engineer_smart_patterns(df)\n",
    "\n",
    "        # 8. Safe Features (DMC)\n",
    "        df = self._engineer_safe_features(df)\n",
    "\n",
    "        # 9. Text Features (Creates txt_tags, txt_criteria)\n",
    "        df = self._prepare_text(df)\n",
    "\n",
    "        # 10. ATTACH EMBEDDINGS (TEMPORARILY SKIPPED FOR COLLEAGUE'S WORKFLOW)\n",
    "        df = self._attach_embeddings(df)\n",
    "\n",
    "        # 11. Attach User-Facing Text Fields\n",
    "        df = self._load_user_facing_text(df)\n",
    "\n",
    "        # --- CLEANUP (FIXED LOGIC) ---\n",
    "        old_col = 'number_of_primary_outcomes_to_measure'\n",
    "        new_col = 'num_primary_endpoints'\n",
    "\n",
    "        if old_col in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "            # Convert to numeric and fill NaNs in the new column\n",
    "            df[new_col] = pd.to_numeric(df[new_col], errors='coerce').fillna(1)\n",
    "        else:\n",
    "            # If the column was never loaded, create it with the default value 1\n",
    "            df[new_col] = 1\n",
    "        # -----------------------------\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- NEW HELPER FUNCTION FOR UI/USER-FACING DATA ---\n",
    "\n",
    "    def _load_user_facing_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads text fields that are useful for the UI but not necessarily for the model.\n",
    "        \"\"\"\n",
    "        print(\"    -> Loading User-Facing Text (Brief Summary, Detailed Description)...\")\n",
    "\n",
    "        # 1. Brief Summary\n",
    "        cols_brief = ['nct_id', 'description']\n",
    "        df_brief = self._safe_load('brief_summaries.txt', cols=cols_brief)\n",
    "        if not df_brief.empty:\n",
    "            df_brief = df_brief.rename(columns={'description': 'brief_summary_text'})\n",
    "            df = df.merge(df_brief.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['brief_summary_text'] = \"\"\n",
    "\n",
    "        # 2. Detailed Description\n",
    "        cols_detailed = ['nct_id', 'description']\n",
    "        df_detailed = self._safe_load('detailed_descriptions.txt', cols=cols_detailed)\n",
    "        if not df_detailed.empty:\n",
    "            df_detailed = df_detailed.rename(columns={'description': 'detailed_description_text'})\n",
    "            df = df.merge(df_detailed.drop_duplicates('nct_id'), on='nct_id', how='left')\n",
    "        else:\n",
    "            df['detailed_description_text'] = \"\"\n",
    "\n",
    "        return df\n",
    "\n",
    "    # --- ALL OTHER NECESSARY HELPER METHODS (KEPT FOR FUNCTIONALITY) ---\n",
    "\n",
    "    def _engineer_smart_patterns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Smart Patterns (Rigor & Strictness)...\")\n",
    "        def get_masking_score(val):\n",
    "            val = str(val).lower()\n",
    "            if 'quadruple' in val: return 3\n",
    "            if 'double' in val: return 2\n",
    "            if 'single' in val: return 1\n",
    "            return 0\n",
    "        def get_allocation_score(val):\n",
    "            return 1 if 'randomized' in str(val).lower() else 0\n",
    "        def get_model_score(val):\n",
    "            val = str(val).lower()\n",
    "            return 1 if 'crossover' in val or 'factorial' in val else 0\n",
    "        df['score_masking'] = df['masking'].apply(get_masking_score)\n",
    "        df['score_allocation'] = df['allocation'].apply(get_allocation_score)\n",
    "        df['score_model'] = df['intervention_model'].apply(get_model_score)\n",
    "        df['design_rigor_score'] = df['score_masking'] + df['score_allocation'] + df['score_model']\n",
    "        df['is_gender_restricted'] = df['gender'].apply(lambda x: 0 if str(x).lower() == 'all' else 1)\n",
    "        df['is_sick_only'] = df['healthy_volunteers'].apply(lambda x: 1 if str(x).lower() == 'no' else 0)\n",
    "        for col in ['child', 'adult', 'older_adult']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(lambda x: 1 if x.lower() in ['true', '1', 'yes'] else 0)\n",
    "            else:\n",
    "                df[col] = 1\n",
    "        df['eligibility_strictness_score'] = (\n",
    "            df['is_gender_restricted'] +\n",
    "            df['is_sick_only'] +\n",
    "            (1 - df['child']) +\n",
    "            (1 - df['older_adult'])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _engineer_agent_type(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Agent Type (Bulletproof Classifier)...\")\n",
    "        if self.df_drugs.empty:\n",
    "            df['agent_category'] = 'UNKNOWN'\n",
    "            return df\n",
    "        type_priority = {'GENETIC': 1,'BIOLOGICAL': 2,'DRUG': 3,'DIETARY SUPPLEMENT': 4,'OTHER': 5}\n",
    "        df_int = self.df_drugs.copy()\n",
    "        df_int['type_upper'] = df_int['intervention_type'].str.upper()\n",
    "        df_int['priority'] = df_int['type_upper'].map(lambda x: type_priority.get(x, 5))\n",
    "        df_int = df_int.sort_values('priority')\n",
    "        best_types = df_int.drop_duplicates('nct_id')[['nct_id', 'type_upper', 'name']]\n",
    "        df = df.merge(best_types, on='nct_id', how='left')\n",
    "        def classify_molecule(row):\n",
    "            itype = str(row['type_upper'])\n",
    "            name = str(row['name']).lower()\n",
    "            if 'placebo' in name: return 'PLACEBO_CTRL'\n",
    "            if itype == 'GENETIC': return 'GENE_THERAPY'\n",
    "            if any(x in name for x in ['car-t', 'chimeric antigen', 'autologous', 'allogeneic', 't-cell', 'nk cell']):\n",
    "                return 'CELL_THERAPY'\n",
    "            if name.endswith('cel'): return 'CELL_THERAPY'\n",
    "            if any(x in name for x in ['crispr', 'cas9', 'mrna', 'sirna', 'antisense', 'oligonucleotide', 'plasmid', 'vector', 'aav']):\n",
    "                return 'RNA_GENE_THERAPY'\n",
    "            if itype == 'BIOLOGICAL': return 'BIOLOGIC'\n",
    "            if 'mab' in name:\n",
    "                if 'adc' in name or 'conjugate' in name: return 'ANTIBODY_DRUG_CONJUGATE'\n",
    "                return 'MONOCLONAL_ANTIBODY'\n",
    "            if name.endswith('cept'): return 'BIOLOGIC_FUSION'\n",
    "            if 'vaccine' in name: return 'VACCINE'\n",
    "            if any(x in name for x in ['interferon', 'interleukin', 'cytokine']): return 'IMMUNOTHERAPY'\n",
    "            if name.endswith('ib') or 'ib ' in name:\n",
    "                if 'tinib' in name: return 'KINASE_INHIBITOR_TYROSINE'\n",
    "                if 'parib' in name: return 'PARP_INHIBITOR'\n",
    "                if 'lisib' in name: return 'PI3K_INHIBITOR'\n",
    "                return 'TARGETED_KINASE_INHIBITOR'\n",
    "            if 'vastatin' in name: return 'STATIN_CHOLESTEROL'\n",
    "            if name.endswith('stat') or 'stat ' in name: return 'ENZYME_INHIBITOR'\n",
    "            if name.endswith('degib'): return 'HEDGEHOG_INHIBITOR'\n",
    "            if name.endswith('clax'): return 'BCL2_INHIBITOR'\n",
    "            chemo_stems = ['platin', 'taxel', 'rubicin', 'fluorouracil', 'gemcitabine',\n",
    "                           'cyclophosphamide', 'methotrexate', 'etoposide', 'vincristine', 'vinblastine']\n",
    "            if any(x in name for x in chemo_stems):\n",
    "                return 'CHEMOTHERAPY'\n",
    "            return 'SMALL_MOLECULE_OTHER'\n",
    "        df['agent_category'] = df.apply(classify_molecule, axis=1)\n",
    "        df.drop(columns=['type_upper', 'priority', 'name_y'], inplace=True, errors='ignore')\n",
    "        if 'name_x' in df.columns: df.rename(columns={'name_x': 'official_title'}, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _engineer_safe_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Safe Protocol Features...\")\n",
    "        for col in ['has_dmc', 'is_fda_regulated_drug']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).apply(\n",
    "                    lambda x: 1 if x.lower() in ['true', 't', '1', 'yes'] else 0\n",
    "                )\n",
    "            else:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def _attach_medical_hierarchy(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Attaching Medical Hierarchy (Bridge: nct_id -> mesh_term -> area)...\")\n",
    "        cols_bridge = ['nct_id', 'mesh_term']\n",
    "        df_bridge = self._safe_load('browse_conditions.txt', cols=cols_bridge)\n",
    "        if df_bridge.empty:\n",
    "            cols_cond = ['nct_id', 'name']\n",
    "            df_bridge = self._safe_load('conditions.txt', cols=cols_cond)\n",
    "            if not df_bridge.empty:\n",
    "                df_bridge.rename(columns={'name': 'mesh_term'}, inplace=True)\n",
    "        mesh_path = os.path.join(self.data_path, 'mesh_lookup.csv')\n",
    "        df_dictionary = pd.DataFrame()\n",
    "        if os.path.exists(mesh_path):\n",
    "            try:\n",
    "                df_dictionary = pd.read_csv(mesh_path, sep='|', on_bad_lines='skip')\n",
    "                if 'mesh_term' in df_dictionary.columns and 'therapeutic_area' in df_dictionary.columns:\n",
    "                    df_dictionary = df_dictionary[['mesh_term', 'therapeutic_area']].drop_duplicates()\n",
    "                    df_dictionary.rename(columns={'therapeutic_area': 'lookup_area'}, inplace=True)\n",
    "                else:\n",
    "                    df_dictionary = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"       [!] Error reading mesh_lookup.csv: {e}\")\n",
    "        if not df_bridge.empty:\n",
    "            if not df_dictionary.empty:\n",
    "                df_full_mesh = df_bridge.merge(df_dictionary, on='mesh_term', how='left')\n",
    "            else:\n",
    "                df_full_mesh = df_bridge\n",
    "                df_full_mesh['lookup_area'] = np.nan\n",
    "            df_grouped = df_full_mesh.groupby('nct_id').agg({\n",
    "                'mesh_term': 'first',\n",
    "                'lookup_area': 'first'\n",
    "            }).reset_index()\n",
    "            df = df.merge(df_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['mesh_term'] = np.nan\n",
    "            df['lookup_area'] = np.nan\n",
    "        smart_path = os.path.join(self.data_path, 'smart_pathology_lookup.csv')\n",
    "        if os.path.exists(smart_path):\n",
    "            try:\n",
    "                df_smart = pd.read_csv(smart_path)\n",
    "                if 'nct_id' in df_smart.columns:\n",
    "                    df = df.merge(df_smart, on='nct_id', how='left')\n",
    "            except:\n",
    "                pass\n",
    "        if 'best_pathology' not in df.columns: df['best_pathology'] = np.nan\n",
    "        if 'therapeutic_area' not in df.columns: df['therapeutic_area'] = np.nan\n",
    "        def get_final_category(row):\n",
    "            if pd.notna(row.get('best_pathology')) and str(row.get('best_pathology')) != 'Unknown':\n",
    "                return row['best_pathology']\n",
    "            if pd.notna(row.get('lookup_area')) and str(row.get('lookup_area')) != 'Other/Unclassified':\n",
    "                return row['lookup_area']\n",
    "            if pd.notna(row.get('mesh_term')):\n",
    "                return row['mesh_term']\n",
    "            if pd.notna(row.get('therapeutic_area')) and str(row.get('therapeutic_area')) != 'Other':\n",
    "                return row['therapeutic_area']\n",
    "            return 'Unclassified Condition'\n",
    "        df['therapeutic_subgroup_name'] = df.apply(get_final_category, axis=1)\n",
    "        df.drop(columns=['mesh_term', 'lookup_area'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _engineer_sponsor_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Sponsor Tiers...\")\n",
    "        cols_needed = ['nct_id', 'lead_or_collaborator', 'name', 'agency_class']\n",
    "        df_sponsors = self._safe_load('sponsors.txt', cols=cols_needed)\n",
    "        if df_sponsors.empty:\n",
    "            df['sponsor_tier'] = 'TIER_2_OTHER'\n",
    "            df['sponsor_clean'] = 'UNKNOWN'\n",
    "            df['agency_class'] = 'UNKNOWN'\n",
    "            return df\n",
    "        leads = df_sponsors[df_sponsors['lead_or_collaborator'].str.lower() == 'lead'][['nct_id', 'name', 'agency_class']]\n",
    "        leads = leads.rename(columns={'name': 'lead_sponsor'})\n",
    "        leads = leads.drop_duplicates('nct_id')\n",
    "        df = df.merge(leads, on='nct_id', how='left')\n",
    "        df['lead_sponsor'] = df['lead_sponsor'].fillna('UNKNOWN')\n",
    "        df['agency_class'] = df['agency_class'].fillna('UNKNOWN')\n",
    "        clean_col = df['lead_sponsor'].astype(str).str.lower().str.strip()\n",
    "        legal_pattern = r'[.,]|\\binc\\b|\\bltd\\b|\\bllc\\b|\\bcorp\\b|\\bgmbh\\b|\\bsa\\b|\\bplc\\b'\n",
    "        clean_col = clean_col.str.replace(legal_pattern, '', regex=True).str.strip()\n",
    "        mappings = {\n",
    "            'Pfizer': ['pfizer', 'wyeth', 'hospira'], 'GSK': ['glaxo', 'gsk', 'smithkline'],\n",
    "            'Novartis': ['novartis', 'sandoz'], 'AstraZeneca': ['astrazeneca', 'medimmune'],\n",
    "            'Merck': ['merck', 'msd'], 'Roche': ['roche', 'genentech', 'hoffmann'],\n",
    "            'Sanofi': ['sanofi', 'aventis', 'genzyme'], 'J&J': ['johnson & johnson', 'janssen'],\n",
    "            'Bayer': ['bayer', 'monsanto'], 'Boehringer': ['boehringer'],\n",
    "            'BMS': ['bristol-myers', 'squibb', 'celgene'], 'Lilly': ['lilly'],\n",
    "            'Abbott': ['abbott', 'abbvie'], 'Amgen': ['amgen'],\n",
    "            'Takeda': ['takeda', 'shire'], 'Gilead': ['gilead'],\n",
    "            'Novo Nordisk': ['novo nordisk'], 'NIH': ['national cancer institute', 'nci', 'national institutes of health', 'nih']\n",
    "        }\n",
    "        final_names = clean_col.copy()\n",
    "        for std, keys in mappings.items():\n",
    "            pattern = '|'.join(keys)\n",
    "            mask = clean_col.str.contains(pattern, case=False, regex=True)\n",
    "            final_names.loc[mask] = std\n",
    "        df['sponsor_clean'] = final_names\n",
    "        def get_tier(name):\n",
    "            if name in mappings.keys(): return 'TIER_1_GIANT'\n",
    "            return 'TIER_2_OTHER'\n",
    "        df['sponsor_tier'] = df['sponsor_clean'].apply(get_tier)\n",
    "        return df\n",
    "\n",
    "    def _engineer_complexity(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Engineering Protocol Complexity (Calculating Age Flags)...\")\n",
    "        cols_needed = ['nct_id', 'criteria', 'gender', 'healthy_volunteers', 'minimum_age', 'maximum_age']\n",
    "        df_elig = self._safe_load('eligibilities.txt', cols=cols_needed)\n",
    "        if df_elig.empty:\n",
    "            df['criteria_len_log'] = 0\n",
    "            for c in ['gender', 'healthy_volunteers', 'adult', 'child', 'older_adult']: df[c] = 0\n",
    "            return df\n",
    "        df_elig = df_elig.drop_duplicates('nct_id')\n",
    "        df = df.merge(df_elig, on='nct_id', how='left')\n",
    "        df['criteria_len_log'] = np.log1p(df['criteria'].astype(str).str.len().fillna(0))\n",
    "        df['healthy_volunteers'] = df['healthy_volunteers'].astype(str).str.lower().apply(\n",
    "            lambda x: 'no' if x in ['f', 'false', '0', 'no', 'nan'] else 'yes'\n",
    "        )\n",
    "        df['gender'] = df['gender'].fillna('UNKNOWN')\n",
    "        def parse_age_to_years(val, default_val):\n",
    "            if pd.isna(val) or str(val).lower() in ['n/a', 'nan', '', 'none']: return default_val\n",
    "            try:\n",
    "                match = re.search(r'(\\d+(\\.\\d+)?)', str(val))\n",
    "                if not match: return default_val\n",
    "                num = float(match.group(1))\n",
    "                text = str(val).lower()\n",
    "                if 'month' in text: num /= 12.0\n",
    "                elif 'week' in text: num /= 52.0\n",
    "                elif 'day' in text: num /= 365.0\n",
    "                elif 'hour' in text: num /= 8760.0\n",
    "                return num\n",
    "            except:\n",
    "                return default_val\n",
    "        df['min_age_years'] = df['minimum_age'].apply(lambda x: parse_age_to_years(x, 0.0))\n",
    "        df['max_age_years'] = df['maximum_age'].apply(lambda x: parse_age_to_years(x, 100.0))\n",
    "        df['child'] = (df['min_age_years'] < 18).astype(int)\n",
    "        df['adult'] = ((df['max_age_years'] >= 18) & (df['min_age_years'] < 65)).astype(int)\n",
    "        df['older_adult'] = (df['max_age_years'] > 65).astype(int)\n",
    "        df.drop(columns=['minimum_age', 'maximum_age', 'min_age_years', 'max_age_years'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def _calculate_competition(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Calculating Competition...\")\n",
    "        try:\n",
    "            req_cols = ['start_year', 'therapeutic_area', 'phase']\n",
    "            if not all(col in df.columns for col in req_cols):\n",
    "                df['competition_broad'] = 0\n",
    "                df['competition_niche'] = 0\n",
    "                return df\n",
    "            grid = df.groupby(['start_year', 'therapeutic_area']).size().reset_index(name='count')\n",
    "            lookup = dict(zip(zip(grid['start_year'], grid['therapeutic_area']), grid['count']))\n",
    "            def get_comp(row):\n",
    "                y, area = row['start_year'], row['therapeutic_area']\n",
    "                return lookup.get((y, area), 0) + lookup.get((y-1, area), 0)\n",
    "            df['competition_broad'] = df.apply(get_comp, axis=1)\n",
    "            if 'therapeutic_subgroup_name' in df.columns:\n",
    "                grid_niche = df.groupby(['start_year', 'therapeutic_subgroup_name']).size().reset_index(name='count')\n",
    "                lookup_niche = dict(zip(zip(grid_niche['start_year'], grid_niche['therapeutic_subgroup_name']), grid_niche['count']))\n",
    "                def get_niche(row):\n",
    "                    y, sub = row['start_year'], row['therapeutic_subgroup_name']\n",
    "                    return lookup_niche.get((y, sub), 0) + lookup_niche.get((y-1, sub), 0)\n",
    "                df['competition_niche'] = df.apply(get_niche, axis=1)\n",
    "            else:\n",
    "                df['competition_niche'] = 0\n",
    "        except:\n",
    "            df['competition_broad'] = 0\n",
    "            df['competition_niche'] = 0\n",
    "        return df\n",
    "\n",
    "    def _prepare_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Preparing Text Features...\")\n",
    "        df_keys = self._safe_load('keywords.txt', cols=['nct_id', 'name'])\n",
    "        if not df_keys.empty:\n",
    "            keys_grouped = df_keys.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_keywords')\n",
    "            df = df.merge(keys_grouped, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_keywords'] = \"\"\n",
    "        if not self.df_drugs.empty:\n",
    "            int_names = self.df_drugs.groupby('nct_id')['name'].apply(lambda x: \" \".join(x.dropna().astype(str))).reset_index(name='txt_int_names')\n",
    "            df = df.merge(int_names, on='nct_id', how='left')\n",
    "        else:\n",
    "            df['txt_int_names'] = \"\"\n",
    "        text_cols = ['official_title', 'txt_keywords', 'txt_int_names', 'criteria']\n",
    "        for c in text_cols:\n",
    "            if c in df.columns: df[c] = df[c].fillna(\"\")\n",
    "        df['txt_tags'] = (df['official_title'] + \" \" + df['txt_keywords'] + \" \" + df['txt_int_names'])\n",
    "        if 'criteria' in df.columns:\n",
    "            df['txt_criteria'] = df['criteria']\n",
    "            df.drop(columns=['txt_keywords', 'txt_int_names'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    # NOTE: _attach_embeddings is now a placeholder function\n",
    "\n",
    "    def _attach_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"    -> Attaching Vectorized Text Embeddings...\")\n",
    "        emb_path = os.path.join(self.data_path, 'embeddings_with_nctid.csv')\n",
    "\n",
    "        # 1. Check if file exists\n",
    "        if not os.path.exists(emb_path):\n",
    "            print(\"    [!] Warning: embeddings_with_nctid.csv not found.\")\n",
    "            return df\n",
    "\n",
    "        # 2. LOAD AND MERGE\n",
    "        try:\n",
    "            # Load the embeddings CSV\n",
    "            df_emb = pd.read_csv(emb_path)\n",
    "\n",
    "            # Check if 'nct_id' is present to perform the merge\n",
    "            if 'nct_id' in df_emb.columns:\n",
    "                # Merge logic: Left join on nct_id\n",
    "                df = df.merge(df_emb, on='nct_id', how='left')\n",
    "                print(f\"       -> Successfully merged embeddings (Columns added: {df_emb.shape[1] - 1})\")\n",
    "            else:\n",
    "                print(\"       [!] Error: 'nct_id' column missing in embeddings file. Cannot merge.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"       [!] Error reading/merging embeddings: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # NOTE: _attach_p_values is ONLY called in the original training pipeline (not in prediction)\n",
    "    def _attach_p_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # This function is not used in the prediction pipeline\n",
    "        return df\n",
    "\n",
    "    def _merge_file(self, df: pd.DataFrame, filename: str, cols: list, filter_col: Optional[str] = None, filter_val: Optional[str] = None) -> pd.DataFrame:\n",
    "        try:\n",
    "            aux = self._safe_load(filename, cols=cols + ([filter_col] if filter_col else []))\n",
    "            if aux.empty: return df\n",
    "            if filter_col:\n",
    "                aux = aux[aux[filter_col] == filter_val].drop(columns=[filter_col])\n",
    "            aux = aux.drop_duplicates('nct_id')\n",
    "            return df.merge(aux, on='nct_id', how='left')\n",
    "        except:\n",
    "            return df\n",
    "\n",
    "    def save(self, df: pd.DataFrame, filename: str = 'data_predict.csv'):\n",
    "        out_path = os.path.join(self.data_path, filename)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\">>> Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. EXECUTION CODE (The part that runs the process)\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # 1. Instantiate the loader\n",
    "    loader = ClinicalTrialLoader(data_path=DATA_PATH)\n",
    "\n",
    "    # --- STEP 1: Load and Calculate Time-Sensitive Features on the Full Cohort (2005-2025) ---\n",
    "    df_full = loader.load_full_cohort_for_competition(min_year=2005, max_year=2025)\n",
    "\n",
    "    # --- STEP 2: Generate Prediction Set (2024-2025, Active/Pending) ---\n",
    "    # Note: start_year_predict=2024 enforces the new requirement\n",
    "    df_predict = loader.load_and_clean_prediction_set(df_full, start_year_predict=2015)\n",
    "\n",
    "    # 3. Save the final dataset for prediction\n",
    "    loader.save(df_predict)\n",
    "\n",
    "    print(\"\\n[SUCCESS] Prediction cohort ready for model scoring and embedding generation.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n[CRITICAL FAILURE] Data loading stopped: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check if 'studies.txt' is present in the DATA_PATH folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[UNEXPECTED FAILURE] An unexpected error occurred: {e}\")\n",
    "    print(\"ACTION REQUIRED: Check file paths, file names, and data integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e570aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/data_predict.csv...\n",
      "Converting to data/data_predict.xlsx...\n",
      "Done! File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "csv_path = 'data/data_predict.csv'\n",
    "excel_path = 'data/data_predict.xlsx'  # .xlsx is the modern Excel format\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"Reading {csv_path}...\")\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Save as Excel\n",
    "    print(f\"Converting to {excel_path}...\")\n",
    "    df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(\"Done! File saved successfully.\")\n",
    "else:\n",
    "    print(f\"Error: Could not find {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clintrialpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
